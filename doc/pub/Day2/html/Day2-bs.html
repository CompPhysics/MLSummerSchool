<!--
Automatically generated HTML file from DocOnce source
(https://github.com/doconce/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Day 2, Ridge and Lasso Regression and Resampling Methods">

<title>Data Analysis and Machine Learning: Day 2, Ridge and Lasso Regression and Resampling Methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for Day 2', 2, None, 'plans-for-day-2'),
              ('Linear Regression, basic overview',
               2,
               None,
               'linear-regression-basic-overview'),
              ('Why Linear Regression (aka Ordinary Least Squares and family)?',
               2,
               None,
               'why-linear-regression-aka-ordinary-least-squares-and-family'),
              ('Additional Reading', 2, None, 'additional-reading'),
              ('Regression Analysis, Definitions and Aims',
               2,
               None,
               'regression-analysis-definitions-and-aims'),
              ('Regression analysis, overarching aims',
               2,
               None,
               'regression-analysis-overarching-aims'),
              ('Regression analysis, overarching aims II',
               2,
               None,
               'regression-analysis-overarching-aims-ii'),
              ('Examples', 2, None, 'examples'),
              ('General linear models', 2, None, 'general-linear-models'),
              ('Rewriting the fitting procedure as a linear algebra problem',
               2,
               None,
               'rewriting-the-fitting-procedure-as-a-linear-algebra-problem'),
              ('Rewriting the fitting procedure as a linear algebra problem, '
               'more details',
               2,
               None,
               'rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details'),
              ('Generalizing the fitting procedure as a linear algebra problem',
               2,
               None,
               'generalizing-the-fitting-procedure-as-a-linear-algebra-problem'),
              ('Generalizing the fitting procedure as a linear algebra problem',
               2,
               None,
               'generalizing-the-fitting-procedure-as-a-linear-algebra-problem'),
              ('Optimizing our parameters',
               2,
               None,
               'optimizing-our-parameters'),
              ('Our model for the nuclear binding energies',
               2,
               None,
               'our-model-for-the-nuclear-binding-energies'),
              ('Optimizing our parameters, more details',
               2,
               None,
               'optimizing-our-parameters-more-details'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               'interpretations-and-optimizing-our-parameters'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               'interpretations-and-optimizing-our-parameters'),
              ('Some useful matrix and vector expressions',
               2,
               None,
               'some-useful-matrix-and-vector-expressions'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               'interpretations-and-optimizing-our-parameters'),
              ('Own code for Ordinary Least Squares',
               2,
               None,
               'own-code-for-ordinary-least-squares'),
              ('Adding error analysis and training set up',
               2,
               None,
               'adding-error-analysis-and-training-set-up'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('Regression Examples', 2, None, 'regression-examples'),
              ('Fitting an Equation of State for Dense Nuclear Matter',
               2,
               None,
               'fitting-an-equation-of-state-for-dense-nuclear-matter'),
              ('The code', 2, None, 'the-code'),
              ('Splitting our Data in Training and Test data',
               2,
               None,
               'splitting-our-data-in-training-and-test-data'),
              ('The Boston housing data example',
               2,
               None,
               'the-boston-housing-data-example'),
              ('Housing data, the code', 2, None, 'housing-data-the-code'),
              ('Reducing the number of degrees of freedom, overarching view',
               2,
               None,
               'reducing-the-number-of-degrees-of-freedom-overarching-view'),
              ('Preprocessing our data', 2, None, 'preprocessing-our-data'),
              ('More preprocessing', 2, None, 'more-preprocessing'),
              ('Simple preprocessing examples, Franke function and regression',
               2,
               None,
               'simple-preprocessing-examples-franke-function-and-regression'),
              ('Beyond Ordinary Least Squares',
               2,
               None,
               'beyond-ordinary-least-squares'),
              ('Ridge and LASSO Regression',
               2,
               None,
               'ridge-and-lasso-regression'),
              ('More on Ridge Regression', 2, None, 'more-on-ridge-regression'),
              ('Simple Ridge  interpretations',
               2,
               None,
               'simple-ridge-interpretations'),
              ('Statistics', 2, None, 'statistics'),
              ('Where are we going?', 2, None, 'where-are-we-going'),
              ('Linking the regression analysis with a statistical '
               'interpretation',
               2,
               None,
               'linking-the-regression-analysis-with-a-statistical-interpretation'),
              ('Assumptions made', 2, None, 'assumptions-made'),
              ('Expectation value and variance',
               2,
               None,
               'expectation-value-and-variance'),
              ('Expectation value and variance for $\\boldsymbol{\\beta}$',
               2,
               None,
               'expectation-value-and-variance-for-boldsymbol-beta'),
              ('Why resampling methods', 2, None, 'why-resampling-methods'),
              ('Resampling methods', 2, None, 'resampling-methods'),
              ('Resampling approaches can be computationally expensive',
               2,
               None,
               'resampling-approaches-can-be-computationally-expensive'),
              ('Why resampling methods ?', 2, None, 'why-resampling-methods'),
              ('Statistical analysis', 2, None, 'statistical-analysis'),
              ('Resampling methods: Jackknife and Bootstrap',
               2,
               None,
               'resampling-methods-jackknife-and-bootstrap'),
              ('Resampling methods: Jackknife',
               2,
               None,
               'resampling-methods-jackknife'),
              ('Jackknife code example', 2, None, 'jackknife-code-example'),
              ('Resampling methods: Bootstrap',
               2,
               None,
               'resampling-methods-bootstrap'),
              ('Resampling methods: Bootstrap background',
               2,
               None,
               'resampling-methods-bootstrap-background'),
              ('Resampling methods: More Bootstrap background',
               2,
               None,
               'resampling-methods-more-bootstrap-background'),
              ('Resampling methods: Bootstrap approach',
               2,
               None,
               'resampling-methods-bootstrap-approach'),
              ('Resampling methods: Bootstrap steps',
               2,
               None,
               'resampling-methods-bootstrap-steps'),
              ('Code example for the Bootstrap method',
               2,
               None,
               'code-example-for-the-bootstrap-method'),
              ('Various steps in cross-validation',
               2,
               None,
               'various-steps-in-cross-validation'),
              ('Cross-validation in brief',
               2,
               None,
               'cross-validation-in-brief'),
              ('Code Example for Cross-validation and $k$-fold '
               'Cross-validation',
               2,
               None,
               'code-example-for-cross-validation-and-k-fold-cross-validation'),
              ('The bias-variance tradeoff',
               2,
               None,
               'the-bias-variance-tradeoff'),
              ('Overfitting, Training and Test data before bias-variance '
               'trade-off analysis',
               2,
               None,
               'overfitting-training-and-test-data-before-bias-variance-trade-off-analysis'),
              ('Understanding what happens',
               2,
               None,
               'understanding-what-happens'),
              ('Summing up', 2, None, 'summing-up'),
              ("Another Example from Scikit-Learn's Repository",
               2,
               None,
               'another-example-from-scikit-learn-s-repository'),
              ('More examples on bootstrap and cross-validation and errors',
               2,
               None,
               'more-examples-on-bootstrap-and-cross-validation-and-errors'),
              ('The same example but now with cross-validation',
               2,
               None,
               'the-same-example-but-now-with-cross-validation'),
              ('Cross-validation with Ridge',
               2,
               None,
               'cross-validation-with-ridge'),
              ('Exercise 1: making your own data and exploring scikit-learn',
               2,
               None,
               'exercise-1-making-your-own-data-and-exploring-scikit-learn'),
              ('Exercise 2: mean values and variances in linear regression',
               2,
               None,
               'exercise-2-mean-values-and-variances-in-linear-regression'),
              ('Exercise 3: Adding Ridge and Lasso Regression',
               2,
               None,
               'exercise-3-adding-ridge-and-lasso-regression'),
              ('Exercise 4: Normalizing our data',
               2,
               None,
               'exercise-4-normalizing-our-data'),
              ('Exercise 5: Bias-Variance tradeoff and Bootstrap',
               2,
               None,
               'exercise-5-bias-variance-tradeoff-and-bootstrap'),
              ('Part (1a) Proving the bias-variance tradeoff',
               3,
               None,
               'part-1a-proving-the-bias-variance-tradeoff'),
              ('Part (1b) Adding Bootstrap and Bias-Variance Tradeoff',
               3,
               None,
               'part-1b-adding-bootstrap-and-bias-variance-tradeoff'),
              ('Exercise 6: Linear Regression for  a two-dimensional function',
               2,
               None,
               'exercise-6-linear-regression-for-a-two-dimensional-function'),
              ('(a) Ordinary Least Square on the Franke function  with '
               'resampling',
               3,
               None,
               'a-ordinary-least-square-on-the-franke-function-with-resampling'),
              ('Part (b) Resampling techniques, adding more complexity',
               3,
               None,
               'part-b-resampling-techniques-adding-more-complexity'),
              ('Part (c): Bias-variance tradeoff',
               3,
               None,
               'part-c-bias-variance-tradeoff'),
              ('Part (d): Ridge Regression on the Franke function  with '
               'resampling',
               3,
               None,
               'part-d-ridge-regression-on-the-franke-function-with-resampling'),
              ('Part (e): Lasso Regression on the Franke function  with '
               'resampling',
               3,
               None,
               'part-e-lasso-regression-on-the-franke-function-with-resampling')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="Day2-bs.html">Data Analysis and Machine Learning: Day 2, Ridge and Lasso Regression and Resampling Methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#plans-for-day-2" style="font-size: 80%;"><b>Plans for Day 2</b></a></li>
     <!-- navigation toc: --> <li><a href="#linear-regression-basic-overview" style="font-size: 80%;"><b>Linear Regression, basic overview</b></a></li>
     <!-- navigation toc: --> <li><a href="#why-linear-regression-aka-ordinary-least-squares-and-family" style="font-size: 80%;"><b>Why Linear Regression (aka Ordinary Least Squares and family)?</b></a></li>
     <!-- navigation toc: --> <li><a href="#additional-reading" style="font-size: 80%;"><b>Additional Reading</b></a></li>
     <!-- navigation toc: --> <li><a href="#regression-analysis-definitions-and-aims" style="font-size: 80%;"><b>Regression Analysis, Definitions and Aims</b></a></li>
     <!-- navigation toc: --> <li><a href="#regression-analysis-overarching-aims" style="font-size: 80%;"><b>Regression analysis, overarching aims</b></a></li>
     <!-- navigation toc: --> <li><a href="#regression-analysis-overarching-aims-ii" style="font-size: 80%;"><b>Regression analysis, overarching aims II</b></a></li>
     <!-- navigation toc: --> <li><a href="#examples" style="font-size: 80%;"><b>Examples</b></a></li>
     <!-- navigation toc: --> <li><a href="#general-linear-models" style="font-size: 80%;"><b>General linear models</b></a></li>
     <!-- navigation toc: --> <li><a href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem" style="font-size: 80%;"><b>Rewriting the fitting procedure as a linear algebra problem</b></a></li>
     <!-- navigation toc: --> <li><a href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details" style="font-size: 80%;"><b>Rewriting the fitting procedure as a linear algebra problem, more details</b></a></li>
     <!-- navigation toc: --> <li><a href="#generalizing-the-fitting-procedure-as-a-linear-algebra-problem" style="font-size: 80%;"><b>Generalizing the fitting procedure as a linear algebra problem</b></a></li>
     <!-- navigation toc: --> <li><a href="#generalizing-the-fitting-procedure-as-a-linear-algebra-problem" style="font-size: 80%;"><b>Generalizing the fitting procedure as a linear algebra problem</b></a></li>
     <!-- navigation toc: --> <li><a href="#optimizing-our-parameters" style="font-size: 80%;"><b>Optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="#our-model-for-the-nuclear-binding-energies" style="font-size: 80%;"><b>Our model for the nuclear binding energies</b></a></li>
     <!-- navigation toc: --> <li><a href="#optimizing-our-parameters-more-details" style="font-size: 80%;"><b>Optimizing our parameters, more details</b></a></li>
     <!-- navigation toc: --> <li><a href="#interpretations-and-optimizing-our-parameters" style="font-size: 80%;"><b>Interpretations and optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="#interpretations-and-optimizing-our-parameters" style="font-size: 80%;"><b>Interpretations and optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="#some-useful-matrix-and-vector-expressions" style="font-size: 80%;"><b>Some useful matrix and vector expressions</b></a></li>
     <!-- navigation toc: --> <li><a href="#interpretations-and-optimizing-our-parameters" style="font-size: 80%;"><b>Interpretations and optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="#own-code-for-ordinary-least-squares" style="font-size: 80%;"><b>Own code for Ordinary Least Squares</b></a></li>
     <!-- navigation toc: --> <li><a href="#adding-error-analysis-and-training-set-up" style="font-size: 80%;"><b>Adding error analysis and training set up</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="#regression-examples" style="font-size: 80%;"><b>Regression Examples</b></a></li>
     <!-- navigation toc: --> <li><a href="#fitting-an-equation-of-state-for-dense-nuclear-matter" style="font-size: 80%;"><b>Fitting an Equation of State for Dense Nuclear Matter</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-code" style="font-size: 80%;"><b>The code</b></a></li>
     <!-- navigation toc: --> <li><a href="#splitting-our-data-in-training-and-test-data" style="font-size: 80%;"><b>Splitting our Data in Training and Test data</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-boston-housing-data-example" style="font-size: 80%;"><b>The Boston housing data example</b></a></li>
     <!-- navigation toc: --> <li><a href="#housing-data-the-code" style="font-size: 80%;"><b>Housing data, the code</b></a></li>
     <!-- navigation toc: --> <li><a href="#reducing-the-number-of-degrees-of-freedom-overarching-view" style="font-size: 80%;"><b>Reducing the number of degrees of freedom, overarching view</b></a></li>
     <!-- navigation toc: --> <li><a href="#preprocessing-our-data" style="font-size: 80%;"><b>Preprocessing our data</b></a></li>
     <!-- navigation toc: --> <li><a href="#more-preprocessing" style="font-size: 80%;"><b>More preprocessing</b></a></li>
     <!-- navigation toc: --> <li><a href="#simple-preprocessing-examples-franke-function-and-regression" style="font-size: 80%;"><b>Simple preprocessing examples, Franke function and regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#beyond-ordinary-least-squares" style="font-size: 80%;"><b>Beyond Ordinary Least Squares</b></a></li>
     <!-- navigation toc: --> <li><a href="#ridge-and-lasso-regression" style="font-size: 80%;"><b>Ridge and LASSO Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#more-on-ridge-regression" style="font-size: 80%;"><b>More on Ridge Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#simple-ridge-interpretations" style="font-size: 80%;"><b>Simple Ridge  interpretations</b></a></li>
     <!-- navigation toc: --> <li><a href="#statistics" style="font-size: 80%;"><b>Statistics</b></a></li>
     <!-- navigation toc: --> <li><a href="#where-are-we-going" style="font-size: 80%;"><b>Where are we going?</b></a></li>
     <!-- navigation toc: --> <li><a href="#linking-the-regression-analysis-with-a-statistical-interpretation" style="font-size: 80%;"><b>Linking the regression analysis with a statistical interpretation</b></a></li>
     <!-- navigation toc: --> <li><a href="#assumptions-made" style="font-size: 80%;"><b>Assumptions made</b></a></li>
     <!-- navigation toc: --> <li><a href="#expectation-value-and-variance" style="font-size: 80%;"><b>Expectation value and variance</b></a></li>
     <!-- navigation toc: --> <li><a href="#expectation-value-and-variance-for-boldsymbol-beta" style="font-size: 80%;"><b>Expectation value and variance for \( \boldsymbol{\beta} \)</b></a></li>
     <!-- navigation toc: --> <li><a href="#why-resampling-methods" style="font-size: 80%;"><b>Why resampling methods</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods" style="font-size: 80%;"><b>Resampling methods</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-approaches-can-be-computationally-expensive" style="font-size: 80%;"><b>Resampling approaches can be computationally expensive</b></a></li>
     <!-- navigation toc: --> <li><a href="#why-resampling-methods" style="font-size: 80%;"><b>Why resampling methods ?</b></a></li>
     <!-- navigation toc: --> <li><a href="#statistical-analysis" style="font-size: 80%;"><b>Statistical analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-jackknife-and-bootstrap" style="font-size: 80%;"><b>Resampling methods: Jackknife and Bootstrap</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-jackknife" style="font-size: 80%;"><b>Resampling methods: Jackknife</b></a></li>
     <!-- navigation toc: --> <li><a href="#jackknife-code-example" style="font-size: 80%;"><b>Jackknife code example</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-bootstrap" style="font-size: 80%;"><b>Resampling methods: Bootstrap</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-bootstrap-background" style="font-size: 80%;"><b>Resampling methods: Bootstrap background</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-more-bootstrap-background" style="font-size: 80%;"><b>Resampling methods: More Bootstrap background</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-bootstrap-approach" style="font-size: 80%;"><b>Resampling methods: Bootstrap approach</b></a></li>
     <!-- navigation toc: --> <li><a href="#resampling-methods-bootstrap-steps" style="font-size: 80%;"><b>Resampling methods: Bootstrap steps</b></a></li>
     <!-- navigation toc: --> <li><a href="#code-example-for-the-bootstrap-method" style="font-size: 80%;"><b>Code example for the Bootstrap method</b></a></li>
     <!-- navigation toc: --> <li><a href="#various-steps-in-cross-validation" style="font-size: 80%;"><b>Various steps in cross-validation</b></a></li>
     <!-- navigation toc: --> <li><a href="#cross-validation-in-brief" style="font-size: 80%;"><b>Cross-validation in brief</b></a></li>
     <!-- navigation toc: --> <li><a href="#code-example-for-cross-validation-and-k-fold-cross-validation" style="font-size: 80%;"><b>Code Example for Cross-validation and \( k \)-fold Cross-validation</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-bias-variance-tradeoff" style="font-size: 80%;"><b>The bias-variance tradeoff</b></a></li>
     <!-- navigation toc: --> <li><a href="#overfitting-training-and-test-data-before-bias-variance-trade-off-analysis" style="font-size: 80%;"><b>Overfitting, Training and Test data before bias-variance trade-off analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="#understanding-what-happens" style="font-size: 80%;"><b>Understanding what happens</b></a></li>
     <!-- navigation toc: --> <li><a href="#summing-up" style="font-size: 80%;"><b>Summing up</b></a></li>
     <!-- navigation toc: --> <li><a href="#another-example-from-scikit-learn-s-repository" style="font-size: 80%;"><b>Another Example from Scikit-Learn's Repository</b></a></li>
     <!-- navigation toc: --> <li><a href="#more-examples-on-bootstrap-and-cross-validation-and-errors" style="font-size: 80%;"><b>More examples on bootstrap and cross-validation and errors</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-same-example-but-now-with-cross-validation" style="font-size: 80%;"><b>The same example but now with cross-validation</b></a></li>
     <!-- navigation toc: --> <li><a href="#cross-validation-with-ridge" style="font-size: 80%;"><b>Cross-validation with Ridge</b></a></li>
     <!-- navigation toc: --> <li><a href="#exercise-1-making-your-own-data-and-exploring-scikit-learn" style="font-size: 80%;"><b>Exercise 1: making your own data and exploring scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="#exercise-2-mean-values-and-variances-in-linear-regression" style="font-size: 80%;"><b>Exercise 2: mean values and variances in linear regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#exercise-3-adding-ridge-and-lasso-regression" style="font-size: 80%;"><b>Exercise 3: Adding Ridge and Lasso Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#exercise-4-normalizing-our-data" style="font-size: 80%;"><b>Exercise 4: Normalizing our data</b></a></li>
     <!-- navigation toc: --> <li><a href="#exercise-5-bias-variance-tradeoff-and-bootstrap" style="font-size: 80%;"><b>Exercise 5: Bias-Variance tradeoff and Bootstrap</b></a></li>
     <!-- navigation toc: --> <li><a href="#part-1a-proving-the-bias-variance-tradeoff" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (1a) Proving the bias-variance tradeoff</a></li>
     <!-- navigation toc: --> <li><a href="#part-1b-adding-bootstrap-and-bias-variance-tradeoff" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (1b) Adding Bootstrap and Bias-Variance Tradeoff</a></li>
     <!-- navigation toc: --> <li><a href="#exercise-6-linear-regression-for-a-two-dimensional-function" style="font-size: 80%;"><b>Exercise 6: Linear Regression for  a two-dimensional function</b></a></li>
     <!-- navigation toc: --> <li><a href="#a-ordinary-least-square-on-the-franke-function-with-resampling" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;(a) Ordinary Least Square on the Franke function  with resampling</a></li>
     <!-- navigation toc: --> <li><a href="#part-b-resampling-techniques-adding-more-complexity" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (b) Resampling techniques, adding more complexity</a></li>
     <!-- navigation toc: --> <li><a href="#part-c-bias-variance-tradeoff" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (c): Bias-variance tradeoff</a></li>
     <!-- navigation toc: --> <li><a href="#part-d-ridge-regression-on-the-franke-function-with-resampling" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (d): Ridge Regression on the Franke function  with resampling</a></li>
     <!-- navigation toc: --> <li><a href="#part-e-lasso-regression-on-the-franke-function-with-resampling" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (e): Lasso Regression on the Franke function  with resampling</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Data Analysis and Machine Learning: Day 2, Ridge and Lasso Regression and Resampling Methods</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b></center>
<center>[2] <b>Department of Physics and Astronomy and Facility for Rare Ion Beams and National Superconducting Cyclotron Laboratory, Michigan State University, USA</b></center>
<br>
<p>
<center><h4>Feb 17, 2021</h4></center> <!-- date -->
<br>
<p>
<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->

<h2 id="plans-for-day-2" class="anchor">Plans for Day 2 </h2>

<ul>
<li> Basics of linear regression</li>
<li> Statistics, probability theory and resampling methods</li>
<li> Shrinkage methods: Ridge and Lasso Regression</li>
</ul>

<!-- !split -->

<h2 id="linear-regression-basic-overview" class="anchor">Linear Regression, basic overview </h2>

<p>
The aim of this set of lectures is to introduce basic aspects of linear regression, a widely applied set of methods used to fit continuous functions.

<p>
We will also use these widely popular methods to introduce resampling techniques like bootstrapping and cross-validation.

<p>
We will in particular focus on

<p>
<!-- !bpop -->

<ul>
<li> Ordinary linear regression</li>
<li> Ridge regression</li>
<li> Lasso regression</li>
<li> Resampling techniques</li>
<li> Bias-variance tradeoff</li>
</ul>

<!-- !epop -->

<p>
<!-- !split -->

<h2 id="why-linear-regression-aka-ordinary-least-squares-and-family" class="anchor">Why Linear Regression (aka Ordinary Least Squares and family)? </h2>

<p>
Fitting a continuous function with linear parameterization in terms of the parameters  \( \boldsymbol{\beta} \).
<!-- !bpop -->

<ul>
<li> Method of choice for fitting a continuous function!</li>
<li> Gives an excellent introduction to central Machine Learning features with <b>understandable pedagogical</b> links to other methods like <b>Neural Networks</b>, <b>Support Vector Machines</b> etc</li>
<li> Analytical expression for the fitting parameters \( \boldsymbol{\beta} \)</li>
<li> Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more</li>
<li> Analytical relation with probabilistic interpretations</li> 
<li> Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics</li>
<li> Easy to code! And links well with classification problems and logistic regression and neural networks</li>
<li> Allows for <b>easy</b> hands-on understanding of gradient descent methods. These methods are at the heart of all essentially all Machine Learning methods.</li>
<li> and many more features</li>
</ul>

<!-- !epop -->

<p>
<!-- !split -->

<h2 id="additional-reading" class="anchor">Additional Reading  </h2>

<p>
For more discussions of Ridge and Lasso regression, <a href="https://arxiv.org/abs/1509.09169" target="_self">Wessel van Wieringen's</a> article is highly recommended.
Similarly, <a href="https://arxiv.org/abs/1803.08823" target="_self">Mehta et al's article</a> is also recommended. The textbook by <a href="https://link.springer.com/book/10.1007/978-0-387-84858-7" target="_self">Hastie, Tibshirani, and Friedman on The Elements of Statistical Learning Data Mining</a>, chapter 3 is highly recommended.  
Also <a href="https://www.springer.com/gp/book/9780387310732" target="_self">Bishop's text</a>, chapter 3 is an excellent read.

<p>
<!-- !split -->

<h2 id="regression-analysis-definitions-and-aims" class="anchor">Regression Analysis, Definitions and Aims  </h2>

<p>
<!-- !split -->

<h2 id="regression-analysis-overarching-aims" class="anchor">Regression analysis, overarching aims  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Regression modeling deals with the description of  the sampling distribution of a given random variable \( y \) and how it varies as function of another variable or a set of such variables \( \boldsymbol{x} =[x_0, x_1,\dots, x_{n-1}]^T \). 
The first variable is called the <b>dependent</b>, the <b>outcome</b> or the <b>response</b> variable while the set of variables \( \boldsymbol{x} \) is called the independent variable, or the predictor variable or the explanatory variable.

<p>
A regression model aims at finding a likelihood function \( p(\boldsymbol{y}\vert \boldsymbol{x}) \), that is the conditional distribution for \( \boldsymbol{y} \) with a given \( \boldsymbol{x} \). The estimation of  \( p(\boldsymbol{y}\vert \boldsymbol{x}) \) is made using a data set with 

<ul>
<li> \( n \) cases \( i = 0, 1, 2, \dots, n-1 \)</li> 
<li> Response (target, dependent or outcome) variable \( y_i \) with \( i = 0, 1, 2, \dots, n-1 \)</li> 
<li> \( p \) so-called explanatory (independent or predictor) variables \( \boldsymbol{x}_i=[x_{i0}, x_{i1}, \dots, x_{ip-1}] \) with \( i = 0, 1, 2, \dots, n-1 \) and explanatory variables running from \( 0 \) to \( p-1 \). See below for more explicit examples.   These variables are also called features or predictors.</li>
</ul>

 The goal of the regression analysis is to extract/exploit relationship between \( \boldsymbol{y} \) and \( \boldsymbol{x} \) in or to infer causal dependencies, approximations to the likelihood functions, functional relationships and to make predictions, making fits and many other things.
</div>
</div>


<p>
<!-- !split -->

<h2 id="regression-analysis-overarching-aims-ii" class="anchor">Regression analysis, overarching aims II </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Consider an experiment in which \( p \) characteristics of \( n \) samples are
measured. The data from this experiment, for various explanatory variables \( p \) are normally represented by a matrix  
\( \mathbf{X} \).

<p>
The matrix \( \mathbf{X} \) is called the <em>design
matrix</em>. Additional information of the samples is available in the
form of \( \boldsymbol{y} \) (also as above). The variable \( \boldsymbol{y} \) is
generally referred to as the <em>response variable</em>. The aim of
regression analysis is to explain \( \boldsymbol{y} \) in terms of
\( \boldsymbol{X} \) through a functional relationship like \( y_i =
f(\mathbf{X}_{i,\ast}) \). When no prior knowledge on the form of
\( f(\cdot) \) is available, it is common to assume a linear relationship
between \( \boldsymbol{X} \) and \( \boldsymbol{y} \). This assumption gives rise to
the <em>linear regression model</em> where \( \boldsymbol{\beta} = [\beta_0, \ldots,
\beta_{p-1}]^{T} \) are the <em>regression parameters</em>.

<p>
Linear regression gives us a set of analytical equations for the parameters \( \beta_j \).

<p>
<b>Note</b>: The optimal values of the parameters \( \boldsymbol{\beta} \) are obtained by minimizing a chosen <b>cost/risk/loss</b> function. We will label these
as \( \boldsymbol{\hat{\beta}} \) or as \( \boldsymbol{\beta}^{\mathrm{opt}} \).

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="examples" class="anchor">Examples  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
In order to understand the relation among the predictors \( p \), the set of data \( n \) and the target (outcome, output etc) \( \boldsymbol{y} \),
consider the model we discussed for describing nuclear binding energies.

<p>
There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.
Assuming 
$$
BE(A) = a_0+a_1A+a_2A^{2/3}+a_3A^{-1/3}+a_4A^{-1},
$$

we have five predictors, that is the intercept, the \( A \) dependent term, the \( A^{2/3} \) term and the \( A^{-1/3} \) and \( A^{-1} \) terms.
This gives \( p=0,1,2,3,4 \). Furthermore we have \( n \) entries for each predictor. It means that our design matrix is an 
\( n\times p \) matrix \( \boldsymbol{X} \).

<p>
Here the predictors are based on a model we have made. A popular data set which is widely encountered in ML applications is the
so-called <a href="https://www.sciencedirect.com/science/article/pii/S0957417407006719?via%3Dihub" target="_self">credit card default data from Taiwan</a>. The data set contains data on \( n=30000 \) credit card holders with predictors like gender, marital status, age, profession, education, etc. In total there are \( 24 \) such predictors or attributes leading to a design matrix of dimensionality \( 24 \times 30000 \). This is however a classification problem and we will come back to it when we discuss Logistic Regression.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="general-linear-models" class="anchor">General linear models  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
Before we proceed let us study a case from linear algebra where we aim at fitting a set of data \( \boldsymbol{y}=[y_0,y_1,\dots,y_{n-1}] \). We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a series of variables \( \boldsymbol{x}=[x_0,x_1,\dots,x_{n-1}] \), that is \( y_i = y(x_i) \) with \( i=0,1,2,\dots,n-1 \). The variables \( x_i \) could represent physical quantities like time, temperature, position etc. We assume that \( y(x) \) is a smooth function.

<p>
Since obtaining these data points may not be trivial, we want to use these data to fit a function which can allow us to make predictions for values of \( y \) which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial of degree \( n-1 \) with \( n \) points, that is
$$
y=y(x) \rightarrow y(x_i)=\tilde{y}_i+\epsilon_i=\sum_{j=0}^{n-1} \beta_j x_i^j+\epsilon_i,
$$

where \( \epsilon_i \) is the error in our approximation.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="rewriting-the-fitting-procedure-as-a-linear-algebra-problem" class="anchor">Rewriting the fitting procedure as a linear algebra problem  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
For every set of values \( y_i,x_i \) we have thus the corresponding set of equations
$$
\begin{align*}
y_0&=\beta_0+\beta_1x_0^1+\beta_2x_0^2+\dots+\beta_{n-1}x_0^{n-1}+\epsilon_0\\
y_1&=\beta_0+\beta_1x_1^1+\beta_2x_1^2+\dots+\beta_{n-1}x_1^{n-1}+\epsilon_1\\
y_2&=\beta_0+\beta_1x_2^1+\beta_2x_2^2+\dots+\beta_{n-1}x_2^{n-1}+\epsilon_2\\
\dots & \dots \\
y_{n-1}&=\beta_0+\beta_1x_{n-1}^1+\beta_2x_{n-1}^2+\dots+\beta_{n-1}x_{n-1}^{n-1}+\epsilon_{n-1}.\\
\end{align*}
$$
</div>
</div>


<p>
<!-- !split -->

<h2 id="rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details" class="anchor">Rewriting the fitting procedure as a linear algebra problem, more details  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
Defining the vectors
$$
\boldsymbol{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,
$$

and
$$
\boldsymbol{\beta} = [\beta_0,\beta_1, \beta_2,\dots, \beta_{n-1}]^T,
$$

and
$$
\boldsymbol{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,
$$

and the design matrix
$$
\boldsymbol{X}=
\begin{bmatrix} 
1& x_{0}^1 &x_{0}^2& \dots & \dots &x_{0}^{n-1}\\
1& x_{1}^1 &x_{1}^2& \dots & \dots &x_{1}^{n-1}\\
1& x_{2}^1 &x_{2}^2& \dots & \dots &x_{2}^{n-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
1& x_{n-1}^1 &x_{n-1}^2& \dots & \dots &x_{n-1}^{n-1}\\
\end{bmatrix} 
$$

we can rewrite our equations as
$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}.
$$

The above design matrix is called a <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix" target="_self">Vandermonde matrix</a>.
</div>
</div>


<p>
<!-- !split -->

<h2 id="generalizing-the-fitting-procedure-as-a-linear-algebra-problem" class="anchor">Generalizing the fitting procedure as a linear algebra problem  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
We are obviously not limited to the above polynomial expansions.  We
could replace the various powers of \( x \) with elements of Fourier
series or instead of \( x_i^j \) we could have \( \cos{(j x_i)} \) or \( \sin{(j
x_i)} \), or time series or other orthogonal functions.  For every set
of values \( y_i,x_i \) we can then generalize the equations to

$$
\begin{align*}
y_0&=\beta_0x_{00}+\beta_1x_{01}+\beta_2x_{02}+\dots+\beta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&=\beta_0x_{10}+\beta_1x_{11}+\beta_2x_{12}+\dots+\beta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&=\beta_0x_{20}+\beta_1x_{21}+\beta_2x_{22}+\dots+\beta_{n-1}x_{2n-1}+\epsilon_2\\
\dots & \dots \\
y_{i}&=\beta_0x_{i0}+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{n-1}x_{in-1}+\epsilon_i\\
\dots & \dots \\
y_{n-1}&=\beta_0x_{n-1,0}+\beta_1x_{n-1,2}+\beta_2x_{n-1,2}+\dots+\beta_{n-1}x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
$$

<p>
Note that we have used \( p=n \) here. The matrix is thus quadratic (it may be symmetric). This is generally not the case!
</div>
</div>


<p>
<!-- !split -->

<h2 id="generalizing-the-fitting-procedure-as-a-linear-algebra-problem" class="anchor">Generalizing the fitting procedure as a linear algebra problem  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We redefine in turn the matrix \( \boldsymbol{X} \) as
$$
\boldsymbol{X}=
\begin{bmatrix} 
x_{00}& x_{01} &x_{02}& \dots & \dots &x_{0,n-1}\\
x_{10}& x_{11} &x_{12}& \dots & \dots &x_{1,n-1}\\
x_{20}& x_{21} &x_{22}& \dots & \dots &x_{2,n-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
x_{n-1,0}& x_{n-1,1} &x_{n-1,2}& \dots & \dots &x_{n-1,n-1}\\
\end{bmatrix} 
$$

and without loss of generality we rewrite again  our equations as
$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}.
$$

The left-hand side of this equation is kwown. Our error vector \( \boldsymbol{\epsilon} \) and the parameter vector \( \boldsymbol{\beta} \) are our unknown quantities. How can we obtain the optimal set of \( \beta_i \) values?
</div>
</div>


<p>
<!-- !split -->

<h2 id="optimizing-our-parameters" class="anchor">Optimizing our parameters  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We have defined the matrix \( \boldsymbol{X} \) via the equations
$$
\begin{align*}
y_0&=\beta_0x_{00}+\beta_1x_{01}+\beta_2x_{02}+\dots+\beta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&=\beta_0x_{10}+\beta_1x_{11}+\beta_2x_{12}+\dots+\beta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&=\beta_0x_{20}+\beta_1x_{21}+\beta_2x_{22}+\dots+\beta_{n-1}x_{2n-1}+\epsilon_1\\
\dots & \dots \\
y_{i}&=\beta_0x_{i0}+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{n-1}x_{in-1}+\epsilon_1\\
\dots & \dots \\
y_{n-1}&=\beta_0x_{n-1,0}+\beta_1x_{n-1,2}+\beta_2x_{n-1,2}+\dots+\beta_{n-1}x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
$$

<p>
As we noted above, we stayed with a system with the design matrix 
 \( \boldsymbol{X}\in {\mathbb{R}}^{n\times n} \), that is we have \( p=n \). For reasons to come later (algorithmic arguments) we will hereafter define 
our matrix as \( \boldsymbol{X}\in {\mathbb{R}}^{n\times p} \), with the predictors refering to the column numbers and the entries \( n \) being the row elements.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="our-model-for-the-nuclear-binding-energies" class="anchor">Our model for the nuclear binding energies </h2>

<p>
In our <a href="https://compphysics.github.io/MachineLearning/doc/pub/How2ReadData/html/How2ReadData.html" target="_self">introductory notes</a> we looked at the so-called <a href="https://en.wikipedia.org/wiki/Semi-empirical_mass_formula" target="_self">liquid drop model</a>. Let us remind ourselves about what we did by looking at the code.

<p>
We restate the parts of the code we are most interested in.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> display
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;MassEval2016.dat&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)


<span style="color: #408080; font-style: italic"># Read the experimental data with Pandas</span>
Masses <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_fwf(infile, usecols<span style="color: #666666">=</span>(<span style="color: #666666">2</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">6</span>,<span style="color: #666666">11</span>),
              names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;N&#39;</span>, <span style="color: #BA2121">&#39;Z&#39;</span>, <span style="color: #BA2121">&#39;A&#39;</span>, <span style="color: #BA2121">&#39;Element&#39;</span>, <span style="color: #BA2121">&#39;Ebinding&#39;</span>),
              widths<span style="color: #666666">=</span>(<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">1</span>,<span style="color: #666666">13</span>,<span style="color: #666666">11</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">1</span>,<span style="color: #666666">12</span>,<span style="color: #666666">11</span>,<span style="color: #666666">1</span>),
              header<span style="color: #666666">=39</span>,
              index_col<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so</span>
<span style="color: #408080; font-style: italic"># the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.</span>
Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Convert from keV to MeV.</span>
Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>] <span style="color: #666666">/=</span> <span style="color: #666666">1000</span>

<span style="color: #408080; font-style: italic"># Group the DataFrame by nucleon number, A.</span>
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>groupby(<span style="color: #BA2121">&#39;A&#39;</span>)
<span style="color: #408080; font-style: italic"># Find the rows of the grouped DataFrame with the maximum binding energy.</span>
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>apply(<span style="color: #008000; font-weight: bold">lambda</span> t: t[t<span style="color: #666666">.</span>Ebinding<span style="color: #666666">==</span>t<span style="color: #666666">.</span>Ebinding<span style="color: #666666">.</span>max()])
A <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;A&#39;</span>]
Z <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Z&#39;</span>]
N <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;N&#39;</span>]
Element <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Element&#39;</span>]
Energies <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>]

<span style="color: #408080; font-style: italic"># Now we set up the design matrix X</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(A),<span style="color: #666666">5</span>))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> A
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> A<span style="color: #666666">**</span>(<span style="color: #666666">2.0/3.0</span>)
X[:,<span style="color: #666666">3</span>] <span style="color: #666666">=</span> A<span style="color: #666666">**</span>(<span style="color: #666666">-1.0/3.0</span>)
X[:,<span style="color: #666666">4</span>] <span style="color: #666666">=</span> A<span style="color: #666666">**</span>(<span style="color: #666666">-1.0</span>)
<span style="color: #408080; font-style: italic"># Then nice printout using pandas</span>
DesignMatrix <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(X)
DesignMatrix<span style="color: #666666">.</span>index <span style="color: #666666">=</span> A
DesignMatrix<span style="color: #666666">.</span>columns <span style="color: #666666">=</span> [<span style="color: #BA2121">&#39;1&#39;</span>, <span style="color: #BA2121">&#39;A&#39;</span>, <span style="color: #BA2121">&#39;A^(2/3)&#39;</span>, <span style="color: #BA2121">&#39;A^(-1/3)&#39;</span>, <span style="color: #BA2121">&#39;1/A&#39;</span>]
display(DesignMatrix)
</pre></div>
<p>
With \( \boldsymbol{\beta}\in {\mathbb{R}}^{p\times 1} \), it means that we will hereafter write our equations for the approximation as
$$
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\beta},
$$

throughout these lectures.

<p>
<!-- !split -->

<h2 id="optimizing-our-parameters-more-details" class="anchor">Optimizing our parameters, more details  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
With the above we use the design matrix to define the approximation \( \boldsymbol{\tilde{y}} \) via the unknown quantity \( \boldsymbol{\beta} \) as
$$
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\beta},
$$

and in order to find the optimal parameters \( \beta_i \) instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values \( y_i \) (which represent hopefully the exact values) and the parameterized values \( \tilde{y}_i \), namely
$$
C(\boldsymbol{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
$$

or using the matrix \( \boldsymbol{X} \) and in a more compact matrix-vector notation as
$$
C(\boldsymbol{\beta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right\}.
$$

This function is one possible way to define the so-called cost function.

<p>
It is also common to define
the function \( C \) as

$$
C(\boldsymbol{\beta})=\frac{1}{2n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2,
$$

since when taking the first derivative with respect to the unknown parameters \( \beta \), the factor of \( 2 \) cancels out.
</div>
</div>


<p>
<!-- !split -->

<h2 id="interpretations-and-optimizing-our-parameters" class="anchor">Interpretations and optimizing our parameters  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
The function 
$$
C(\boldsymbol{\beta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right\},
$$

can be linked to the variance of the quantity \( y_i \) if we interpret the latter as the mean value.
Below we will show that
$$
y_{i}=\langle y_i \rangle = \beta_0x_{i,0}+\beta_1x_{i,1}+\beta_2x_{i,2}+\dots+\beta_{n-1}x_{i,n-1}+\epsilon_i,
$$

<p>
where \( \langle y_i \rangle \) is the mean value. Keep in mind also that
till now we have treated \( y_i \) as the exact value. Normally, the
response (dependent or outcome) variable \( y_i \) the outcome of a
numerical experiment or another type of experiment and is thus only an
approximation to the true value. It is then always accompanied by an
error estimate, often limited to a statistical error estimate given by
the standard deviation discussed earlier. In the discussion here we
will treat \( y_i \) as our exact value for the response variable.

<p>
In order to find the parameters \( \beta_i \) we will then minimize the spread of \( C(\boldsymbol{\beta}) \), that is we are going to solve the problem
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right\}.
$$

In practical terms it means we will require
$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \beta_j} = \frac{\partial }{\partial \beta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}\right)^2\right]=0, 
$$

which results in
$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \beta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}\right)\right]=0, 
$$

or in a matrix-vector form as
$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right).  
$$

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="interpretations-and-optimizing-our-parameters" class="anchor">Interpretations and optimizing our parameters  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We can rewrite
$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right),  
$$

as
$$
\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta},  
$$

and if the matrix \( \boldsymbol{X}^T\boldsymbol{X} \) is invertible we have the solution
$$
\boldsymbol{\beta} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$

<p>
We note also that since our design matrix is defined as \( \boldsymbol{X}\in
{\mathbb{R}}^{n\times p} \), the product \( \boldsymbol{X}^T\boldsymbol{X} \in
{\mathbb{R}}^{p\times p} \).  In the above case we have that \( p \ll n \),
in our case \( p=5 \) meaning that we end up with inverting a small
\( 5\times 5 \) matrix. This is a rather common situation, in many cases we end up with low-dimensional
matrices to invert. The methods discussed here and for many other
supervised learning algorithms like classification with logistic
regression or support vector machines, exhibit dimensionalities which
allow for the usage of direct linear algebra methods such as <b>LU</b> decomposition or <b>Singular Value Decomposition</b> (SVD) for finding the inverse of the matrix
\( \boldsymbol{X}^T\boldsymbol{X} \).
</div>
</div>


<p>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<b>Small question</b>: Do you think the example we have at hand here (the nuclear binding energies) can lead to problems in inverting the matrix  \( \boldsymbol{X}^T\boldsymbol{X} \)? What kind of problems can we expect?
</div>
</div>


<p>
<!-- !split -->

<h2 id="some-useful-matrix-and-vector-expressions" class="anchor">Some useful matrix and vector expressions </h2>

<p>
The following matrix and vector relation will be useful here and for the rest of the course. Vectors are always written as boldfaced lower case letters and 
matrices as upper case boldfaced letters. Here we list some useful expressions
$$
\frac{\partial (\boldsymbol{b}^T\boldsymbol{a})}{\partial \boldsymbol{a}} = \boldsymbol{b},
$$

$$
\frac{\partial (\boldsymbol{a}^T\boldsymbol{A}\boldsymbol{a})}{\partial \boldsymbol{a}} = (\boldsymbol{A}+\boldsymbol{A}^T)\boldsymbol{a},
$$

$$
\frac{\partial tr(\boldsymbol{B}\boldsymbol{A})}{\partial \boldsymbol{A}} = \boldsymbol{B}^T,
$$

$$
\frac{\partial \log{\vert\boldsymbol{A}\vert}}{\partial \boldsymbol{A}} = (\boldsymbol{A}^{-1})^T.
$$

<p>
<!-- !split -->

<h2 id="interpretations-and-optimizing-our-parameters" class="anchor">Interpretations and optimizing our parameters  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The residuals \( \boldsymbol{\epsilon} \) are in turn given by
$$
\boldsymbol{\epsilon} = \boldsymbol{y}-\boldsymbol{\tilde{y}} = \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta},
$$

and with 
$$
\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)= 0, 
$$

we have
$$
\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)= 0, 
$$

meaning that the solution for \( \boldsymbol{\beta} \) is the one which minimizes the residuals.  Later we will link this with the maximum likelihood approach.

<p>
</div>
</div>


<p>
Let us now return to our nuclear binding energies and simply code the above equations.

<p>
<!-- !split -->

<h2 id="own-code-for-ordinary-least-squares" class="anchor">Own code for Ordinary Least Squares </h2>

<p>
It is rather straightforward to implement the matrix inversion and obtain the parameters \( \boldsymbol{\beta} \). After having defined the matrix \( \boldsymbol{X} \) we simply need to 
write
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
beta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X) <span style="color: #666666">@</span> X<span style="color: #666666">.</span>T <span style="color: #666666">@</span> Energies
<span style="color: #408080; font-style: italic"># or in a more old-fashioned way</span>
<span style="color: #408080; font-style: italic"># beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Energies)</span>
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytilde <span style="color: #666666">=</span> X <span style="color: #666666">@</span> beta
</pre></div>
<p>
Alternatively, you can use the least squares functionality in <b>Numpy</b> as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>fit <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>lstsq(X, Energies, rcond <span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>)[<span style="color: #666666">0</span>]
ytildenp <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(fit,X<span style="color: #666666">.</span>T)
</pre></div>
<p>
And finally we plot our fit with and compare with data
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>Masses[<span style="color: #BA2121">&#39;Eapprox&#39;</span>]  <span style="color: #666666">=</span> ytilde
<span style="color: #408080; font-style: italic"># Generate a plot comparing the experimental with the fitted values values.</span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">r&#39;$A = N + Z$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">r&#39;$E_\mathrm</span><span style="color: #BB6688; font-weight: bold">{bind}</span><span style="color: #BA2121">\,/\mathrm</span><span style="color: #BB6688; font-weight: bold">{MeV}</span><span style="color: #BA2121">$&#39;</span>)
ax<span style="color: #666666">.</span>plot(Masses[<span style="color: #BA2121">&#39;A&#39;</span>], Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Ame2016&#39;</span>)
ax<span style="color: #666666">.</span>plot(Masses[<span style="color: #BA2121">&#39;A&#39;</span>], Masses[<span style="color: #BA2121">&#39;Eapprox&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;m&#39;</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Fit&#39;</span>)
ax<span style="color: #666666">.</span>legend()
save_fig(<span style="color: #BA2121">&quot;Masses2016OLS&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="adding-error-analysis-and-training-set-up" class="anchor">Adding error analysis and training set up </h2>

<p>
We can easily test our fit by computing the \( R2 \) score that we discussed in connection with the functionality of <b>Scikit-Learn</b> in the introductory slides.
Since we are not using <b>Scikit-Learn</b> here we can define our own \( R2 \) function as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
</pre></div>
<p>
and we would be using it as 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000">print</span>(R2(Energies,ytilde))
</pre></div>
<p>
We can also add our <b>MSE</b> score as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

<span style="color: #008000">print</span>(MSE(Energies,ytilde))
</pre></div>
<p>
and finally the relative error as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">RelativeError</span>(y_data,y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">abs</span>((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">/</span>y_data)
<span style="color: #008000">print</span>(RelativeError(Energies, ytilde))
</pre></div>
<p>
<!-- !split -->

<h2 id="the-chi-2-function" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Normally, the response (dependent or outcome) variable \( y_i \) is the
outcome of a numerical experiment or another type of experiment and is
thus only an approximation to the true value. It is then always
accompanied by an error estimate, often limited to a statistical error
estimate given by the standard deviation discussed earlier. In the
discussion here we will treat \( y_i \) as our exact value for the
response variable.

<p>
Introducing the standard deviation \( \sigma_i \) for each measurement
\( y_i \), we define now the \( \chi^2 \) function (omitting the \( 1/n \) term)
as

$$
\chi^2(\boldsymbol{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\frac{1}{\boldsymbol{\Sigma^2}}\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
$$

where the matrix \( \boldsymbol{\Sigma} \) is a diagonal matrix with \( \sigma_i \) as matrix elements.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="the-chi-2-function" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
In order to find the parameters \( \beta_i \) we will then minimize the spread of \( \chi^2(\boldsymbol{\beta}) \) by requiring
$$
\frac{\partial \chi^2(\boldsymbol{\beta})}{\partial \beta_j} = \frac{\partial }{\partial \beta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0, 
$$

which results in
$$
\frac{\partial \chi^2(\boldsymbol{\beta})}{\partial \beta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0, 
$$

or in a matrix-vector form as
$$
\frac{\partial \chi^2(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\beta}\right).  
$$

where we have defined the matrix \( \boldsymbol{A} =\boldsymbol{X}/\boldsymbol{\Sigma} \) with matrix elements \( a_{ij} = x_{ij}/\sigma_i \) and the vector \( \boldsymbol{b} \) with elements \( b_i = y_i/\sigma_i \).
</div>
</div>


<p>
<!-- !split -->

<h2 id="the-chi-2-function" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
We can rewrite
$$
\frac{\partial \chi^2(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\beta}\right),  
$$

as
$$
\boldsymbol{A}^T\boldsymbol{b} = \boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\beta},  
$$

and if the matrix \( \boldsymbol{A}^T\boldsymbol{A} \) is invertible we have the solution
$$
\boldsymbol{\beta} =\left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1}\boldsymbol{A}^T\boldsymbol{b}.
$$
</div>
</div>


<p>
<!-- !split -->

<h2 id="the-chi-2-function" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
If we then introduce the matrix
$$
\boldsymbol{H} =  \left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1},
$$

we have then the following expression for the parameters \( \beta_j \) (the matrix elements of \( \boldsymbol{H} \) are \( h_{ij} \))
$$
\beta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}
$$

We state without proof the expression for the uncertainty  in the parameters \( \beta_j \) as (we leave this as an exercise)
$$
\sigma^2(\beta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \beta_j}{\partial y_i}\right)^2, 
$$

resulting in 
$$
\sigma^2(\beta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!
$$
</div>
</div>


<p>
<!-- !split -->

<h2 id="the-chi-2-function" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The first step here is to approximate the function \( y \) with a first-order polynomial, that is we write
$$
y=y(x) \rightarrow y(x_i) \approx \beta_0+\beta_1 x_i.
$$

By computing the derivatives of \( \chi^2 \) with respect to \( \beta_0 \) and \( \beta_1 \) show that these are given by
$$
\frac{\partial \chi^2(\boldsymbol{\beta})}{\partial \beta_0} = -2\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\beta_0-\beta_1x_{i}}{\sigma_i^2}\right)\right]=0,
$$

and
$$
\frac{\partial \chi^2(\boldsymbol{\beta})}{\partial \beta_1} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_i\left(\frac{y_i-\beta_0-\beta_1x_{i}}{\sigma_i^2}\right)\right]=0.
$$
</div>
</div>


<p>
<!-- !split -->

<h2 id="the-chi-2-function" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
For a linear fit (a first-order polynomial) we don't need to invert a matrix!!  
Defining
$$
\gamma =  \sum_{i=0}^{n-1}\frac{1}{\sigma_i^2},
$$


$$
\gamma_x =  \sum_{i=0}^{n-1}\frac{x_{i}}{\sigma_i^2},
$$


$$
\gamma_y = \sum_{i=0}^{n-1}\left(\frac{y_i}{\sigma_i^2}\right),
$$


$$
\gamma_{xx} =  \sum_{i=0}^{n-1}\frac{x_ix_{i}}{\sigma_i^2},
$$


$$
\gamma_{xy} = \sum_{i=0}^{n-1}\frac{y_ix_{i}}{\sigma_i^2},
$$

<p>
we obtain

$$
\beta_0 = \frac{\gamma_{xx}\gamma_y-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2},
$$


$$
\beta_1 = \frac{\gamma_{xy}\gamma-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2}.
$$

<p>
This approach (different linear and non-linear regression) suffers
often from both being underdetermined and overdetermined in the
unknown coefficients \( \beta_i \).  A better approach is to use the
Singular Value Decomposition (SVD) method discussed below. Or using
Lasso and Ridge regression. See below.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="regression-examples" class="anchor">Regression Examples </h2>

<p>
<!-- !split -->

<h2 id="fitting-an-equation-of-state-for-dense-nuclear-matter" class="anchor">Fitting an Equation of State for Dense Nuclear Matter </h2>

<p>
Before we continue, let us introduce yet another example. We are going to fit the
nuclear equation of state using results from many-body calculations.
The equation of state we have made available here, as function of
density, has been derived using modern nucleon-nucleon potentials with
<a href="https://www.sciencedirect.com/science/article/pii/S0370157399001106" target="_self">the addition of three-body
interactions</a>. This
time the file is presented as a standard <b>csv</b> file.

<p>
The beginning of the Python code here is similar to what you have seen
before, with the same initializations and declarations. We use also
<b>pandas</b> again, rather extensively in order to organize our data.

<p>
The difference now is that we use <b>Scikit-Learn's</b> regression tools
instead of our own matrix inversion implementation. Furthermore, we
sneak in <b>Ridge</b> regression (to be discussed below) which includes a
hyperparameter \( \lambda \), also to be explained below.

<p>
<!-- !split -->

<h2 id="the-code" class="anchor">The code </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skl</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error, r2_score, mean_absolute_error

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;EoS.csv&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)

<span style="color: #408080; font-style: italic"># Read the EoS data as  csv file and organize the data into two arrays with density and energies</span>
EoS <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_csv(infile, names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;Density&#39;</span>, <span style="color: #BA2121">&#39;Energy&#39;</span>))
EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
EoS <span style="color: #666666">=</span> EoS<span style="color: #666666">.</span>dropna()
Energies <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>]
Density <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Density&#39;</span>]
<span style="color: #408080; font-style: italic">#  The design matrix now as function of various polytrops</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(Density),<span style="color: #666666">4</span>))
X[:,<span style="color: #666666">3</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">4.0/3.0</span>)
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> Density
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">2.0/3.0</span>)
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1</span>

<span style="color: #408080; font-style: italic"># We use now Scikit-Learn&#39;s linear regressor and ridge regressor</span>
<span style="color: #408080; font-style: italic"># OLS part</span>
clf <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>LinearRegression()<span style="color: #666666">.</span>fit(X, Energies)
ytilde <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X)
EoS[<span style="color: #BA2121">&#39;Eols&#39;</span>]  <span style="color: #666666">=</span> ytilde
<span style="color: #408080; font-style: italic"># The mean squared error                               </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Mean squared error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> mean_squared_error(Energies, ytilde))
<span style="color: #408080; font-style: italic"># Explained variance score: 1 is perfect prediction                                 </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Variance score: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> r2_score(Energies, ytilde))
<span style="color: #408080; font-style: italic"># Mean absolute error                                                           </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Mean absolute error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_absolute_error(Energies, ytilde))
<span style="color: #008000">print</span>(clf<span style="color: #666666">.</span>coef_, clf<span style="color: #666666">.</span>intercept_)

<span style="color: #408080; font-style: italic"># The Ridge regression with a hyperparameter lambda = 0.1</span>
_lambda <span style="color: #666666">=</span> <span style="color: #666666">0.1</span>
clf_ridge <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>Ridge(alpha<span style="color: #666666">=</span>_lambda)<span style="color: #666666">.</span>fit(X, Energies)
yridge <span style="color: #666666">=</span> clf_ridge<span style="color: #666666">.</span>predict(X)
EoS[<span style="color: #BA2121">&#39;Eridge&#39;</span>]  <span style="color: #666666">=</span> yridge
<span style="color: #408080; font-style: italic"># The mean squared error                               </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Mean squared error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> mean_squared_error(Energies, yridge))
<span style="color: #408080; font-style: italic"># Explained variance score: 1 is perfect prediction                                 </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Variance score: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> r2_score(Energies, yridge))
<span style="color: #408080; font-style: italic"># Mean absolute error                                                           </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Mean absolute error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_absolute_error(Energies, yridge))
<span style="color: #008000">print</span>(clf_ridge<span style="color: #666666">.</span>coef_, clf_ridge<span style="color: #666666">.</span>intercept_)

fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">r&#39;$\rho[\mathrm</span><span style="color: #BB6688; font-weight: bold">{fm}</span><span style="color: #BA2121">^{-3}]$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">r&#39;Energy per particle&#39;</span>)
ax<span style="color: #666666">.</span>plot(EoS[<span style="color: #BA2121">&#39;Density&#39;</span>], EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Theoretical data&#39;</span>)
ax<span style="color: #666666">.</span>plot(EoS[<span style="color: #BA2121">&#39;Density&#39;</span>], EoS[<span style="color: #BA2121">&#39;Eols&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;m&#39;</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;OLS&#39;</span>)
ax<span style="color: #666666">.</span>plot(EoS[<span style="color: #BA2121">&#39;Density&#39;</span>], EoS[<span style="color: #BA2121">&#39;Eridge&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;g&#39;</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Ridge $\lambda = 0.1$&#39;</span>)
ax<span style="color: #666666">.</span>legend()
save_fig(<span style="color: #BA2121">&quot;EoSfitting&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
The above simple polynomial in density \( \rho \) gives an excellent fit
to the data.

<p>
We note also that there is a small deviation between the
standard OLS and the Ridge regression at higher densities. We discuss this in more detail
below.

<p>
<!-- !split -->

<h2 id="splitting-our-data-in-training-and-test-data" class="anchor">Splitting our Data in Training and Test data </h2>

<p>
It is normal in essentially all Machine Learning studies to split the
data in a training set and a test set (sometimes also an additional
validation set).  <b>Scikit-Learn</b> has an own function for this. There
is no explicit recipe for how much data should be included as training
data and say test data.  An accepted rule of thumb is to use
approximately \( 2/3 \) to \( 4/5 \) of the data as training data. We will
postpone a discussion of this splitting to the end of these notes and
our discussion of the so-called <b>bias-variance</b> tradeoff. Here we
limit ourselves to repeat the above equation of state fitting example
but now splitting the data into a training set and a test set.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;EoS.csv&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)

<span style="color: #408080; font-style: italic"># Read the EoS data as  csv file and organized into two arrays with density and energies</span>
EoS <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_csv(infile, names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;Density&#39;</span>, <span style="color: #BA2121">&#39;Energy&#39;</span>))
EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
EoS <span style="color: #666666">=</span> EoS<span style="color: #666666">.</span>dropna()
Energies <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>]
Density <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Density&#39;</span>]
<span style="color: #408080; font-style: italic">#  The design matrix now as function of various polytrops</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(Density),<span style="color: #666666">5</span>))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">2.0/3.0</span>)
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> Density
X[:,<span style="color: #666666">3</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">4.0/3.0</span>)
X[:,<span style="color: #666666">4</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">5.0/3.0</span>)
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, Energies, test_size<span style="color: #666666">=0.2</span>)
<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
beta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytilde <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_train,ytilde))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytilde))
ypredict <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_test,ypredict))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredict))
</pre></div>
<p>
<!-- !split  -->

<h2 id="the-boston-housing-data-example" class="anchor">The Boston housing data example </h2>

<p>
The Boston housing  
data set was originally a part of UCI Machine Learning Repository
and has been removed now. The data set is now included in <b>Scikit-Learn</b>'s 
library.  There are 506 samples and 13 feature (predictor) variables
in this data set. The objective is to predict the value of prices of
the house using the features (predictors) listed here.

<p>
The features/predictors are

<ol>
<li> CRIM: Per capita crime rate by town</li>
<li> ZN: Proportion of residential land zoned for lots over 25000 square feet</li>
<li> INDUS: Proportion of non-retail business acres per town</li>
<li> CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li> NOX: Nitric oxide concentration (parts per 10 million)</li>
<li> RM: Average number of rooms per dwelling</li>
<li> AGE: Proportion of owner-occupied units built prior to 1940</li>
<li> DIS: Weighted distances to five Boston employment centers</li>
<li> RAD: Index of accessibility to radial highways</li>
<li> TAX: Full-value property tax rate per USD10000</li>
<li> B: \( 1000(Bk - 0.63)^2 \), where \( Bk \) is the proportion of [people of African American descent] by town</li>
<li> LSTAT: Percentage of lower status of the population</li>
<li> MEDV: Median value of owner-occupied homes in USD 1000s</li>
</ol>

<!-- !split -->

<h2 id="housing-data-the-code" class="anchor">Housing data, the code </h2>
We start by importing the libraries
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span> 

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>  
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">seaborn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">sns</span> 
</pre></div>
<p>
and load the Boston Housing DataSet from <b>Scikit-Learn</b>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_boston

boston_dataset <span style="color: #666666">=</span> load_boston()

<span style="color: #408080; font-style: italic"># boston_dataset is a dictionary</span>
<span style="color: #408080; font-style: italic"># let&#39;s check what it contains</span>
boston_dataset<span style="color: #666666">.</span>keys()
</pre></div>
<p>
Then we invoke Pandas
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>boston <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(boston_dataset<span style="color: #666666">.</span>data, columns<span style="color: #666666">=</span>boston_dataset<span style="color: #666666">.</span>feature_names)
boston<span style="color: #666666">.</span>head()
boston[<span style="color: #BA2121">&#39;MEDV&#39;</span>] <span style="color: #666666">=</span> boston_dataset<span style="color: #666666">.</span>target
</pre></div>
<p>
and preprocess the data
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># check for missing values in all the columns</span>
boston<span style="color: #666666">.</span>isnull()<span style="color: #666666">.</span>sum()
</pre></div>
<p>
We can then visualize the data
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># set the size of the figure</span>
sns<span style="color: #666666">.</span>set(rc<span style="color: #666666">=</span>{<span style="color: #BA2121">&#39;figure.figsize&#39;</span>:(<span style="color: #666666">11.7</span>,<span style="color: #666666">8.27</span>)})

<span style="color: #408080; font-style: italic"># plot a histogram showing the distribution of the target values</span>
sns<span style="color: #666666">.</span>distplot(boston[<span style="color: #BA2121">&#39;MEDV&#39;</span>], bins<span style="color: #666666">=30</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
It is now useful to look at the correlation matrix
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># compute the pair wise correlation for all columns  </span>
correlation_matrix <span style="color: #666666">=</span> boston<span style="color: #666666">.</span>corr()<span style="color: #666666">.</span>round(<span style="color: #666666">2</span>)
<span style="color: #408080; font-style: italic"># use the heatmap function from seaborn to plot the correlation matrix</span>
<span style="color: #408080; font-style: italic"># annot = True to print the values inside the square</span>
sns<span style="color: #666666">.</span>heatmap(data<span style="color: #666666">=</span>correlation_matrix, annot<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
</pre></div>
<p>
From the above coorelation plot we can see that <b>MEDV</b> is strongly correlated to <b>LSTAT</b> and  <b>RM</b>. We see also that <b>RAD</b> and <b>TAX</b> are stronly correlated, but we don't include this in our features together to avoid multi-colinearity

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">20</span>, <span style="color: #666666">5</span>))

features <span style="color: #666666">=</span> [<span style="color: #BA2121">&#39;LSTAT&#39;</span>, <span style="color: #BA2121">&#39;RM&#39;</span>]
target <span style="color: #666666">=</span> boston[<span style="color: #BA2121">&#39;MEDV&#39;</span>]

<span style="color: #008000; font-weight: bold">for</span> i, col <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(features):
    plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">1</span>, <span style="color: #008000">len</span>(features) , i<span style="color: #666666">+1</span>)
    x <span style="color: #666666">=</span> boston[col]
    y <span style="color: #666666">=</span> target
    plt<span style="color: #666666">.</span>scatter(x, y, marker<span style="color: #666666">=</span><span style="color: #BA2121">&#39;o&#39;</span>)
    plt<span style="color: #666666">.</span>title(col)
    plt<span style="color: #666666">.</span>xlabel(col)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MEDV&#39;</span>)
</pre></div>
<p>
Now we start training our model
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>X <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(np<span style="color: #666666">.</span>c_[boston[<span style="color: #BA2121">&#39;LSTAT&#39;</span>], boston[<span style="color: #BA2121">&#39;RM&#39;</span>]], columns <span style="color: #666666">=</span> [<span style="color: #BA2121">&#39;LSTAT&#39;</span>,<span style="color: #BA2121">&#39;RM&#39;</span>])
Y <span style="color: #666666">=</span> boston[<span style="color: #BA2121">&#39;MEDV&#39;</span>]
</pre></div>
<p>
We split the data into training and test sets

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split

<span style="color: #408080; font-style: italic"># splits the training and test data set in 80% : 20%</span>
<span style="color: #408080; font-style: italic"># assign random_state to any value.This ensures consistency.</span>
X_train, X_test, Y_train, Y_test <span style="color: #666666">=</span> train_test_split(X, Y, test_size <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>, random_state<span style="color: #666666">=5</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(Y_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(Y_test<span style="color: #666666">.</span>shape)
</pre></div>
<p>
Then we use the linear regression functionality from <b>Scikit-Learn</b>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error, r2_score

lin_model <span style="color: #666666">=</span> LinearRegression()
lin_model<span style="color: #666666">.</span>fit(X_train, Y_train)

<span style="color: #408080; font-style: italic"># model evaluation for training set</span>

y_train_predict <span style="color: #666666">=</span> lin_model<span style="color: #666666">.</span>predict(X_train)
rmse <span style="color: #666666">=</span> (np<span style="color: #666666">.</span>sqrt(mean_squared_error(Y_train, y_train_predict)))
r2 <span style="color: #666666">=</span> r2_score(Y_train, y_train_predict)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;The model performance for training set&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;--------------------------------------&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;RMSE is </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(rmse))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;R2 score is </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(r2))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&quot;</span>)

<span style="color: #408080; font-style: italic"># model evaluation for testing set</span>

y_test_predict <span style="color: #666666">=</span> lin_model<span style="color: #666666">.</span>predict(X_test)
<span style="color: #408080; font-style: italic"># root mean square error of the model</span>
rmse <span style="color: #666666">=</span> (np<span style="color: #666666">.</span>sqrt(mean_squared_error(Y_test, y_test_predict)))

<span style="color: #408080; font-style: italic"># r-squared score of the model</span>
r2 <span style="color: #666666">=</span> r2_score(Y_test, y_test_predict)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;The model performance for testing set&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;--------------------------------------&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;RMSE is </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(rmse))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;R2 score is </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(r2))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># plotting the y_test vs y_pred</span>
<span style="color: #408080; font-style: italic"># ideally should have been a straight line</span>
plt<span style="color: #666666">.</span>scatter(Y_test, y_test_predict)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="reducing-the-number-of-degrees-of-freedom-overarching-view" class="anchor">Reducing the number of degrees of freedom, overarching view  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Many Machine Learning problems involve thousands or even millions of
features for each training instance. Not only does this make training
extremely slow, it can also make it much harder to find a good
solution, as we will see. This problem is often referred to as the
curse of dimensionality.  Fortunately, in real-world problems, it is
often possible to reduce the number of features considerably, turning
an intractable problem into a tractable one.

<p>
Later  we will discuss some of the most popular dimensionality reduction
techniques: the principal component analysis (PCA), Kernel PCA, and
Locally Linear Embedding (LLE).

<p>
Principal component analysis and its various variants deal with the
problem of fitting a low-dimensional <a href="https://en.wikipedia.org/wiki/Affine_space" target="_self">affine
subspace</a> to a set of of
data points in a high-dimensional space. With its family of methods it
is one of the most used tools in data modeling, compression and
visualization.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="preprocessing-our-data" class="anchor">Preprocessing our data  </h2>

<p>
Before we proceed however, we will discuss how to preprocess our
data. Till now and in connection with our previous examples we have
not met so many cases where we are too sensitive to the scaling of our
data. Normally the data may need a rescaling and/or may be sensitive
to extreme values. Scaling the data renders our inputs much more
suitable for the algorithms we want to employ.

<p>
<b>Scikit-Learn</b> has several functions which allow us to rescale the
data, normally resulting in much better results in terms of various
accuracy scores.  The <b>StandardScaler</b> function in <b>Scikit-Learn</b>
ensures that for each feature/predictor we study the mean value is
zero and the variance is one (every column in the design/feature
matrix).  This scaling has the drawback that it does not ensure that
we have a particular maximum or minimum in our data set. Another
function included in <b>Scikit-Learn</b> is the <b>MinMaxScaler</b> which
ensures that all features are exactly between \( 0 \) and \( 1 \). The

<p>
<!-- !split -->

<h2 id="more-preprocessing" class="anchor">More preprocessing </h2>

<p>
The <b>Normalizer</b> scales each data
point such that the feature vector has a euclidean length of one. In other words, it
projects a data point on the circle (or sphere in the case of higher dimensions) with a
radius of 1. This means every data point is scaled by a different number (by the
inverse of it&#8217;s length).
This normalization is often used when only the direction (or angle) of the data matters,
not the length of the feature vector.

<p>
The <b>RobustScaler</b> works similarly to the StandardScaler in that it
ensures statistical properties for each feature that guarantee that
they are on the same scale. However, the RobustScaler uses the median
and quartiles, instead of mean and variance. This makes the
RobustScaler ignore data points that are very different from the rest
(like measurement errors). These odd data points are also called
outliers, and might often lead to trouble for other scaling
techniques.

<p>
<!-- !split -->

<h2 id="simple-preprocessing-examples-franke-function-and-regression" class="anchor">Simple preprocessing examples, Franke function and regression </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skl</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> MinMaxScaler, StandardScaler, Normalizer

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">FrankeFunction</span>(x,y):
	term1 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">0.25*</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>) <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>))
	term2 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>((<span style="color: #666666">9*</span>x<span style="color: #666666">+1</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">/49.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.1*</span>(<span style="color: #666666">9*</span>y<span style="color: #666666">+1</span>))
	term3 <span style="color: #666666">=</span> <span style="color: #666666">0.5*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-7</span>)<span style="color: #666666">**2/4.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-3</span>)<span style="color: #666666">**2</span>))
	term4 <span style="color: #666666">=</span> <span style="color: #666666">-0.2*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-4</span>)<span style="color: #666666">**2</span> <span style="color: #666666">-</span> (<span style="color: #666666">9*</span>y<span style="color: #666666">-7</span>)<span style="color: #666666">**2</span>)
	<span style="color: #008000; font-weight: bold">return</span> term1 <span style="color: #666666">+</span> term2 <span style="color: #666666">+</span> term3 <span style="color: #666666">+</span> term4


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">create_X</span>(x, y, n ):
	<span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(x<span style="color: #666666">.</span>shape) <span style="color: #666666">&gt;</span> <span style="color: #666666">1</span>:
		x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ravel(x)
		y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ravel(y)

	N <span style="color: #666666">=</span> <span style="color: #008000">len</span>(x)
	l <span style="color: #666666">=</span> <span style="color: #008000">int</span>((n<span style="color: #666666">+1</span>)<span style="color: #666666">*</span>(n<span style="color: #666666">+2</span>)<span style="color: #666666">/2</span>)		<span style="color: #408080; font-style: italic"># Number of elements in beta</span>
	X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ones((N,l))

	<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,n<span style="color: #666666">+1</span>):
		q <span style="color: #666666">=</span> <span style="color: #008000">int</span>((i)<span style="color: #666666">*</span>(i<span style="color: #666666">+1</span>)<span style="color: #666666">/2</span>)
		<span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(i<span style="color: #666666">+1</span>):
			X[:,q<span style="color: #666666">+</span>k] <span style="color: #666666">=</span> (x<span style="color: #666666">**</span>(i<span style="color: #666666">-</span>k))<span style="color: #666666">*</span>(y<span style="color: #666666">**</span>k)

	<span style="color: #008000; font-weight: bold">return</span> X


<span style="color: #408080; font-style: italic"># Making meshgrid of datapoints and compute Franke&#39;s function</span>
n <span style="color: #666666">=</span> <span style="color: #666666">5</span>
N <span style="color: #666666">=</span> <span style="color: #666666">1000</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>uniform(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N))
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>uniform(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N))
z <span style="color: #666666">=</span> FrankeFunction(x, y)
X <span style="color: #666666">=</span> create_X(x, y, n<span style="color: #666666">=</span>n)    
<span style="color: #408080; font-style: italic"># split in training and test data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X,z,test_size<span style="color: #666666">=0.2</span>)


clf <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>LinearRegression()<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #408080; font-style: italic"># The mean squared error and R2 score</span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;MSE before scaling: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(mean_squared_error(clf<span style="color: #666666">.</span>predict(X_test), y_test)))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;R2 score before scaling </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(clf<span style="color: #666666">.</span>score(X_test,y_test)))

scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Feature min values before scaling:</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(X_train<span style="color: #666666">.</span>min(axis<span style="color: #666666">=0</span>)))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Feature max values before scaling:</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(X_train<span style="color: #666666">.</span>max(axis<span style="color: #666666">=0</span>)))

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Feature min values after scaling:</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(X_train_scaled<span style="color: #666666">.</span>min(axis<span style="color: #666666">=0</span>)))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Feature max values after scaling:</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(X_train_scaled<span style="color: #666666">.</span>max(axis<span style="color: #666666">=0</span>)))

clf <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>LinearRegression()<span style="color: #666666">.</span>fit(X_train_scaled, y_train)


<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;MSE after  scaling: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(mean_squared_error(clf<span style="color: #666666">.</span>predict(X_test_scaled), y_test)))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;R2 score for  scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(clf<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
</pre></div>
<p>
<!-- !split -->

<h2 id="beyond-ordinary-least-squares" class="anchor">Beyond Ordinary Least Squares </h2>

<p>
<!-- !split -->

<h2 id="ridge-and-lasso-regression" class="anchor">Ridge and LASSO Regression </h2>

<p>
Let us remind ourselves about the expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, that is 
our optimization problem is
$$
{\displaystyle \min_{\boldsymbol{\beta}\in {\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right\}.
$$

or we can state it as
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2,
$$

where we have used the definition of  a norm-2 vector, that is
$$
\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}. 
$$

<p>
By minimizing the above equation with respect to the parameters
\( \boldsymbol{\beta} \) we could then obtain an analytical expression for the
parameters \( \boldsymbol{\beta} \).  We can add a regularization parameter \( \lambda \) by
defining a new cost function to be optimized, that is

$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_2^2
$$

<p>
which leads to the Ridge regression minimization problem where we
require that \( \vert\vert \boldsymbol{\beta}\vert\vert_2^2\le t \), where \( t \) is
a finite number larger than zero. By defining

$$
C(\boldsymbol{X},\boldsymbol{\beta})=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1,
$$

<p>
we have a new optimization equation
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1
$$

which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator.

<p>
Here we have defined the norm-1 as 
$$
\vert\vert \boldsymbol{x}\vert\vert_1 = \sum_i \vert x_i\vert. 
$$

<p>
<!-- !split -->

<h2 id="more-on-ridge-regression" class="anchor">More on Ridge Regression </h2>

<p>
Using the matrix-vector expression for Ridge regression (we drop the \( 1/n \) factor),

$$
C(\boldsymbol{X},\boldsymbol{\beta})=\left\{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\right\}+\lambda\boldsymbol{\beta}^T\boldsymbol{\beta},
$$

<p>
by taking the derivatives with respect to \( \boldsymbol{\beta} \) we obtain
$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0 = 2\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)-2\lambda\boldsymbol{\beta}.
$$

<p>
We obtain 
a slightly modified matrix inversion problem which for finite values
of \( \lambda \) does not suffer from singularity problems, that is

$$
\boldsymbol{\beta}^{\mathrm{Ridge}} = \left(\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y},
$$

<p>
with \( \boldsymbol{I} \) being a \( p\times p \) identity matrix with the constraint that

$$
\sum_{i=0}^{p-1} \beta_i^2 \leq t,
$$

<p>
with \( t \) a finite positive number.

<p>
We see that Ridge regression is nothing but the standard
OLS with a modified diagonal term added to \( \boldsymbol{X}^T\boldsymbol{X} \). The
consequences, in particular for our discussion of the bias-variance tradeoff 
are rather interesting.

<p>
<!-- !split -->

<h2 id="simple-ridge-interpretations" class="anchor">Simple Ridge  interpretations </h2>

<p>
For the sake of simplicity, let us assume that the design matrix is orthonormal, that is 

$$
\boldsymbol{X}^T\boldsymbol{X}=(\boldsymbol{X}^T\boldsymbol{X})^{-1} =\boldsymbol{I}. 
$$

<p>
In this case the standard OLS results in 
$$
\boldsymbol{\beta}^{\mathrm{OLS}} = \boldsymbol{X}^T\boldsymbol{y}=\sum_{i=0}^{p-1}\boldsymbol{u}_j\boldsymbol{u}_j^T\boldsymbol{y},
$$

<p>
and

$$
\boldsymbol{\beta}^{\mathrm{Ridge}} = \left(\boldsymbol{I}+\lambda\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}=\left(1+\lambda\right)^{-1}\boldsymbol{\beta}^{\mathrm{OLS}},
$$

<p>
that is the Ridge estimator scales the OLS estimator by the inverse of a factor \( 1+\lambda \), and
the Ridge estimator converges to zero when the hyperparameter goes to
infinity.

<p>
For more discussions of Ridge and Lasso regression, <a href="https://arxiv.org/abs/1509.09169" target="_self">Wessel van Wieringen's</a> article is highly recommended.
Similarly, <a href="https://arxiv.org/abs/1803.08823" target="_self">Mehta et al's article</a> is also recommended.

<p>
<!-- !split -->

<h2 id="statistics" class="anchor">Statistics  </h2>

<p>
<!-- !split -->

<h2 id="where-are-we-going" class="anchor">Where are we going? </h2>

<p>
Before we proceed, we need to rethink what we have been doing. In our
eager to fit the data, we have omitted several important elements in
our regression analysis. In what follows we will

<ol>
<li> remind ourselves about some statistical properties, including a discussion of mean values, variance and the so-called bias-variance tradeoff</li>
<li> introduce resampling techniques like cross-validation, bootstrapping and jackknife and more</li>
</ol>

This will allow us to link the standard linear algebra methods we have discussed above to a statistical interpretation of the methods.

<p>
<!-- !split  -->

<h2 id="linking-the-regression-analysis-with-a-statistical-interpretation" class="anchor">Linking the regression analysis with a statistical interpretation </h2>

<p>
We are going to discuss several statistical properties which can be obtained in terms of analytical expressions. 
The
advantage of doing linear regression is that we actually end up with
analytical expressions for several statistical quantities.  
Standard least squares and Ridge regression  allow us to
derive quantities like the variance and other expectation values in a
rather straightforward way.

<p>
It is assumed that \( \varepsilon_i
\sim \mathcal{N}(0, \sigma^2) \) and the \( \varepsilon_{i} \) are
independent, i.e.: 
$$
\begin{align*} 
\mbox{Cov}(\varepsilon_{i_1},
\varepsilon_{i_2}) & = \left\{ \begin{array}{lcc} \sigma^2 & \mbox{if}
& i_1 = i_2, \\ 0 & \mbox{if} & i_1 \not= i_2.  \end{array} \right.
\end{align*} 
$$

The randomness of \( \varepsilon_i \) implies that
\( \mathbf{y}_i \) is also a random variable. In particular,
\( \mathbf{y}_i \) is normally distributed, because \( \varepsilon_i \sim
\mathcal{N}(0, \sigma^2) \) and \( \mathbf{X}_{i,\ast} \, \boldsymbol{\beta} \) is a
non-random scalar. To specify the parameters of the distribution of
\( \mathbf{y}_i \) we need to calculate its first two moments.

<p>
Recall that \( \boldsymbol{X} \) is a matrix of dimensionality \( n\times p \). The
notation above \( \mathbf{X}_{i,\ast} \) means that we are looking at the
row number \( i \) and perform a sum over all values \( p \).

<p>
<!-- !split -->

<h2 id="assumptions-made" class="anchor">Assumptions made </h2>

<p>
The assumption we have made here can be summarized as (and this is going to be useful when we discuss the bias-variance trade off)
that there exists a function \( f(\boldsymbol{x}) \) and  a normal distributed error \( \boldsymbol{\varepsilon}\sim \mathcal{N}(0, \sigma^2) \)
which describe our data
$$
\boldsymbol{y} = f(\boldsymbol{x})+\boldsymbol{\varepsilon}
$$

<p>
We approximate this function with our model from the solution of the linear regression equations, that is our
function \( f \) is approximated by \( \boldsymbol{\tilde{y}} \) where we want to minimize \( (\boldsymbol{y}-\boldsymbol{\tilde{y}})^2 \), our MSE, with
$$
\boldsymbol{\tilde{y}} = \boldsymbol{X}\boldsymbol{\beta}\approx f(\boldsymbol{x}).
$$

<p>
Note that we reserve the design matrix \( \boldsymbol{X} \) to represent our specific rewrite of the input variables \( \boldsymbol{x} \).

<p>
<!-- !split -->

<h2 id="expectation-value-and-variance" class="anchor">Expectation value and variance </h2>

<p>
We can calculate the expectation value of \( \boldsymbol{y} \) for a given element \( i \) 
$$
\begin{align*} 
\mathbb{E}(y_i) & =
\mathbb{E}(\mathbf{X}_{i, \ast} \, \boldsymbol{\beta}) + \mathbb{E}(\varepsilon_i)
\, \, \, = \, \, \, \mathbf{X}_{i, \ast} \, \beta, 
\end{align*} 
$$

while
its variance is 
$$
\begin{align*} \mbox{Var}(y_i) & = \mathbb{E} \{ [y_i
- \mathbb{E}(y_i)]^2 \} \, \, \, = \, \, \, \mathbb{E} ( y_i^2 ) -
[\mathbb{E}(y_i)]^2  \\  & = \mathbb{E} [ ( \mathbf{X}_{i, \ast} \,
\beta + \varepsilon_i )^2] - ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 \\ &
= \mathbb{E} [ ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 + 2 \varepsilon_i
\mathbf{X}_{i, \ast} \, \boldsymbol{\beta} + \varepsilon_i^2 ] - ( \mathbf{X}_{i,
\ast} \, \beta)^2 \\  & = ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 + 2
\mathbb{E}(\varepsilon_i) \mathbf{X}_{i, \ast} \, \boldsymbol{\beta} +
\mathbb{E}(\varepsilon_i^2 ) - ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 
\\ & = \mathbb{E}(\varepsilon_i^2 ) \, \, \, = \, \, \,
\mbox{Var}(\varepsilon_i) \, \, \, = \, \, \, \sigma^2.  
\end{align*}
$$

Hence, \( y_i \sim \mathcal{N}( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta}, \sigma^2) \), that is \( \boldsymbol{y} \) follows a normal distribution with 
mean value \( \boldsymbol{X}\boldsymbol{\beta} \) and variance \( \sigma^2 \) (not be confused with the singular values of the SVD).

<p>
<!-- !split -->

<h2 id="expectation-value-and-variance-for-boldsymbol-beta" class="anchor">Expectation value and variance for \( \boldsymbol{\beta} \) </h2>

<p>
With the OLS expressions for the parameters \( \boldsymbol{\beta} \) we can evaluate the expectation value
$$
\mathbb{E}(\boldsymbol{\beta}) = \mathbb{E}[ (\mathbf{X}^{\top} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbb{E}[ \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1} \mathbf{X}^{T}\mathbf{X}\boldsymbol{\beta}=\boldsymbol{\beta}.
$$

This means that the estimator of the regression parameters is unbiased.

<p>
We can also calculate the variance

<p>
The variance of \( \boldsymbol{\beta} \) is
$$
\begin{eqnarray*}
\mbox{Var}(\boldsymbol{\beta}) & = & \mathbb{E} \{ [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})] [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})]^{T} \}
\\
& = & \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} - \boldsymbol{\beta}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} - \boldsymbol{\beta}]^{T} \}
\\
% & = & \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y}]^{T} \} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
% & = & \mathbb{E} \{ (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} \, \mathbf{Y}^{T} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1}  \} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
& = & (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \mathbb{E} \{ \mathbf{Y} \, \mathbf{Y}^{T} \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\\
& = & (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \{ \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \,  \mathbf{X}^{T} + \sigma^2 \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
% & = & (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^T \,  \mathbf{X}^T \, \mathbf{X} \, (\mathbf{X}^T % \mathbf{X})^{-1}
% \\
% & & + \, \, \sigma^2 \, (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T  \, \mathbf{X} \, (\mathbf{X}^T \mathbf{X})^{-1} - \boldsymbol{\beta} \boldsymbol{\beta}^T
\\
& = & \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}  + \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\, \, \, = \, \, \, \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1},
\end{eqnarray*}
$$

<p>
where we have used  that \( \mathbb{E} (\mathbf{Y} \mathbf{Y}^{T}) =
\mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \, \mathbf{X}^{T} +
\sigma^2 \, \mathbf{I}_{nn} \). From \( \mbox{Var}(\boldsymbol{\beta}) = \sigma^2
\, (\mathbf{X}^{T} \mathbf{X})^{-1} \), one obtains an estimate of the
variance of the estimate of the \( j \)-th regression coefficient:
\( \boldsymbol{\sigma}^2 (\hat{\beta}_j ) = \boldsymbol{\sigma}^2 \sqrt{
[(\mathbf{X}^{T} \mathbf{X})^{-1}]_{jj} } \). This may be used to
construct a confidence interval for the estimates.

<p>
In a similar way, we can obtain analytical expressions for say the
expectation values of the parameters \( \boldsymbol{\beta} \) and their variance
when we employ Ridge regression, allowing us again to define a confidence interval.

<p>
It is rather straightforward to show that
$$
\mathbb{E} \big[ \boldsymbol{\beta}^{\mathrm{Ridge}} \big]=(\mathbf{X}^{T} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X})\boldsymbol{\beta}^{\mathrm{OLS}}.
$$

We see clearly that 
\( \mathbb{E} \big[ \boldsymbol{\beta}^{\mathrm{Ridge}} \big] \not= \boldsymbol{\beta}^{\mathrm{OLS}} \) for any \( \lambda > 0 \). We say then that the ridge estimator is biased.

<p>
We can also compute the variance as 

$$
\mbox{Var}[\boldsymbol{\beta}^{\mathrm{Ridge}}]=\sigma^2[  \mathbf{X}^{T} \mathbf{X} + \lambda \mathbf{I} ]^{-1}  \mathbf{X}^{T} \mathbf{X} \{ [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I} ]^{-1}\}^{T},
$$

and it is easy to see that if the parameter \( \lambda \) goes to infinity then the variance of Ridge parameters \( \boldsymbol{\beta} \) goes to zero.

<p>
With this, we can compute the difference 

$$
\mbox{Var}[\boldsymbol{\beta}^{\mathrm{OLS}}]-\mbox{Var}(\boldsymbol{\beta}^{\mathrm{Ridge}})=\sigma^2 [  \mathbf{X}^{T} \mathbf{X} + \lambda \mathbf{I} ]^{-1}[ 2\lambda\mathbf{I} + \lambda^2 (\mathbf{X}^{T} \mathbf{X})^{-1} ] \{ [  \mathbf{X}^{T} \mathbf{X} + \lambda \mathbf{I} ]^{-1}\}^{T}.
$$

The difference is non-negative definite since each component of the
matrix product is non-negative definite. 
This means the variance we obtain with the standard OLS will always for \( \lambda > 0 \) be larger than the variance of \( \boldsymbol{\beta} \) obtained with the Ridge estimator. This has interesting consequences when we discuss the so-called bias-variance trade-off tomorrow.

<p>
<!-- !split -->

<h2 id="why-resampling-methods" class="anchor">Why resampling methods </h2>

<p>
Before we proceed, we need to rethink what we have been doing. In our
eager to fit the data, we have omitted several important elements in
our regression analysis. In what follows we will

<ol>
<li> look at statistical properties, including a discussion of mean values, variance and the so-called bias-variance tradeoff</li>
<li> introduce resampling techniques like cross-validation, bootstrapping and jackknife and more</li>
</ol>

This will allow us to link the standard linear algebra methods we have discussed above to a statistical interpretation of the methods.

<p>
<!-- !split -->

<h2 id="resampling-methods" class="anchor">Resampling methods </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
Resampling methods are an indispensable tool in modern
statistics. They involve repeatedly drawing samples from a training
set and refitting a model of interest on each sample in order to
obtain additional information about the fitted model. For example, in
order to estimate the variability of a linear regression fit, we can
repeatedly draw different samples from the training data, fit a linear
regression to each new sample, and then examine the extent to which
the resulting fits differ. Such an approach may allow us to obtain
information that would not be available from fitting the model only
once using the original training sample.

<p>
Two resampling methods are often used in Machine Learning analyses,

<ol>
<li> The <b>bootstrap method</b></li>
<li> and <b>Cross-Validation</b></li>
</ol>

In addition there are several other methods such as the Jackknife and the Blocking methods. We will discuss in particular
cross-validation and the bootstrap method.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="resampling-approaches-can-be-computationally-expensive" class="anchor">Resampling approaches can be computationally expensive </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Resampling approaches can be computationally expensive, because they
involve fitting the same statistical method multiple times using
different subsets of the training data. However, due to recent
advances in computing power, the computational requirements of
resampling methods generally are not prohibitive. In this chapter, we
discuss two of the most commonly used resampling methods,
cross-validation and the bootstrap. Both methods are important tools
in the practical application of many statistical learning
procedures. For example, cross-validation can be used to estimate the
test error associated with a given statistical learning method in
order to evaluate its performance, or to select the appropriate level
of flexibility. The process of evaluating a model&#8217;s performance is
known as model assessment, whereas the process of selecting the proper
level of flexibility for a model is known as model selection. The
bootstrap is widely used.

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="why-resampling-methods" class="anchor">Why resampling methods ? </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<ul>
<li> Our simulations can be treated as <em>computer experiments</em>. This is particularly the case for Monte Carlo methods</li>
<li> The results can be analysed with the same statistical tools as we would use analysing experimental data.</li>
<li> As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.</li>
</ul>
</div>
</div>


<p>
<!-- !split -->

<h2 id="statistical-analysis" class="anchor">Statistical analysis </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<ul>
<li> As in other experiments, many numerical  experiments have two classes of errors:</li>

<ul>
  <li> Statistical errors</li>
  <li> Systematical errors</li>
</ul>

<li> Statistical errors can be estimated using standard tools from statistics</li>
<li> Systematical errors are method specific and must be treated differently from case to case.</li> 
</ul>
</div>
</div>


<p>
<!-- !split -->

<h2 id="resampling-methods-jackknife-and-bootstrap" class="anchor">Resampling methods: Jackknife and Bootstrap </h2>

<p>
Two famous
resampling methods are the <b>independent bootstrap</b> and <b>the jackknife</b>.

<p>
The jackknife is a special case of the independent bootstrap. Still, the jackknife was made
popular prior to the independent bootstrap. And as the popularity of
the independent bootstrap soared, new variants, such as <b>the dependent bootstrap</b>.

<p>
The Jackknife and independent bootstrap work for
independent, identically distributed random variables.
If these conditions are not
satisfied, the methods will fail.  Yet, it should be said that if the data are
independent, identically distributed, and we only want to estimate the
variance of \( \overline{X} \) (which often is the case), then there is no
need for bootstrapping.

<p>
<!-- !split -->

<h2 id="resampling-methods-jackknife" class="anchor">Resampling methods: Jackknife </h2>

<p>
The Jackknife works by making many replicas of the estimator \( \widehat{\theta} \). 
The jackknife is a resampling method where we systematically leave out one observation from the vector of observed values \( \boldsymbol{x} = (x_1,x_2,\cdots,X_n) \). 
Let \( \boldsymbol{x}_i \) denote the vector
$$
\boldsymbol{x}_i = (x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_n),
$$

<p>
which equals the vector \( \boldsymbol{x} \) with the exception that observation
number \( i \) is left out. Using this notation, define
\( \widehat{\theta}_i \) to be the estimator
\( \widehat{\theta} \) computed using \( \vec{X}_i \).

<p>
<!-- !split -->

<h2 id="jackknife-code-example" class="anchor">Jackknife code example </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">import</span> <span style="color: #666666">*</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">numpy.random</span> <span style="color: #008000; font-weight: bold">import</span> randint, randn
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">time</span> <span style="color: #008000; font-weight: bold">import</span> time

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">jackknife</span>(data, stat):
    n <span style="color: #666666">=</span> <span style="color: #008000">len</span>(data);t <span style="color: #666666">=</span> zeros(n); inds <span style="color: #666666">=</span> arange(n); t0 <span style="color: #666666">=</span> time()
    <span style="color: #408080; font-style: italic">## &#39;jackknifing&#39; by leaving out an observation for each i                                                                                                                      </span>
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n):
        t[i] <span style="color: #666666">=</span> stat(delete(data,i) )

    <span style="color: #408080; font-style: italic"># analysis                                                                                                                                                                     </span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Runtime: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121"> sec&quot;</span> <span style="color: #666666">%</span> (time()<span style="color: #666666">-</span>t0)); <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Jackknife Statistics :&quot;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;original           bias      std. error&quot;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;</span><span style="color: #BB6688; font-weight: bold">%8g</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">%14g</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">%15g</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> (stat(data),(n<span style="color: #666666">-1</span>)<span style="color: #666666">*</span>mean(t)<span style="color: #666666">/</span>n, (n<span style="color: #666666">*</span>var(t))<span style="color: #666666">**.5</span>))

    <span style="color: #008000; font-weight: bold">return</span> t


<span style="color: #408080; font-style: italic"># Returns mean of data samples                                                                                                                                                     </span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">stat</span>(data):
    <span style="color: #008000; font-weight: bold">return</span> mean(data)


mu, sigma <span style="color: #666666">=</span> <span style="color: #666666">100</span>, <span style="color: #666666">15</span>
datapoints <span style="color: #666666">=</span> <span style="color: #666666">10000</span>
x <span style="color: #666666">=</span> mu <span style="color: #666666">+</span> sigma<span style="color: #666666">*</span>random<span style="color: #666666">.</span>randn(datapoints)
<span style="color: #408080; font-style: italic"># jackknife returns the data sample                                                                                                                                                </span>
t <span style="color: #666666">=</span> jackknife(x, stat)
</pre></div>
<p>
<!-- !split -->

<h2 id="resampling-methods-bootstrap" class="anchor">Resampling methods: Bootstrap </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
Bootstrapping is a nonparametric approach to statistical inference
that substitutes computation for more traditional distributional
assumptions and asymptotic results. Bootstrapping offers a number of
advantages: 

<ol>
<li> The bootstrap is quite general, although there are some cases in which it fails.</li>  
<li> Because it does not require distributional assumptions (such as normally distributed errors), the bootstrap can provide more accurate inferences when the data are not well behaved or when the sample size is small.</li>  
<li> It is possible to apply the bootstrap to statistics with sampling distributions that are difficult to derive, even asymptotically.</li> 
<li> It is relatively simple to apply the bootstrap to complex data-collection plans (such as stratified and clustered samples).</li>
</ol>
</div>
</div>


<p>
<!-- !split -->

<h2 id="resampling-methods-bootstrap-background" class="anchor">Resampling methods: Bootstrap background </h2>

<p>
Since \( \widehat{\theta} = \widehat{\theta}(\boldsymbol{X}) \) is a function of random variables,
\( \widehat{\theta} \) itself must be a random variable. Thus it has
a pdf, call this function \( p(\boldsymbol{t}) \). The aim of the bootstrap is to
estimate \( p(\boldsymbol{t}) \) by the relative frequency of
\( \widehat{\theta} \). You can think of this as using a histogram
in the place of \( p(\boldsymbol{t}) \). If the relative frequency closely
resembles \( p(\vec{t}) \), then using numerics, it is straight forward to
estimate all the interesting parameters of \( p(\boldsymbol{t}) \) using point
estimators.

<p>
<!-- !split -->

<h2 id="resampling-methods-more-bootstrap-background" class="anchor">Resampling methods: More Bootstrap background </h2>

<p>
In the case that \( \widehat{\theta} \) has
more than one component, and the components are independent, we use the
same estimator on each component separately.  If the probability
density function of \( X_i \), \( p(x) \), had been known, then it would have
been straight forward to do this by: 

<ol>
<li> Drawing lots of numbers from \( p(x) \), suppose we call one such set of numbers \( (X_1^*, X_2^*, \cdots, X_n^*) \).</li> 
<li> Then using these numbers, we could compute a replica of \( \widehat{\theta} \) called \( \widehat{\theta}^* \).</li> 
</ol>

By repeated use of (1) and (2), many
estimates of \( \widehat{\theta} \) could have been obtained. The
idea is to use the relative frequency of \( \widehat{\theta}^* \)
(think of a histogram) as an estimate of \( p(\boldsymbol{t}) \).

<p>
<!-- !split -->

<h2 id="resampling-methods-bootstrap-approach" class="anchor">Resampling methods: Bootstrap approach </h2>

<p>
But
unless there is enough information available about the process that
generated \( X_1,X_2,\cdots,X_n \), \( p(x) \) is in general
unknown. Therefore, <a href="https://projecteuclid.org/euclid.aos/1176344552" target="_self">Efron in 1979</a>  asked the
question: What if we replace \( p(x) \) by the relative frequency
of the observation \( X_i \); if we draw observations in accordance with
the relative frequency of the observations, will we obtain the same
result in some asymptotic sense? The answer is yes.

<p>
Instead of generating the histogram for the relative
frequency of the observation \( X_i \), just draw the values
\( (X_1^*,X_2^*,\cdots,X_n^*) \) with replacement from the vector
\( \boldsymbol{X} \).

<p>
<!-- !split -->

<h2 id="resampling-methods-bootstrap-steps" class="anchor">Resampling methods: Bootstrap steps </h2>

<p>
The independent bootstrap works like this: 

<ol>
<li> Draw with replacement \( n \) numbers for the observed variables \( \boldsymbol{x} = (x_1,x_2,\cdots,x_n) \).</li> 
<li> Define a vector \( \boldsymbol{x}^* \) containing the values which were drawn from \( \boldsymbol{x} \).</li> 
<li> Using the vector \( \boldsymbol{x}^* \) compute \( \widehat{\theta}^* \) by evaluating \( \widehat \theta \) under the observations \( \boldsymbol{x}^* \).</li> 
<li> Repeat this process \( k \) times.</li> 
</ol>

When you are done, you can draw a histogram of the relative frequency
of \( \widehat \theta^* \). This is your estimate of the probability
distribution \( p(t) \). Using this probability distribution you can
estimate any statistics thereof. In principle you never draw the
histogram of the relative frequency of \( \widehat{\theta}^* \). Instead
you use the estimators corresponding to the statistic of interest. For
example, if you are interested in estimating the variance of \( \widehat
\theta \), apply the etsimator \( \widehat \sigma^2 \) to the values
\( \widehat \theta ^* \).

<p>
<!-- !split -->

<h2 id="code-example-for-the-bootstrap-method" class="anchor">Code example for the Bootstrap method </h2>

<p>
The following code starts with a Gaussian distribution with mean value
\( \mu =100 \) and variance \( \sigma=15 \). We use this to generate the data
used in the bootstrap analysis. The bootstrap analysis returns a data
set after a given number of bootstrap operations (as many as we have
data points). This data set consists of estimated mean values for each
bootstrap operation. The histogram generated by the bootstrap method
shows that the distribution for these mean values is also a Gaussian,
centered around the mean value \( \mu=100 \) but with standard deviation
\( \sigma/\sqrt{n} \), where \( n \) is the number of bootstrap samples (in
this case the same as the number of original data points). The value
of the standard deviation is what we expect from the central limit
theorem.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">import</span> <span style="color: #666666">*</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">numpy.random</span> <span style="color: #008000; font-weight: bold">import</span> randint, randn
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">time</span> <span style="color: #008000; font-weight: bold">import</span> time
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.mlab</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">mlab</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #408080; font-style: italic"># Returns mean of bootstrap samples                                                                                                                                                </span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">stat</span>(data):
    <span style="color: #008000; font-weight: bold">return</span> mean(data)

<span style="color: #408080; font-style: italic"># Bootstrap algorithm</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">bootstrap</span>(data, statistic, R):
    t <span style="color: #666666">=</span> zeros(R); n <span style="color: #666666">=</span> <span style="color: #008000">len</span>(data); inds <span style="color: #666666">=</span> arange(n); t0 <span style="color: #666666">=</span> time()
    <span style="color: #408080; font-style: italic"># non-parametric bootstrap         </span>
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(R):
        t[i] <span style="color: #666666">=</span> statistic(data[randint(<span style="color: #666666">0</span>,n,n)])

    <span style="color: #408080; font-style: italic"># analysis    </span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Runtime: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121"> sec&quot;</span> <span style="color: #666666">%</span> (time()<span style="color: #666666">-</span>t0)); <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Bootstrap Statistics :&quot;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;original           bias      std. error&quot;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;</span><span style="color: #BB6688; font-weight: bold">%8g</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">%8g</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">%14g</span><span style="color: #BA2121"> </span><span style="color: #BB6688; font-weight: bold">%15g</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> (statistic(data), std(data),mean(t),std(t)))
    <span style="color: #008000; font-weight: bold">return</span> t


mu, sigma <span style="color: #666666">=</span> <span style="color: #666666">100</span>, <span style="color: #666666">15</span>
datapoints <span style="color: #666666">=</span> <span style="color: #666666">10000</span>
x <span style="color: #666666">=</span> mu <span style="color: #666666">+</span> sigma<span style="color: #666666">*</span>random<span style="color: #666666">.</span>randn(datapoints)
<span style="color: #408080; font-style: italic"># bootstrap returns the data sample                                    </span>
t <span style="color: #666666">=</span> bootstrap(x, stat, datapoints)
<span style="color: #408080; font-style: italic"># the histogram of the bootstrapped  data                                                                                                    </span>
n, binsboot, patches <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>hist(t, <span style="color: #666666">50</span>, normed<span style="color: #666666">=1</span>, facecolor<span style="color: #666666">=</span><span style="color: #BA2121">&#39;red&#39;</span>, alpha<span style="color: #666666">=0.75</span>)

<span style="color: #408080; font-style: italic"># add a &#39;best fit&#39; line  </span>
y <span style="color: #666666">=</span> mlab<span style="color: #666666">.</span>normpdf( binsboot, mean(t), std(t))
lt <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>plot(binsboot, y, <span style="color: #BA2121">&#39;r--&#39;</span>, linewidth<span style="color: #666666">=1</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;Smarts&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;Probability&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">99.5</span>, <span style="color: #666666">100.6</span>, <span style="color: #666666">0</span>, <span style="color: #666666">3.0</span>])
plt<span style="color: #666666">.</span>grid(<span style="color: #008000; font-weight: bold">True</span>)

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split  -->

<h2 id="various-steps-in-cross-validation" class="anchor">Various steps in cross-validation </h2>

<p>
When the repetitive splitting of the data set is done randomly,
samples may accidently end up in a fast majority of the splits in
either training or test set. Such samples may have an unbalanced
influence on either model building or prediction evaluation. To avoid
this \( k \)-fold cross-validation structures the data splitting. The
samples are divided into \( k \) more or less equally sized exhaustive and
mutually exclusive subsets. In turn (at each split) one of these
subsets plays the role of the test set while the union of the
remaining subsets constitutes the training set. Such a splitting
warrants a balanced representation of each sample in both training and
test set over the splits. Still the division into the \( k \) subsets
involves a degree of randomness. This may be fully excluded when
choosing \( k=n \). This particular case is referred to as leave-one-out
cross-validation (LOOCV).

<p>
<!-- !split -->

<h2 id="cross-validation-in-brief" class="anchor">Cross-validation in brief </h2>

<p>
For the various values of \( k \)

<ol>
<li> shuffle the dataset randomly.</li>
<li> Split the dataset into \( k \) groups.</li>
<li> For each unique group:

<ol type="a"></li>
<li> Decide which group to use as set for test data</li>
<li> Take the remaining groups as a training data set</li>
<li> Fit a model on the training set and evaluate it on the test set</li>
<li> Retain the evaluation score and discard the model</li>
</ol>

<li> Summarize the model using the sample of model evaluation scores</li>
</ol>

<!-- !split -->

<h2 id="code-example-for-cross-validation-and-k-fold-cross-validation" class="anchor">Code Example for Cross-validation and \( k \)-fold Cross-validation </h2>

<p>
The code here uses Ridge regression with cross-validation (CV)  resampling and \( k \)-fold CV in order to fit a specific polynomial. 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> KFold
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> Ridge
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_val_score
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures

<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">3155</span>)

<span style="color: #408080; font-style: italic"># Generate the data.</span>
nsamples <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(nsamples)
y <span style="color: #666666">=</span> <span style="color: #666666">3*</span>x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(nsamples)

<span style="color: #408080; font-style: italic">## Cross-validation on Ridge regression using KFold only</span>

<span style="color: #408080; font-style: italic"># Decide degree on polynomial to fit</span>
poly <span style="color: #666666">=</span> PolynomialFeatures(degree <span style="color: #666666">=</span> <span style="color: #666666">6</span>)

<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">500</span>
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-3</span>, <span style="color: #666666">5</span>, nlambdas)

<span style="color: #408080; font-style: italic"># Initialize a KFold instance</span>
k <span style="color: #666666">=</span> <span style="color: #666666">5</span>
kfold <span style="color: #666666">=</span> KFold(n_splits <span style="color: #666666">=</span> k)

<span style="color: #408080; font-style: italic"># Perform the cross-validation to estimate MSE</span>
scores_KFold <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((nlambdas, k))

i <span style="color: #666666">=</span> <span style="color: #666666">0</span>
<span style="color: #008000; font-weight: bold">for</span> lmb <span style="color: #AA22FF; font-weight: bold">in</span> lambdas:
    ridge <span style="color: #666666">=</span> Ridge(alpha <span style="color: #666666">=</span> lmb)
    j <span style="color: #666666">=</span> <span style="color: #666666">0</span>
    <span style="color: #008000; font-weight: bold">for</span> train_inds, test_inds <span style="color: #AA22FF; font-weight: bold">in</span> kfold<span style="color: #666666">.</span>split(x):
        xtrain <span style="color: #666666">=</span> x[train_inds]
        ytrain <span style="color: #666666">=</span> y[train_inds]

        xtest <span style="color: #666666">=</span> x[test_inds]
        ytest <span style="color: #666666">=</span> y[test_inds]

        Xtrain <span style="color: #666666">=</span> poly<span style="color: #666666">.</span>fit_transform(xtrain[:, np<span style="color: #666666">.</span>newaxis])
        ridge<span style="color: #666666">.</span>fit(Xtrain, ytrain[:, np<span style="color: #666666">.</span>newaxis])

        Xtest <span style="color: #666666">=</span> poly<span style="color: #666666">.</span>fit_transform(xtest[:, np<span style="color: #666666">.</span>newaxis])
        ypred <span style="color: #666666">=</span> ridge<span style="color: #666666">.</span>predict(Xtest)

        scores_KFold[i,j] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum((ypred <span style="color: #666666">-</span> ytest[:, np<span style="color: #666666">.</span>newaxis])<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>np<span style="color: #666666">.</span>size(ypred)

        j <span style="color: #666666">+=</span> <span style="color: #666666">1</span>
    i <span style="color: #666666">+=</span> <span style="color: #666666">1</span>


estimated_mse_KFold <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(scores_KFold, axis <span style="color: #666666">=</span> <span style="color: #666666">1</span>)

<span style="color: #408080; font-style: italic">## Cross-validation using cross_val_score from sklearn along with KFold</span>

<span style="color: #408080; font-style: italic"># kfold is an instance initialized above as:</span>
<span style="color: #408080; font-style: italic"># kfold = KFold(n_splits = k)</span>

estimated_mse_sklearn <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
i <span style="color: #666666">=</span> <span style="color: #666666">0</span>
<span style="color: #008000; font-weight: bold">for</span> lmb <span style="color: #AA22FF; font-weight: bold">in</span> lambdas:
    ridge <span style="color: #666666">=</span> Ridge(alpha <span style="color: #666666">=</span> lmb)

    X <span style="color: #666666">=</span> poly<span style="color: #666666">.</span>fit_transform(x[:, np<span style="color: #666666">.</span>newaxis])
    estimated_mse_folds <span style="color: #666666">=</span> cross_val_score(ridge, X, y[:, np<span style="color: #666666">.</span>newaxis], scoring<span style="color: #666666">=</span><span style="color: #BA2121">&#39;neg_mean_squared_error&#39;</span>, cv<span style="color: #666666">=</span>kfold)

    <span style="color: #408080; font-style: italic"># cross_val_score return an array containing the estimated negative mse for every fold.</span>
    <span style="color: #408080; font-style: italic"># we have to the the mean of every array in order to get an estimate of the mse of the model</span>
    estimated_mse_sklearn[i] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(<span style="color: #666666">-</span>estimated_mse_folds)

    i <span style="color: #666666">+=</span> <span style="color: #666666">1</span>

<span style="color: #408080; font-style: italic">## Plot and compare the slightly different ways to perform cross-validation</span>

plt<span style="color: #666666">.</span>figure()

plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), estimated_mse_sklearn, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;cross_val_score&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), estimated_mse_KFold, <span style="color: #BA2121">&#39;r--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;KFold&#39;</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;mse&#39;</span>)

plt<span style="color: #666666">.</span>legend()

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="the-bias-variance-tradeoff" class="anchor">The bias-variance tradeoff </h2>

<p>
We will discuss the bias-variance tradeoff in the context of
continuous predictions such as regression. However, many of the
intuitions and ideas discussed here also carry over to classification
tasks. Consider a dataset \( \mathcal{L} \) consisting of the data
\( \mathbf{X}_\mathcal{L}=\{(y_j, \boldsymbol{x}_j), j=0\ldots n-1\} \).

<p>
Let us assume that the true data is generated from a noisy model

$$
\boldsymbol{y}=f(\boldsymbol{x}) + \boldsymbol{\epsilon}
$$

<p>
where \( \epsilon \) is normally distributed with mean zero and standard deviation \( \sigma^2 \).

<p>
In our derivation of the ordinary least squares method we defined then
an approximation to the function \( f \) in terms of the parameters
\( \boldsymbol{\beta} \) and the design matrix \( \boldsymbol{X} \) which embody our model,
that is \( \boldsymbol{\tilde{y}}=\boldsymbol{X}\boldsymbol{\beta} \).

<p>
Thereafter we found the parameters \( \boldsymbol{\beta} \) by optimizing the means squared error via the so-called cost function
$$
C(\boldsymbol{X},\boldsymbol{\beta}) =\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right].
$$

<p>
We can rewrite this as 
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\frac{1}{n}\sum_i(\tilde{y}_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\sigma^2.
$$

<p>
The three terms represent the square of the bias of the learning
method, which can be thought of as the error caused by the simplifying
assumptions built into the method. The second term represents the
variance of the chosen model and finally the last terms is variance of
the error \( \boldsymbol{\epsilon} \).

<p>
To derive this equation, we need to recall that the variance of \( \boldsymbol{y} \) and \( \boldsymbol{\epsilon} \) are both equal to \( \sigma^2 \). The mean value of \( \boldsymbol{\epsilon} \) is by definition equal to zero. Furthermore, the function \( f \) is not a stochastics variable, idem for \( \boldsymbol{\tilde{y}} \).
We use a more compact notation in terms of the expectation value 
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\mathbb{E}\left[(\boldsymbol{f}+\boldsymbol{\epsilon}-\boldsymbol{\tilde{y}})^2\right],
$$

and adding and subtracting \( \mathbb{E}\left[\boldsymbol{\tilde{y}}\right] \) we get
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\mathbb{E}\left[(\boldsymbol{f}+\boldsymbol{\epsilon}-\boldsymbol{\tilde{y}}+\mathbb{E}\left[\boldsymbol{\tilde{y}}\right]-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2\right],
$$

which, using the abovementioned expectation values can be rewritten as 
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\mathbb{E}\left[(\boldsymbol{y}-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2\right]+\mathrm{Var}\left[\boldsymbol{\tilde{y}}\right]+\sigma^2,
$$

that is the rewriting in terms of the so-called bias, the variance of the model \( \boldsymbol{\tilde{y}} \) and the variance of \( \boldsymbol{\epsilon} \).

<p>
<!-- !split -->

<h2 id="overfitting-training-and-test-data-before-bias-variance-trade-off-analysis" class="anchor">Overfitting, Training and Test data before bias-variance trade-off analysis </h2>

<p>
Let us just analyze the means-squared error as function of model complexity by comparing the training data against the test data. The first example does also not contain any resampling.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> make_pipeline

np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2018</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">80</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">18</span>

<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
TrainMSE <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
TestMSE <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
polydegree <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
<span style="color: #408080; font-style: italic"># Split data in train and test</span>
x_train, x_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(x, y, test_size<span style="color: #666666">=0.2</span>)
<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(maxdegree):
    model <span style="color: #666666">=</span> make_pipeline(PolynomialFeatures(degree<span style="color: #666666">=</span>degree), LinearRegression(fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>))
    y_TrainPred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>empty(y_train<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>])
    y_TestPred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>empty(y_test<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>])
    y_TrainPred <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(x_train, y_train)<span style="color: #666666">.</span>predict(x_train)
    y_TestPred <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(x_test)

    polydegree[degree] <span style="color: #666666">=</span> degree
    TestMSE[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> y_TestPred)<span style="color: #666666">**2</span>, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>) )
    TrainMSE[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_train <span style="color: #666666">-</span> y_TrainPred)<span style="color: #666666">**2</span>, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>) )
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Polynomial degree:&#39;</span>, degree)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Training MSE:&#39;</span>, TrainMSE[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Test MSE:&#39;</span>, TestMSE[degree])

plt<span style="color: #666666">.</span>plot(polydegree, TrainMSE, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Train MSE&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, TestMSE, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Test MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="understanding-what-happens" class="anchor">Understanding what happens </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression, Ridge, Lasso
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> make_pipeline
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.utils</span> <span style="color: #008000; font-weight: bold">import</span> resample

np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2018</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">40</span>
n_boostraps <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">14</span>


<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
error <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
bias <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
variance <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
polydegree <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
x_train, x_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(x, y, test_size<span style="color: #666666">=0.2</span>)

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(maxdegree):
    model <span style="color: #666666">=</span> make_pipeline(PolynomialFeatures(degree<span style="color: #666666">=</span>degree), LinearRegression(fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>))
    y_pred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>empty((y_test<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], n_boostraps))
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_boostraps):
        x_, y_ <span style="color: #666666">=</span> resample(x_train, y_train)
        y_pred[:, i] <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(x_, y_)<span style="color: #666666">.</span>predict(x_test)<span style="color: #666666">.</span>ravel()

    polydegree[degree] <span style="color: #666666">=</span> degree
    error[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> y_pred)<span style="color: #666666">**2</span>, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>) )
    bias[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( (y_test <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_pred, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>))<span style="color: #666666">**2</span> )
    variance[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>var(y_pred, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>) )
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Polynomial degree:&#39;</span>, degree)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;</span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> &gt;= </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> + </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> = </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(error[degree], bias[degree], variance[degree], bias[degree]<span style="color: #666666">+</span>variance[degree]))

plt<span style="color: #666666">.</span>plot(polydegree, error, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Error&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, bias, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;bias&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, variance, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Variance&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split  -->

<h2 id="summing-up" class="anchor">Summing up </h2>

<p>
The bias-variance tradeoff summarizes the fundamental tension in
machine learning, particularly supervised learning, between the
complexity of a model and the amount of training data needed to train
it.  Since data is often limited, in practice it is often useful to
use a less-complex model with higher bias, that is  a model whose asymptotic
performance is worse than another model because it is easier to
train and less sensitive to sampling noise arising from having a
finite-sized training dataset (smaller variance).

<p>
The above equations tell us that in
order to minimize the expected test error, we need to select a
statistical learning method that simultaneously achieves low variance
and low bias. Note that variance is inherently a nonnegative quantity,
and squared bias is also nonnegative. Hence, we see that the expected
test MSE can never lie below \( Var(\epsilon) \), the irreducible error.

<p>
What do we mean by the variance and bias of a statistical learning
method? The variance refers to the amount by which our model would change if we
estimated it using a different training data set. Since the training
data are used to fit the statistical learning method, different
training data sets  will result in a different estimate. But ideally the
estimate for our model should not vary too much between training
sets. However, if a method has high variance  then small changes in
the training data can result in large changes in the model. In general, more
flexible statistical methods have higher variance.

<p>
You may also find this recent <a href="https://www.pnas.org/content/116/32/15849" target="_self">article</a> of interest.

<p>
<!-- !split -->

<h2 id="another-example-from-scikit-learn-s-repository" class="anchor">Another Example from Scikit-Learn's Repository </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">============================</span>
<span style="color: #BA2121; font-style: italic">Underfitting vs. Overfitting</span>
<span style="color: #BA2121; font-style: italic">============================</span>

<span style="color: #BA2121; font-style: italic">This example demonstrates the problems of underfitting and overfitting and</span>
<span style="color: #BA2121; font-style: italic">how we can use linear regression with polynomial features to approximate</span>
<span style="color: #BA2121; font-style: italic">nonlinear functions. The plot shows the function that we want to approximate,</span>
<span style="color: #BA2121; font-style: italic">which is a part of the cosine function. In addition, the samples from the</span>
<span style="color: #BA2121; font-style: italic">real function and the approximations of different models are displayed. The</span>
<span style="color: #BA2121; font-style: italic">models have polynomial features of different degrees. We can see that a</span>
<span style="color: #BA2121; font-style: italic">linear function (polynomial with degree 1) is not sufficient to fit the</span>
<span style="color: #BA2121; font-style: italic">training samples. This is called **underfitting**. A polynomial of degree 4</span>
<span style="color: #BA2121; font-style: italic">approximates the true function almost perfectly. However, for higher degrees</span>
<span style="color: #BA2121; font-style: italic">the model will **overfit** the training data, i.e. it learns the noise of the</span>
<span style="color: #BA2121; font-style: italic">training data.</span>
<span style="color: #BA2121; font-style: italic">We evaluate quantitatively **overfitting** / **underfitting** by using</span>
<span style="color: #BA2121; font-style: italic">cross-validation. We calculate the mean squared error (MSE) on the validation</span>
<span style="color: #BA2121; font-style: italic">set, the higher, the less likely the model generalizes correctly from the</span>
<span style="color: #BA2121; font-style: italic">training data.</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000">print</span>(<span style="color: #19177C">__doc__</span>)

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> Pipeline
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_val_score


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">true_fun</span>(X):
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>cos(<span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>pi <span style="color: #666666">*</span> X)

np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">0</span>)

n_samples <span style="color: #666666">=</span> <span style="color: #666666">30</span>
degrees <span style="color: #666666">=</span> [<span style="color: #666666">1</span>, <span style="color: #666666">4</span>, <span style="color: #666666">15</span>]

X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n_samples))
y <span style="color: #666666">=</span> true_fun(X) <span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(n_samples) <span style="color: #666666">*</span> <span style="color: #666666">0.1</span>

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">14</span>, <span style="color: #666666">5</span>))
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(degrees)):
    ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">1</span>, <span style="color: #008000">len</span>(degrees), i <span style="color: #666666">+</span> <span style="color: #666666">1</span>)
    plt<span style="color: #666666">.</span>setp(ax, xticks<span style="color: #666666">=</span>(), yticks<span style="color: #666666">=</span>())

    polynomial_features <span style="color: #666666">=</span> PolynomialFeatures(degree<span style="color: #666666">=</span>degrees[i],
                                             include_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
    linear_regression <span style="color: #666666">=</span> LinearRegression()
    pipeline <span style="color: #666666">=</span> Pipeline([(<span style="color: #BA2121">&quot;polynomial_features&quot;</span>, polynomial_features),
                         (<span style="color: #BA2121">&quot;linear_regression&quot;</span>, linear_regression)])
    pipeline<span style="color: #666666">.</span>fit(X[:, np<span style="color: #666666">.</span>newaxis], y)

    <span style="color: #408080; font-style: italic"># Evaluate the models using crossvalidation</span>
    scores <span style="color: #666666">=</span> cross_val_score(pipeline, X[:, np<span style="color: #666666">.</span>newaxis], y,
                             scoring<span style="color: #666666">=</span><span style="color: #BA2121">&quot;neg_mean_squared_error&quot;</span>, cv<span style="color: #666666">=10</span>)

    X_test <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">100</span>)
    plt<span style="color: #666666">.</span>plot(X_test, pipeline<span style="color: #666666">.</span>predict(X_test[:, np<span style="color: #666666">.</span>newaxis]), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Model&quot;</span>)
    plt<span style="color: #666666">.</span>plot(X_test, true_fun(X_test), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;True function&quot;</span>)
    plt<span style="color: #666666">.</span>scatter(X, y, edgecolor<span style="color: #666666">=</span><span style="color: #BA2121">&#39;b&#39;</span>, s<span style="color: #666666">=20</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Samples&quot;</span>)
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;x&quot;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;y&quot;</span>)
    plt<span style="color: #666666">.</span>xlim((<span style="color: #666666">0</span>, <span style="color: #666666">1</span>))
    plt<span style="color: #666666">.</span>ylim((<span style="color: #666666">-2</span>, <span style="color: #666666">2</span>))
    plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;best&quot;</span>)
    plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Degree </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">MSE = </span><span style="color: #BB6688; font-weight: bold">{:.2e}</span><span style="color: #BA2121">(+/- </span><span style="color: #BB6688; font-weight: bold">{:.2e}</span><span style="color: #BA2121">)&quot;</span><span style="color: #666666">.</span>format(
        degrees[i], <span style="color: #666666">-</span>scores<span style="color: #666666">.</span>mean(), scores<span style="color: #666666">.</span>std()))
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="more-examples-on-bootstrap-and-cross-validation-and-errors" class="anchor">More examples on bootstrap and cross-validation and errors </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression, Ridge, Lasso
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.utils</span> <span style="color: #008000; font-weight: bold">import</span> resample
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error
<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;EoS.csv&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)

<span style="color: #408080; font-style: italic"># Read the EoS data as  csv file and organize the data into two arrays with density and energies</span>
EoS <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_csv(infile, names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;Density&#39;</span>, <span style="color: #BA2121">&#39;Energy&#39;</span>))
EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
EoS <span style="color: #666666">=</span> EoS<span style="color: #666666">.</span>dropna()
Energies <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>]
Density <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Density&#39;</span>]
<span style="color: #408080; font-style: italic">#  The design matrix now as function of various polytrops</span>

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">30</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(Density),Maxpolydegree))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
testerror <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(Maxpolydegree)
trainingerror <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(Maxpolydegree)
polynomial <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(Maxpolydegree)

trials <span style="color: #666666">=</span> <span style="color: #666666">100</span>
<span style="color: #008000; font-weight: bold">for</span> polydegree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>, Maxpolydegree):
    polynomial[polydegree] <span style="color: #666666">=</span> polydegree
    <span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(polydegree):
        X[:,degree] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(degree<span style="color: #666666">/3.0</span>)

<span style="color: #408080; font-style: italic"># loop over trials in order to estimate the expectation value of the MSE</span>
    testerror[polydegree] <span style="color: #666666">=</span> <span style="color: #666666">0.0</span>
    trainingerror[polydegree] <span style="color: #666666">=</span> <span style="color: #666666">0.0</span>
    <span style="color: #008000; font-weight: bold">for</span> samples <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(trials):
        x_train, x_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, Energies, test_size<span style="color: #666666">=0.2</span>)
        model <span style="color: #666666">=</span> LinearRegression(fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)<span style="color: #666666">.</span>fit(x_train, y_train)
        ypred <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(x_train)
        ytilde <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(x_test)
        testerror[polydegree] <span style="color: #666666">+=</span> mean_squared_error(y_test, ytilde)
        trainingerror[polydegree] <span style="color: #666666">+=</span> mean_squared_error(y_train, ypred) 

    testerror[polydegree] <span style="color: #666666">/=</span> trials
    trainingerror[polydegree] <span style="color: #666666">/=</span> trials
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Degree of polynomial: </span><span style="color: #BB6688; font-weight: bold">%3d</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span> polynomial[polydegree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Mean squared error on training data: </span><span style="color: #BB6688; font-weight: bold">%.8f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> trainingerror[polydegree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Mean squared error on test data: </span><span style="color: #BB6688; font-weight: bold">%.8f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> testerror[polydegree])

plt<span style="color: #666666">.</span>plot(polynomial, np<span style="color: #666666">.</span>log10(trainingerror), label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Training Error&#39;</span>)
plt<span style="color: #666666">.</span>plot(polynomial, np<span style="color: #666666">.</span>log10(testerror), label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Test Error&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;Polynomial degree&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;log10[MSE]&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split  -->

<h2 id="the-same-example-but-now-with-cross-validation" class="anchor">The same example but now with cross-validation </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression, Ridge, Lasso
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> KFold
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_val_score


<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;EoS.csv&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)

<span style="color: #408080; font-style: italic"># Read the EoS data as  csv file and organize the data into two arrays with density and energies</span>
EoS <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_csv(infile, names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;Density&#39;</span>, <span style="color: #BA2121">&#39;Energy&#39;</span>))
EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
EoS <span style="color: #666666">=</span> EoS<span style="color: #666666">.</span>dropna()
Energies <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>]
Density <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Density&#39;</span>]
<span style="color: #408080; font-style: italic">#  The design matrix now as function of various polytrops</span>

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">30</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(Density),Maxpolydegree))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
estimated_mse_sklearn <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(Maxpolydegree)
polynomial <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(Maxpolydegree)
k <span style="color: #666666">=5</span>
kfold <span style="color: #666666">=</span> KFold(n_splits <span style="color: #666666">=</span> k)

<span style="color: #008000; font-weight: bold">for</span> polydegree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>, Maxpolydegree):
    polynomial[polydegree] <span style="color: #666666">=</span> polydegree
    <span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(polydegree):
        X[:,degree] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(degree<span style="color: #666666">/3.0</span>)
        OLS <span style="color: #666666">=</span> LinearRegression()
<span style="color: #408080; font-style: italic"># loop over trials in order to estimate the expectation value of the MSE</span>
    estimated_mse_folds <span style="color: #666666">=</span> cross_val_score(OLS, X, Energies, scoring<span style="color: #666666">=</span><span style="color: #BA2121">&#39;neg_mean_squared_error&#39;</span>, cv<span style="color: #666666">=</span>kfold)
<span style="color: #408080; font-style: italic">#[:, np.newaxis]</span>
    estimated_mse_sklearn[polydegree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(<span style="color: #666666">-</span>estimated_mse_folds)

plt<span style="color: #666666">.</span>plot(polynomial, np<span style="color: #666666">.</span>log10(estimated_mse_sklearn), label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Test Error&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;Polynomial degree&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;log10[MSE]&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="cross-validation-with-ridge" class="anchor">Cross-validation with Ridge </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> KFold
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> Ridge
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_val_score
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures

<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">3155</span>)
<span style="color: #408080; font-style: italic"># Generate the data.</span>
n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic"># Decide degree on polynomial to fit</span>
poly <span style="color: #666666">=</span> PolynomialFeatures(degree <span style="color: #666666">=</span> <span style="color: #666666">10</span>)

<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">500</span>
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-3</span>, <span style="color: #666666">5</span>, nlambdas)
<span style="color: #408080; font-style: italic"># Initialize a KFold instance</span>
k <span style="color: #666666">=</span> <span style="color: #666666">5</span>
kfold <span style="color: #666666">=</span> KFold(n_splits <span style="color: #666666">=</span> k)
estimated_mse_sklearn <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
i <span style="color: #666666">=</span> <span style="color: #666666">0</span>
<span style="color: #008000; font-weight: bold">for</span> lmb <span style="color: #AA22FF; font-weight: bold">in</span> lambdas:
    ridge <span style="color: #666666">=</span> Ridge(alpha <span style="color: #666666">=</span> lmb)
    estimated_mse_folds <span style="color: #666666">=</span> cross_val_score(ridge, x, y, scoring<span style="color: #666666">=</span><span style="color: #BA2121">&#39;neg_mean_squared_error&#39;</span>, cv<span style="color: #666666">=</span>kfold)
    estimated_mse_sklearn[i] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(<span style="color: #666666">-</span>estimated_mse_folds)
    i <span style="color: #666666">+=</span> <span style="color: #666666">1</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), estimated_mse_sklearn, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;cross_val_score&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<p>
<!-- --- begin exercise --- -->

<h2 id="exercise-1-making-your-own-data-and-exploring-scikit-learn" class="anchor">Exercise 1: making your own data and exploring scikit-learn </h2>

<p>
We will generate our own dataset for a function \( y(x) \) where \( x \in [0,1] \) and defined by random numbers computed with the uniform distribution. The function \( y \) is a quadratic polynomial in \( x \) with added stochastic noise according to the normal distribution \( N(0,1) \).
The following simple Python instructions define our \( x \) and \( y \) values (with 100 data points).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
</pre></div>
<ol>
<li> Write your own code (following the examples under the <a href="https://compphysics.github.io/MachineLearningECT/doc/pub/Day1/html/Day1-bs.html" target="_self">regression slides</a>) for computing the parametrization of the data set fitting a second-order polynomial.</li> 
<li> Use thereafter <b>scikit-learn</b> (see again the examples in the regression slides) and compare with your own code.</li>   
<li> Using scikit-learn, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as</li>
</ol>

$$ MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

and the \( R^2 \) score function.
If \( \tilde{y}_i \) is the predicted value of the \( i-th \) sample and \( y_i \) is the corresponding true value, then the score \( R^2 \) is defined as
$$
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

where we have defined the mean value  of \( \hat{y} \) as
$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

You can use the functionality included in scikit-learn. If you feel for it, you can use your own program and define functions which compute the above two functions. 
Discuss the meaning of these results. Try also to vary the coefficient in front of the added stochastic noise term and discuss the quality of the fits.

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_1_1" style="font-size: 80%;"></a>
<a href="#exer_1_1" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_1_1">

<p>
The code here is an example of where we define our own design matrix and fit parameters \( \beta \).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>)


<span style="color: #408080; font-style: italic">#  The design matrix now as function of a given polynomial</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(x),<span style="color: #666666">3</span>))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> x
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**2</span>
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)
<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
beta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #008000">print</span>(beta)
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytilde <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_train,ytilde))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytilde))
ypredict <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_test,ypredict))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredict))
</pre></div>
<p>
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="exercise-2-mean-values-and-variances-in-linear-regression" class="anchor">Exercise 2: mean values and variances in linear regression </h2>

<p>
This exercise deals with various mean values ad variances in  linear regression method (here it may be useful to look up chapter 3, equation (3.8) of <a href="https://www.springer.com/gp/book/9780387848570" target="_self">Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer</a>).

<p>
The assumption we have made is 
that there exists a function \( f(\boldsymbol{x}) \) and  a normal distributed error \( \boldsymbol{\varepsilon}\sim \mathcal{N}(0, \sigma^2) \)
which describes our data
$$
\boldsymbol{y} = f(\boldsymbol{x})+\boldsymbol{\varepsilon}
$$

<p>
We then approximate this function with our model from the solution of the linear regression equations (ordinary least squares OLS), that is our
function \( f \) is approximated by \( \boldsymbol{\tilde{y}} \) where we minimized  \( (\boldsymbol{y}-\boldsymbol{\tilde{y}})^2 \), with
$$
f(\boldsymbol{x})\approx \boldsymbol{\tilde{y}} = \boldsymbol{X}\boldsymbol{\beta}.
$$

The matrix \( \boldsymbol{X} \) is the so-called design matrix.

<p>
<b>a)</b>
Show that  the expected value of \( \boldsymbol{y} \) for a given element \( i \) 
$$
\begin{align*} 
\mathbb{E}(y_i) & =\mathbf{X}_{i, \ast} \, \beta, 
\end{align*} 
$$

and that
its variance is 
$$
\begin{align*} \mbox{Var}(y_i) & = \sigma^2.  
\end{align*}
$$

Hence, \( y_i \sim \mathcal{N}( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta}, \sigma^2) \), that is \( \boldsymbol{y} \) follows a normal distribution with 
mean value \( \boldsymbol{X}\boldsymbol{\beta} \) and variance \( \sigma^2 \).

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_2_1" style="font-size: 80%;"></a>
<a href="#exer_2_1" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_2_1">

<p>
We can calculate the expected value of \( \boldsymbol{y} \) for a given element \( i \) 
$$
\begin{align*} 
\mathbb{E}(y_i) & =
\mathbb{E}(\mathbf{X}_{i, \ast} \, \boldsymbol{\beta}) + \mathbb{E}(\varepsilon_i)
\, \, \, = \, \, \, \mathbf{X}_{i, \ast} \, \beta, 
\end{align*} 
$$

while
its variance is 
$$
\begin{align*} \mbox{Var}(y_i) & = \mathbb{E} \{ [y_i
- \mathbb{E}(y_i)]^2 \} \, \, \, = \, \, \, \mathbb{E} ( y_i^2 ) -
[\mathbb{E}(y_i)]^2  \\  & = \mathbb{E} [ ( \mathbf{X}_{i, \ast} \,
\beta + \varepsilon_i )^2] - ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 \\ &
= \mathbb{E} [ ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 + 2 \varepsilon_i
\mathbf{X}_{i, \ast} \, \boldsymbol{\beta} + \varepsilon_i^2 ] - ( \mathbf{X}_{i,
\ast} \, \beta)^2 \\  & = ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 + 2
\mathbb{E}(\varepsilon_i) \mathbf{X}_{i, \ast} \, \boldsymbol{\beta} +
\mathbb{E}(\varepsilon_i^2 ) - ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 
\\ & = \mathbb{E}(\varepsilon_i^2 ) \, \, \, = \, \, \,
\mbox{Var}(\varepsilon_i) \, \, \, = \, \, \, \sigma^2.  
\end{align*}
$$

Hence, \( y_i \sim \mathcal{N}( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta}, \sigma^2) \), that is \( \boldsymbol{y} \) follows a normal distribution with 
mean value \( \boldsymbol{X}\boldsymbol{\beta} \) and variance \( \sigma^2 \) (not be confused with the singular values of the SVD).
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>b)</b>
With the OLS expressions for the optimal parameters \( \boldsymbol{\beta}^{\mathrm{opt}} \) show that
$$
\mathbb{E}(\boldsymbol{\beta}^{\mathrm{opt}}) = \boldsymbol{\beta}.
$$

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_2_2" style="font-size: 80%;"></a>
<a href="#exer_2_2" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_2_2">

$$
\mathbb{E}(\boldsymbol{\beta}^{\mathrm{opt}}) = \mathbb{E}[ (\mathbf{X}^{\top} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbb{E}[ \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1} \mathbf{X}^{T}\mathbf{X}\boldsymbol{\beta}=\boldsymbol{\beta}.
$$

This means that the estimator of the regression parameters is unbiased.
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>c)</b>
Show finally that the variance of \( \boldsymbol{\beta} \) is
$$
\begin{eqnarray*}
\mbox{Var}(\boldsymbol{\beta}^{\mathrm{opt}}) & = & \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1}.
\end{eqnarray*}
$$

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_2_3" style="font-size: 80%;"></a>
<a href="#exer_2_3" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_2_3">

<p>
The variance of \( \boldsymbol{\beta} \) is
$$
\begin{eqnarray*}
\mbox{Var}(\boldsymbol{\beta}^{\mathrm{opt}}) & = & \mathbb{E} \{ [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})] [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})]^{T} \}
\\
& = & \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} - \boldsymbol{\beta}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} - \boldsymbol{\beta}]^{T} \}
\\
% & = & \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y}]^{T} \} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
% & = & \mathbb{E} \{ (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} \, \mathbf{Y}^{T} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1}  \} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
& = & (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \mathbb{E} \{ \mathbf{Y} \, \mathbf{Y}^{T} \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\\
& = & (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \{ \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \,  \mathbf{X}^{T} + \sigma^2 \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
% & = & (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^T \,  \mathbf{X}^T \, \mathbf{X} \, (\mathbf{X}^T % \mathbf{X})^{-1}
% \\
% & & + \, \, \sigma^2 \, (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T  \, \mathbf{X} \, (\mathbf{X}^T \mathbf{X})^{-1} - \boldsymbol{\beta} \boldsymbol{\beta}^T
\\
& = & \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}  + \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\, \, \, = \, \, \, \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1},
\end{eqnarray*}
$$

<p>
where we have used  that \( \mathbb{E} (\mathbf{Y} \mathbf{Y}^{T}) =
\mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \, \mathbf{X}^{T} +
\sigma^2 \, \mathbf{I}_{nn} \). From \( \mbox{Var}(\boldsymbol{\beta}) = \sigma^2
\, (\mathbf{X}^{T} \mathbf{X})^{-1} \), one obtains an estimate of the
variance of the estimate of the \( j \)-th regression coefficient:
\( \boldsymbol{\sigma}^2 (\hat{\beta}_j ) = \boldsymbol{\sigma}^2 \sqrt{
[(\mathbf{X}^{T} \mathbf{X})^{-1}]_{jj} } \). This may be used to
construct a confidence interval for the estimates.

<p>
In a similar way, we can obtain analytical expressions for say the
expectation values of the parameters \( \boldsymbol{\beta} \) and their variance
when we employ Ridge regression, allowing us again to define a confidence interval.
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- !split -->

<p>
<!-- --- begin exercise --- -->

<h2 id="exercise-3-adding-ridge-and-lasso-regression" class="anchor">Exercise 3: Adding Ridge and Lasso Regression </h2>

<p>
This exercise is a continuation of exercise 1. We will
use the same function to generate our data set, still staying with a
simple function \( y(x) \) which we want to fit using linear regression,
but now extending the analysis to include the Ridge and the Lasso
regression methods. You can use the code under the Regression as an example on how to use the Ridge and the Lasso methods, see the <a href="https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression-bs.html" target="_self">regression slides</a>).

<p>
We will thus again generate our own dataset for a function \( y(x) \) where 
\( x \in [0,1] \) and defined by random numbers computed with the uniform
distribution. The function \( y \) is a quadratic polynomial in \( x \) with
added stochastic noise according to the normal distribution \( \cal{N}(0,1) \).

<p>
The following simple Python instructions define our \( x \) and \( y \) values (with 100 data points).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>)
</pre></div>
<p>
<b>a)</b>
Write your own code for the Ridge method (see chapter 3.4 of Hastie <em>et al.</em>, equations (3.43) and (3.44)) and compute the parametrization for different values of \( \lambda \). Compare and analyze your results with those from exercise 3. Study the dependence on \( \lambda \) while also varying the strength of the noise in your expression for \( y(x) \).

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_3_1" style="font-size: 80%;"></a>
<a href="#exer_3_1" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_3_1">

<p>
The code here allows you to perform your own Ridge calculation and perform calculations for various values of the regularization parameter \( \lambda \). This program can easily be extended upon.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n


<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">3155</span>)

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>)

<span style="color: #408080; font-style: italic"># number of features p (here degree of polynomial</span>
p <span style="color: #666666">=</span> <span style="color: #666666">3</span>
<span style="color: #408080; font-style: italic">#  The design matrix now as function of a given polynomial</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(x),p))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> x
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> x<span style="color: #666666">*</span>x
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
OLSbeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #008000">print</span>(OLSbeta)
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytildeOLS <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> OLSbeta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training R2 for OLS&quot;</span>)
<span style="color: #008000">print</span>(R2(y_train,ytildeOLS))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE for OLS&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytildeOLS))
ypredictOLS <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OLSbeta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test R2 for OLS&quot;</span>)
<span style="color: #008000">print</span>(R2(y_test,ypredictOLS))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE OLS&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredictOLS))

<span style="color: #408080; font-style: italic"># Repeat now for Ridge regression and various values of the regularization parameter</span>
I <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(p,p)
<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">20</span>
MSEPredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSETrain <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">1</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    Ridgebeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train<span style="color: #666666">+</span>lmb<span style="color: #666666">*</span>I) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
    <span style="color: #408080; font-style: italic"># and then make the prediction</span>
    ytildeRidge <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> Ridgebeta
    ypredictRidge <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> Ridgebeta
    MSEPredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)
    MSETrain[i] <span style="color: #666666">=</span> MSE(y_train,ytildeRidge)
<span style="color: #408080; font-style: italic"># Now plot the resulys</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSETrain, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge train&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEPredict, <span style="color: #BA2121">&#39;r--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>b)</b>
Repeat the above but using the functionality of <b>Scikit-Learn</b>. Compare your code with the results from <b>Scikit-Learn</b>. Remember to run with the same random numbers for generating \( x \) and \( y \).

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_3_2" style="font-size: 80%;"></a>
<a href="#exer_3_2" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_3_2">

<p>
To use <b>scikit-learn</b> with Ridge, we simply need to add the relevant function <b>Ridge()</b>, as done in the code here.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skl</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n


<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">3155</span>)

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>)

<span style="color: #408080; font-style: italic"># number of features p (here degree of polynomial</span>
p <span style="color: #666666">=</span> <span style="color: #666666">3</span>
<span style="color: #408080; font-style: italic">#  The design matrix now as function of a given polynomial</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(x),p))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> x
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> x<span style="color: #666666">*</span>x
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
OLSbeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #008000">print</span>(OLSbeta)
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytildeOLS <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> OLSbeta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training R2 for OLS&quot;</span>)
<span style="color: #008000">print</span>(R2(y_train,ytildeOLS))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE for OLS&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytildeOLS))
ypredictOLS <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OLSbeta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test R2 for OLS&quot;</span>)
<span style="color: #008000">print</span>(R2(y_test,ypredictOLS))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE OLS&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredictOLS))

<span style="color: #408080; font-style: italic"># Repeat now for Ridge regression and various values of the regularization parameter</span>
I <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(p,p)
<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">100</span>
MSEPredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSEPredictSKL <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSETrain <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">0</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    <span style="color: #408080; font-style: italic"># add ridge</span>
    clf_ridge <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>Ridge(alpha<span style="color: #666666">=</span>lmb)<span style="color: #666666">.</span>fit(X_train, y_train)
    yridge <span style="color: #666666">=</span> clf_ridge<span style="color: #666666">.</span>predict(X_test)
    Ridgebeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train<span style="color: #666666">+</span>lmb<span style="color: #666666">*</span>I) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
    <span style="color: #408080; font-style: italic"># and then make the prediction</span>
    ytildeRidge <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> Ridgebeta
    ypredictRidge <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> Ridgebeta
    MSEPredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)
    MSEPredictSKL[i] <span style="color: #666666">=</span> MSE(y_test,yridge)
    MSETrain[i] <span style="color: #666666">=</span> MSE(y_train,ytildeRidge)
<span style="color: #408080; font-style: italic">#then plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSETrain, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge train&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEPredict, <span style="color: #BA2121">&#39;r--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEPredictSKL, <span style="color: #BA2121">&#39;g--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge sickit-learn Test&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>c)</b>
Our next step is to study the variance of the parameters \( \beta_1 \) and \( \beta_2 \) (assuming that we are parameterizing our function with a second-order polynomial). We will use standard linear regression and the Ridge regression.  You can now opt for either writing your own function or using <b>Scikit-Learn</b> to find the parameters \( \beta \). From your results calculate the variance of these parameters (recall that this is equal to the diagonal elements of the matrix \( (\hat{X}^T\hat{X})+\lambda\hat{I})^{-1} \)). Discuss the results of these variances as functions of \( \lambda \). In particular, try to link your discussion with the discussion in Hastie <em>et al.</em> and their figures 3.10 and  3.11. <b>Scikit-Learn</b> may not provide the variance of the parameters \( \beta \). This needs to be checked. With your own code you can however do so.

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_3_3" style="font-size: 80%;"></a>
<a href="#exer_3_3" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_3_3">

<p>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sklearn.linear_model as skl

<p>
def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

<p>
<!-- A seed just to ensure that the random numbers are the same for every run. -->
<!-- Useful for eventual debugging. -->
np.random.seed(3155)

<p>
x = np.random.rand(100)
y = 2.0+5*x*x+0.1*np.random.randn(100)

<p>
<!-- number of features p (here degree of polynomial -->
p = 3
<!-- The design matrix now as function of a given polynomial -->
X = np.zeros((len(x),p))
X[:,0] = 1.0
X[:,1] = x
X[:,2] = x*x
<!-- We split the data in test and training data -->
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

<p>
<!-- matrix inversion to find beta -->
OLSbeta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train
print(OLSbeta)
<!-- The variance is given by the inverse of the matrix X^TX -->
print(np.linalg.inv(X_train.T @ X_train))

<p>
<!-- Repeat now for Ridge regression and various values of the regularization parameter -->
I = np.eye(p,p)
<!-- Decide which values of lambda to use -->
nlambdas = 10
MSEPredict = np.zeros(nlambdas)
MSEPredictSKL = np.zeros(nlambdas)
MSETrain = np.zeros(nlambdas)
lambdas = np.logspace(-4, 0, nlambdas)
for i in range(nlambdas):
    lmb = lambdas[i]
    Ridgebeta = np.linalg.inv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train
    print(np.linalg.inv(X_train.T @ X_train+lmb*I))
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>d)</b>
Repeat the previous step but add now the Lasso method, see equation (3.53) of Hastie <em>et al.</em>. Discuss your results and compare with standard regression and the Ridge regression results. You can write your own code or use the functionality of <b>scikit-learn</b>.  We recommend the latter since we have not yet discussed how to solve the Lasso equations numerically. Also, you do not need to compute the variance of the parameters \( \beta \) but you can extract their values and study their behavior as functions of the regularization parameter \( \lambda \).

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_3_4" style="font-size: 80%;"></a>
<a href="#exer_3_4" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_3_4">

<p>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sklearn.linear_model as skl
<!-- from sklearn.linear_model import LinearRegression, Ridge, Lasso -->
def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

<p>
<!-- A seed just to ensure that the random numbers are the same for every run. -->
<!-- Useful for eventual debugging. -->
np.random.seed(3155)

<p>
x = np.random.rand(100)
y = 2.0+5*x*x+0.1*np.random.randn(100)

<p>
<!-- number of features p (here degree of polynomial -->
p = 3
<!-- The design matrix now as function of a given polynomial -->
X = np.zeros((len(x),p))
X[:,0] = 1.0
X[:,1] = x
X[:,2] = x*x
<!-- We split the data in test and training data -->
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

<p>
<!-- matrix inversion to find beta -->
OLSbeta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train
print(OLSbeta)
<!-- and then make the prediction -->
ytildeOLS = X_train @ OLSbeta
print("Training R2 for OLS")
print(R2(y_train,ytildeOLS))
print("Training MSE for OLS")
print(MSE(y_train,ytildeOLS))
ypredictOLS = X_test @ OLSbeta
print("Test R2 for OLS")
print(R2(y_test,ypredictOLS))
print("Test MSE OLS")
print(MSE(y_test,ypredictOLS))

<p>
<!-- Repeat now for Ridge regression and various values of the regularization parameter -->
I = np.eye(p,p)
<!-- Decide which values of lambda to use -->
nlambdas = 100
MSEPredictLasso = np.zeros(nlambdas)
MSEPredictRidge = np.zeros(nlambdas)
lambdas = np.logspace(-4, 0, nlambdas)
for i in range(nlambdas):
    lmb = lambdas[i]
    # add ridge
    clf_ridge = skl.Ridge(alpha=lmb).fit(X_train, y_train)
    clf_lasso = skl.Lasso(alpha=lmb).fit(X_train, y_train)
    yridge = clf_ridge.predict(X_test)
    ylasso = clf_lasso.predict(X_test)
    MSEPredictLasso[i] = MSE(y_test,ylasso)
    MSEPredictRidge[i] = MSE(y_test,yridge)
<!-- then plot the results -->
plt.figure()
plt.plot(np.log10(lambdas), MSEPredictRidge, 'r--', label = 'MSE Ridge Test')
plt.plot(np.log10(lambdas), MSEPredictLasso, 'g--', label = 'MSE Lasso Test')
plt.xlabel('log10(lambda)')
plt.ylabel('MSE')
plt.legend()
plt.show()
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>e)</b>
Finally, using <b>Scikit-Learn</b> or your own code, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as
$$ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

and the \( R^2 \) score function.
If \( \tilde{\hat{y}}_i \) is the predicted value of the \( i-th \) sample and \( y_i \) is the corresponding true value, then the score \( R^2 \) is defined as
$$
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

where we have defined the mean value  of \( \hat{y} \) as
$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

Discuss these quantities as functions of the variable \( \lambda \) in the Ridge and Lasso regression methods.

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_3_5" style="font-size: 80%;"></a>
<a href="#exer_3_5" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_3_5">

<p>
These results can all be studied with the codes we have above. These scores are included in the codes above.
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- !split -->

<p>
<!-- --- begin exercise --- -->

<h2 id="exercise-4-normalizing-our-data" class="anchor">Exercise 4: Normalizing our data </h2>

<p>
A much used approach before starting to train the data is  to preprocess our
data. Normally the data may need a rescaling and/or may be sensitive
to extreme values. Scaling the data renders our inputs much more
suitable for the algorithms we want to employ.

<p>
<b>Scikit-Learn</b> has several functions which allow us to rescale the
data, normally resulting in much better results in terms of various
accuracy scores.  The <b>StandardScaler</b> function in <b>Scikit-Learn</b>
ensures that for each feature/predictor we study the mean value is
zero and the variance is one (every column in the design/feature
matrix).  This scaling has the drawback that it does not ensure that
we have a particular maximum or minimum in our data set. Another
function included in <b>Scikit-Learn</b> is the <b>MinMaxScaler</b> which
ensures that all features are exactly between \( 0 \) and \( 1 \). The

<p>
The <b>Normalizer</b> scales each data
point such that the feature vector has a euclidean length of one. In other words, it
projects a data point on the circle (or sphere in the case of higher dimensions) with a
radius of 1. This means every data point is scaled by a different number (by the
inverse of it&#8217;s length).
This normalization is often used when only the direction (or angle) of the data matters,
not the length of the feature vector.

<p>
The <b>RobustScaler</b> works similarly to the StandardScaler in that it
ensures statistical properties for each feature that guarantee that
they are on the same scale. However, the RobustScaler uses the median
and quartiles, instead of mean and variance. This makes the
RobustScaler ignore data points that are very different from the rest
(like measurement errors). These odd data points are also called
outliers, and might often lead to trouble for other scaling
techniques.

<p>
It also common to split the data in a <b>training</b> set and a <b>testing</b> set. A typical split is to use \( 80\% \) of the data for training and the rest
for testing. This can be done as follows with our design matrix \( \boldsymbol{X} \) and data \( \boldsymbol{y} \) (remember to import <b>scikit-learn</b>)
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># split in training and test data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X,y,test_size<span style="color: #666666">=0.2</span>)
</pre></div>
<p>
Then we can use the standard scaler to scale our data as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)
</pre></div>
<p>
In this exercise we want you to to compute the MSE for the training
data and the test data as function of the complexity of a polynomial,
that is the degree of a given polynomial. We want you also to compute the \( R2 \) score as function of the complexity of the model for both training data and test data.  You should also run the calculation with and without scaling.

<p>
One of 
the aims is to reproduce Figure 2.11 of <a href="https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf" target="_self">Hastie et al</a>.
We will also use Ridge and Lasso regression.

<p>
Our data is defined by \( x\in [-3,3] \) with a total of for example \( 100 \) data points.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed()
n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">14</span>
<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
</pre></div>
<p>
where \( y \) is the function we want to fit with a given polynomial.

<p>
<b>a)</b>
Write a first code which sets up a design matrix \( X \) defined by a fifth-order polynomial.  Scale your data and split it in training and test data.

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_4_1" style="font-size: 80%;"></a>
<a href="#exer_4_1" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_4_1">

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression, Ridge, Lasso
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> make_pipeline


np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2018</span>)
n <span style="color: #666666">=</span> <span style="color: #666666">50</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">5</span>
<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
TestError <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
TrainError <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
polydegree <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
x_train, x_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(x, y, test_size<span style="color: #666666">=0.2</span>)
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
x_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(x_train)
x_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(x_test)

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(maxdegree):
    model <span style="color: #666666">=</span> make_pipeline(PolynomialFeatures(degree<span style="color: #666666">=</span>degree), LinearRegression(fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>))
    clf <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(x_train_scale,y_train)
    y_fit <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(x_train_scaled)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(x_test_scaled) 
    polydegree[degree] <span style="color: #666666">=</span> degree
    TestError[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> y_pred)<span style="color: #666666">**2</span>) )
    TrainError[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_train <span style="color: #666666">-</span> y_fit)<span style="color: #666666">**2</span>) )

plt<span style="color: #666666">.</span>plot(polydegree, TestError, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Test Error&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, TrainError, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Train Error&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>b)</b>
Perform an ordinary least squares and compute the means squared error and the \( R2 \) factor for the training data and the test data, with and without scaling.

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_4_2" style="font-size: 80%;"></a>
<a href="#exer_4_2" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_4_2">

<p>
This requires a simple extension to the above code where you simply add a statement calling the \( R2 \) function included in the same code.
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>c)</b>
Add now a model which allows you to make polynomials up to degree \( 15 \).  Perform a standard OLS fitting of the training data and compute the MSE and \( R2 \) for the training and test data and plot both test and training data MSE and \( R2 \) as functions of the polynomial degree. Compare what you see with Figure 2.11 of Hastie et al. Comment your results. For which polynomial degree do you find an optimal MSE (smallest value)?

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_4_3" style="font-size: 80%;"></a>
<a href="#exer_4_3" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_4_3">

<p>
Here you simply need to change the degree of the polynomial in the above code to \( n=15 \).
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>d)</b>
Repeat part (2c) but now using Ridge regressions with various hyperparameters \( \lambda \). Make the same plots for the optimal \( \lambda \) value for each polynomial degree. Compare these results with those from the standard OLS approach.

<p>
<!-- --- begin solution of exercise --- -->

<p>
<a class="glyphicon glyphicon-hand-right showdetails" data-toggle="collapse"
 data-target="#exer_4_4" style="font-size: 80%;"></a>
<a href="#exer_4_4" data-toggle="collapse">
<b>Solution.</b>
</a>
<div class="collapse-group">
<p><div class="collapse" id="exer_4_4">

<p>
Here you need to add for example the same loop over the parameters \( \lambda \) as you did in the first exercise, that is add
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>nlambdas <span style="color: #666666">=</span> <span style="color: #666666">100</span>
MSEPredictRidge <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">0</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    <span style="color: #408080; font-style: italic"># add ridge</span>
    clf_ridge <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>Ridge(alpha<span style="color: #666666">=</span>lmb)<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
</pre></div>
<p>
</div></p>
</div>
</p>

<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- !split -->

<p>
<!-- --- begin exercise --- -->

<h2 id="exercise-5-bias-variance-tradeoff-and-bootstrap" class="anchor">Exercise 5: Bias-Variance tradeoff and Bootstrap </h2>

<p>
This exercise is a continuation of exercise 2 from the second homework set.
In that exercise we computed the MSE-score for the training
data and the test data as functions of the complexity of a polynomial,
that is the degree of a given polynomial.

<p>
One of the aims of that exercise was 
to reproduce Figure 2.11 of <a href="https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf" target="_self">Hastie et al</a>.

<p>
Our data is defined by \( x\in [-3,3] \) with a total of for example \( 100 \) data points.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed()
n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">14</span>
<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
</pre></div>
<p>
where \( y \) is the function we want to fit with a given polynomial.

<h3 id="part-1a-proving-the-bias-variance-tradeoff" class="anchor">Part (1a) Proving the bias-variance tradeoff </h3>

<p>
Consider a
dataset \( \mathcal{L} \) consisting of the data
\( \mathbf{X}_\mathcal{L}=\{(y_j, \boldsymbol{x}_j), j=0\ldots n-1\} \).

<p>
Let us assume that the true data is generated from a noisy model

$$
\boldsymbol{y}=f(\boldsymbol{x}) + \boldsymbol{\epsilon}.
$$

<p>
Here \( \epsilon \) is normally distributed with mean zero and standard
deviation \( \sigma^2 \).

<p>
In our derivation of the ordinary least squares method we defined then
an approximation to the function \( f \) in terms of the parameters
\( \boldsymbol{\beta} \) and the design matrix \( \boldsymbol{X} \) which embody our model,
that is \( \boldsymbol{\tilde{y}}=\boldsymbol{X}\boldsymbol{\beta} \).

<p>
The parameters \( \boldsymbol{\beta} \) are in turn found by optimizing the means
squared error via the so-called cost function

$$
C(\boldsymbol{X},\boldsymbol{\beta}) =\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right].
$$

<p>
Show that you can rewrite  this as
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\frac{1}{n}\sum_i(\tilde{y}_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\sigma^2.
$$

<p>
Explain what the terms mean, which one is the bias and which one is
the variance and discuss their interpretations.

<h3 id="part-1b-adding-bootstrap-and-bias-variance-tradeoff" class="anchor">Part (1b) Adding Bootstrap and Bias-Variance Tradeoff </h3>

<p>
Add now bootstrapping as discussed in the <a href="https://compphysics.github.io/MachineLearningECT/doc/pub/Day1/html/Day1.html" target="_self">Regression lectures</a> (scroll down to the bias-variance code).
Add also the expressions for the bias and the variance as discussed above.

<p>
Discuss the bias and variance tradeoff as function
of your model complexity (the degree of the polynomial) and the number
of data points, and possibly also your training and test data.

<p>
Try to make a figure similar to Fig. 2.11 of Hastie et al. You should include an analysis of the bias and variance for the test results. Figure 2.11 displays only the test and training MSEs while indicating regions of low/high bias and variance. You will most likely not get an
equally smooth curve! Note also that when you calculate the bias, in all applications you don't know the function values \( f_i \). You would hence replace them with the actual data points \( y_i \).

<p>
<!-- --- end exercise --- -->

<p>
<!-- !split -->

<p>
<!-- --- begin exercise --- -->

<h2 id="exercise-6-linear-regression-for-a-two-dimensional-function" class="anchor">Exercise 6: Linear Regression for  a two-dimensional function </h2>

<p>
This is a longer  exercise and the aim is to study in more detail various
regression methods, including the Ordinary Least Squares (OLS) method,
Ridge regression and finally Lasso regression.
The methods are in turn combined with resampling techniques.

<p>
We will study how to fit polynomials to a specific
two-dimensional function called <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a081688.pdf" target="_self">Franke's
function</a>.  This
is a function which has been widely used when testing various
interpolation and fitting algorithms. Furthermore, after having
established the model and the method, we will employ resamling
like the bootstrap from the previous exercise in order to perform a
proper assessment of our models. We will also study in detail the
so-called Bias-Variance trade off.

<p>
The Franke function, which is a weighted sum of four exponentials  reads as follows
$$
\begin{align*}
f(x,y) &= \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
&+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}
$$

<p>
The function will be defined for \( x,y\in [0,1] \).  Our first step will
be to perform an OLS regression analysis of this function, trying out
a polynomial fit with an \( x \) and \( y \) dependence of the form \( [x, y,
x^2, y^2, xy, \dots] \). We will also include cross-validation (or bootstrap) as
resampling technique.  As in homeworks 1 and 2, we can use a uniform
distribution to set up the arrays of values for \( x \) and \( y \), or as in
the example below just a set of fixed 
values for \( x \) and \( y \) with a given step
size.  We will fit a
function (for example a polynomial) of \( x \) and \( y \).  Thereafter we
will repeat much of the same procedure using the Ridge and Lasso
regression methods, introducing thus a dependence on the bias
(penalty) \( \lambda \).

<p>
Finally we are going to use (real) digital terrain data and try to
reproduce these data using the same methods. We will also try to go
beyond the second-order polynomials metioned above and explore 
which polynomial fits the data best.

<p>
The Python fucntion for the Franke function is included here (it performs also a three-dimensional plot of it)
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008000; font-weight: bold">import</span> Axes3D
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> cm
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib.ticker</span> <span style="color: #008000; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">random</span> <span style="color: #008000; font-weight: bold">import</span> random, seed

fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)

<span style="color: #408080; font-style: italic"># Make data.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">0.05</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">0.05</span>)
x, y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(x,y)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">FrankeFunction</span>(x,y):
    term1 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">0.25*</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>) <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>))
    term2 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>((<span style="color: #666666">9*</span>x<span style="color: #666666">+1</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">/49.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.1*</span>(<span style="color: #666666">9*</span>y<span style="color: #666666">+1</span>))
    term3 <span style="color: #666666">=</span> <span style="color: #666666">0.5*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-7</span>)<span style="color: #666666">**2/4.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-3</span>)<span style="color: #666666">**2</span>))
    term4 <span style="color: #666666">=</span> <span style="color: #666666">-0.2*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-4</span>)<span style="color: #666666">**2</span> <span style="color: #666666">-</span> (<span style="color: #666666">9*</span>y<span style="color: #666666">-7</span>)<span style="color: #666666">**2</span>)
    <span style="color: #008000; font-weight: bold">return</span> term1 <span style="color: #666666">+</span> term2 <span style="color: #666666">+</span> term3 <span style="color: #666666">+</span> term4


z <span style="color: #666666">=</span> FrankeFunction(x, y)

<span style="color: #408080; font-style: italic"># Plot the surface.</span>
surf <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(x, y, z, cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>coolwarm,
                       linewidth<span style="color: #666666">=0</span>, antialiased<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># Customize the z axis.</span>
ax<span style="color: #666666">.</span>set_zlim(<span style="color: #666666">-0.10</span>, <span style="color: #666666">1.40</span>)
ax<span style="color: #666666">.</span>zaxis<span style="color: #666666">.</span>set_major_locator(LinearLocator(<span style="color: #666666">10</span>))
ax<span style="color: #666666">.</span>zaxis<span style="color: #666666">.</span>set_major_formatter(FormatStrFormatter(<span style="color: #BA2121">&#39;</span><span style="color: #BB6688; font-weight: bold">%.02f</span><span style="color: #BA2121">&#39;</span>))

<span style="color: #408080; font-style: italic"># Add a color bar which maps values to colors.</span>
fig<span style="color: #666666">.</span>colorbar(surf, shrink<span style="color: #666666">=0.5</span>, aspect<span style="color: #666666">=5</span>)

plt<span style="color: #666666">.</span>show()
</pre></div>

<h3 id="a-ordinary-least-square-on-the-franke-function-with-resampling" class="anchor">(a) Ordinary Least Square on the Franke function  with resampling </h3>

<p>
We will generate our own dataset for a function
\( \mathrm{FrankeFunction}(x,y) \) with \( x,y \in [0,1] \). The function
\( f(x,y) \) is the Franke function. You should explore also the addition
an added stochastic noise to this function using the normal
distribution \( \cal{N}(0,1) \).

<p>
Write your own code (using either a matrix inversion or a singular
value decomposition from e.g., <b>numpy</b> ) or use your code from
homeworks 1 and 2 and perform a standard least square regression
analysis using polynomials in \( x \) and \( y \) up to fifth order. You can use <b>scikit-learn</b> as well.

<p>
Evaluate the Mean Squared error (MSE)

$$ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

<p>
and the \( R^2 \) score function.  If \( \tilde{\hat{y}}_i \) is the predicted
value of the \( i-th \) sample and \( y_i \) is the corresponding true value,
then the score \( R^2 \) is defined as

$$
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

<p>
where we have defined the mean value  of \( \hat{y} \) as

$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

<p>
To set up the design matrix, the following code can be used
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">FrankeFunction</span>(x,y):
	term1 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">0.25*</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>) <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>))
	term2 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>((<span style="color: #666666">9*</span>x<span style="color: #666666">+1</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">/49.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.1*</span>(<span style="color: #666666">9*</span>y<span style="color: #666666">+1</span>))
	term3 <span style="color: #666666">=</span> <span style="color: #666666">0.5*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-7</span>)<span style="color: #666666">**2/4.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-3</span>)<span style="color: #666666">**2</span>))
	term4 <span style="color: #666666">=</span> <span style="color: #666666">-0.2*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-4</span>)<span style="color: #666666">**2</span> <span style="color: #666666">-</span> (<span style="color: #666666">9*</span>y<span style="color: #666666">-7</span>)<span style="color: #666666">**2</span>)
	<span style="color: #008000; font-weight: bold">return</span> term1 <span style="color: #666666">+</span> term2 <span style="color: #666666">+</span> term3 <span style="color: #666666">+</span> term4


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">create_X</span>(x, y, n ):
	<span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(x<span style="color: #666666">.</span>shape) <span style="color: #666666">&gt;</span> <span style="color: #666666">1</span>:
		x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ravel(x)
		y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ravel(y)

	N <span style="color: #666666">=</span> <span style="color: #008000">len</span>(x)
	l <span style="color: #666666">=</span> <span style="color: #008000">int</span>((n<span style="color: #666666">+1</span>)<span style="color: #666666">*</span>(n<span style="color: #666666">+2</span>)<span style="color: #666666">/2</span>)		<span style="color: #408080; font-style: italic"># Number of elements in beta</span>
	X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ones((N,l))

	<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,n<span style="color: #666666">+1</span>):
		q <span style="color: #666666">=</span> <span style="color: #008000">int</span>((i)<span style="color: #666666">*</span>(i<span style="color: #666666">+1</span>)<span style="color: #666666">/2</span>)
		<span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(i<span style="color: #666666">+1</span>):
			X[:,q<span style="color: #666666">+</span>k] <span style="color: #666666">=</span> (x<span style="color: #666666">**</span>(i<span style="color: #666666">-</span>k))<span style="color: #666666">*</span>(y<span style="color: #666666">**</span>k)

	<span style="color: #008000; font-weight: bold">return</span> X


<span style="color: #408080; font-style: italic"># Making meshgrid of datapoints and compute Franke&#39;s function</span>
n <span style="color: #666666">=</span> <span style="color: #666666">5</span>
N <span style="color: #666666">=</span> <span style="color: #666666">1000</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>uniform(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N))
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>uniform(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N))
z <span style="color: #666666">=</span> FrankeFunction(x, y)
X <span style="color: #666666">=</span> create_X(x, y, n<span style="color: #666666">=</span>n)    
</pre></div>

<h3 id="part-b-resampling-techniques-adding-more-complexity" class="anchor">Part (b) Resampling techniques, adding more complexity </h3>

<p>
Perform a resampling of the data where you split the data in training
data and test data. Here you can write your own function or use the
function for splitting training data provided by <b>Scikit-Learn</b>.
This function is called \( train\_test\_split \).   You should also renormalize your data.

<p>
It is normal in essentially all Machine Learning studies to split the
data in a training set and a test set (sometimes also an additional
validation set).  There
is no explicit recipe for how much data should be included as training
data and say test data.  An accepted rule of thumb is to use
approximately \( 2/3 \) to \( 4/5 \) of the data as training data.

<p>
Use then the _bootstrap code you developed in the previous exercise to resample your data
and evaluate again the MSE function resulting
from the test data.

<h3 id="part-c-bias-variance-tradeoff" class="anchor">Part (c): Bias-variance tradeoff </h3>

<p>
With a code which does OLS and includes bootstrap
we will now discuss the bias-variance tradeoff in the context of
continuous predictions such as regression. However, many of the
intuitions and ideas discussed here also carry over to classification
tasks and basically all Machine Learning algorithms.

<p>
Use the code from exercise 1 above and implement the bootstrap
resampling and perform a bias-variance tradeoff analysis like you did
in exercise 1.

<h3 id="part-d-ridge-regression-on-the-franke-function-with-resampling" class="anchor">Part (d): Ridge Regression on the Franke function  with resampling </h3>

<p>
Write your own code for the Ridge method, either using matrix
inversion or the singular value decomposition ir use <b>scikit-learn</b> 
Perform the same analysis as in the
previous three steps (for the same polynomials and include resampling
techniques) but now for different values of \( \lambda \). Compare and
analyze your results with those obtained in parts 2a-2c). Study the
dependence on \( \lambda \).

<p>
Study also the bias-variance tradeoff as function of various values of
the parameter \( \lambda \). Comment your results.

<h3 id="part-e-lasso-regression-on-the-franke-function-with-resampling" class="anchor">Part (e): Lasso Regression on the Franke function  with resampling </h3>

<p>
This part is essentially a repeat of the previous ones, but now
with Lasso regression. Write either your own code or
use the functionalities of <b>Scikit-Learn</b> (recommended). 
Give a
critical discussion of the three methods and a judgement of which
model fits the data best.

<p>
<!-- --- end exercise --- -->

<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2021, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

