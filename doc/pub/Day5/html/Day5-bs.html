<!--
Automatically generated HTML file from DocOnce source
(https://github.com/doconce/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Decision trees, Random Forests, Bagging and Boosting">

<title>Decision trees, Random Forests, Bagging and Boosting</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Decision trees, overarching aims',
               2,
               None,
               'decision-trees-overarching-aims'),
              ('Basics of a tree', 2, None, 'basics-of-a-tree'),
              ('A Sketch of a Tree, Regression problem',
               2,
               None,
               'a-sketch-of-a-tree-regression-problem'),
              ('A Sketch of a Tree, Classification  problem',
               2,
               None,
               'a-sketch-of-a-tree-classification-problem'),
              ('A typical Decision Tree with its pertinent Jargon, '
               'Classification Problem',
               2,
               None,
               'a-typical-decision-tree-with-its-pertinent-jargon-classification-problem'),
              ('General Features', 2, None, 'general-features'),
              ('How do we set it up?', 2, None, 'how-do-we-set-it-up'),
              ('Decision trees and Regression',
               2,
               None,
               'decision-trees-and-regression'),
              ('Building a tree, regression',
               2,
               None,
               'building-a-tree-regression'),
              ('A top-down approach, recursive binary splitting',
               2,
               None,
               'a-top-down-approach-recursive-binary-splitting'),
              ('Making a tree', 2, None, 'making-a-tree'),
              ('Pruning the tree', 2, None, 'pruning-the-tree'),
              ('Cost complexity pruning', 2, None, 'cost-complexity-pruning'),
              ('Schematic Regression Procedure',
               2,
               None,
               'schematic-regression-procedure'),
              ('A Classification Tree', 2, None, 'a-classification-tree'),
              ('Growing a classification tree',
               2,
               None,
               'growing-a-classification-tree'),
              ('Classification tree, how to split nodes',
               2,
               None,
               'classification-tree-how-to-split-nodes'),
              ('Visualizing the Tree, Classification',
               2,
               None,
               'visualizing-the-tree-classification'),
              ('Visualizing the Tree, The Moons',
               2,
               None,
               'visualizing-the-tree-the-moons'),
              ('Other ways of visualizing the trees',
               2,
               None,
               'other-ways-of-visualizing-the-trees'),
              ('Printing out as text', 2, None, 'printing-out-as-text'),
              ('Algorithms for Setting up Decision Trees',
               2,
               None,
               'algorithms-for-setting-up-decision-trees'),
              ('The CART algorithm for Classification',
               2,
               None,
               'the-cart-algorithm-for-classification'),
              ('The CART algorithm for Regression',
               2,
               None,
               'the-cart-algorithm-for-regression'),
              ('Cancer Data again now with Decision Trees and other Methods',
               2,
               None,
               'cancer-data-again-now-with-decision-trees-and-other-methods'),
              ('Another example, the moons again',
               2,
               None,
               'another-example-the-moons-again'),
              ('Playing around with regions',
               2,
               None,
               'playing-around-with-regions'),
              ('Regression trees', 2, None, 'regression-trees'),
              ('Final regressor code', 2, None, 'final-regressor-code'),
              ('Pros and cons of trees, pros',
               2,
               None,
               'pros-and-cons-of-trees-pros'),
              ('Disadvantages', 2, None, 'disadvantages'),
              ('Ensemble Methods: From a Single Tree to Many Trees and Extreme '
               'Boosting, Meet the Jungle of Methods',
               2,
               None,
               'ensemble-methods-from-a-single-tree-to-many-trees-and-extreme-boosting-meet-the-jungle-of-methods'),
              ('An Overview of Ensemble Methods',
               2,
               None,
               'an-overview-of-ensemble-methods'),
              ('Bagging', 2, None, 'bagging'),
              ('More bagging', 2, None, 'more-bagging'),
              ('Simple Voting Example, head or tail',
               2,
               None,
               'simple-voting-example-head-or-tail'),
              ('Using the Voting Classifier',
               2,
               None,
               'using-the-voting-classifier'),
              ('Please, not the moons again! Voting and Bagging',
               2,
               None,
               'please-not-the-moons-again-voting-and-bagging'),
              ('Bagging Examples', 2, None, 'bagging-examples'),
              ('Making your own Bootstrap: Changing the Level of the Decision '
               'Tree',
               2,
               None,
               'making-your-own-bootstrap-changing-the-level-of-the-decision-tree'),
              ('Why Voting?', 2, None, 'why-voting'),
              ('Tossing coins', 2, None, 'tossing-coins'),
              ('Standard imports first', 2, None, 'standard-imports-first'),
              ('Simple Voting Example, head or tail',
               2,
               None,
               'simple-voting-example-head-or-tail'),
              ('Using the Voting Classifier',
               2,
               None,
               'using-the-voting-classifier'),
              ('Voting and Bagging', 2, None, 'voting-and-bagging'),
              ('Random forests', 2, None, 'random-forests'),
              ('Random Forest Algorithm', 2, None, 'random-forest-algorithm'),
              ('Random Forests Compared with other Methods on the Cancer Data',
               2,
               None,
               'random-forests-compared-with-other-methods-on-the-cancer-data'),
              ('Compare  Bagging on Trees with Random Forests',
               2,
               None,
               'compare-bagging-on-trees-with-random-forests'),
              ("Boosting, a Bird's Eye View",
               2,
               None,
               'boosting-a-bird-s-eye-view'),
              ('What is boosting? Additive Modelling/Iterative Fitting',
               2,
               None,
               'what-is-boosting-additive-modelling-iterative-fitting'),
              ('Iterative Fitting, Regression and Squared-error Cost Function',
               2,
               None,
               'iterative-fitting-regression-and-squared-error-cost-function'),
              ('Squared-Error Example and Iterative Fitting',
               2,
               None,
               'squared-error-example-and-iterative-fitting'),
              ('Iterative Fitting, Classification and AdaBoost',
               2,
               None,
               'iterative-fitting-classification-and-adaboost'),
              ('Adaptive Boosting, AdaBoost',
               2,
               None,
               'adaptive-boosting-adaboost'),
              ('Building up AdaBoost', 2, None, 'building-up-adaboost'),
              ('Adaptive boosting: AdaBoost, Basic Algorithm',
               2,
               None,
               'adaptive-boosting-adaboost-basic-algorithm'),
              ('Basic Steps of AdaBoost', 2, None, 'basic-steps-of-adaboost'),
              ('AdaBoost Examples', 2, None, 'adaboost-examples'),
              ('Gradient boosting: Basics with Steepest Descent/Functional '
               'Gradient Descent',
               2,
               None,
               'gradient-boosting-basics-with-steepest-descent-functional-gradient-descent'),
              ('The Squared-Error again! Steepest Descent',
               2,
               None,
               'the-squared-error-again-steepest-descent'),
              ('Steepest Descent Example', 2, None, 'steepest-descent-example'),
              ('Gradient Boosting, algorithm',
               2,
               None,
               'gradient-boosting-algorithm'),
              ('Gradient Boosting, Examples of Regression',
               2,
               None,
               'gradient-boosting-examples-of-regression'),
              ('Gradient Boosting, Classification Example',
               2,
               None,
               'gradient-boosting-classification-example'),
              ('XGBoost: Extreme Gradient Boosting',
               2,
               None,
               'xgboost-extreme-gradient-boosting'),
              ('Regression Case', 2, None, 'regression-case'),
              ('Xgboost on the Cancer Data',
               2,
               None,
               'xgboost-on-the-cancer-data'),
              ('Topics we have covered', 2, None, 'topics-we-have-covered'),
              ('Statistical analysis and optimization of data',
               2,
               None,
               'statistical-analysis-and-optimization-of-data'),
              ('Machine learning', 2, None, 'machine-learning'),
              ('Learning outcomes and overarching aims of this course',
               2,
               None,
               'learning-outcomes-and-overarching-aims-of-this-course'),
              ('Perspective on Machine Learning',
               2,
               None,
               'perspective-on-machine-learning'),
              ('Machine Learning Research',
               2,
               None,
               'machine-learning-research'),
              ('Starting your Machine Learning Project',
               2,
               None,
               'starting-your-machine-learning-project'),
              ('Choose a Model and Algorithm',
               2,
               None,
               'choose-a-model-and-algorithm'),
              ('Preparing Your Data', 2, None, 'preparing-your-data'),
              ('Which Activation and Weights to Choose in Neural Networks',
               2,
               None,
               'which-activation-and-weights-to-choose-in-neural-networks'),
              ('Optimization Methods and Hyperparameters',
               2,
               None,
               'optimization-methods-and-hyperparameters'),
              ('Resampling', 2, None, 'resampling'),
              ("What's the future like?", 2, None, 'what-s-the-future-like'),
              ('Types of Machine Learning, a repetition',
               2,
               None,
               'types-of-machine-learning-a-repetition'),
              ('Autoencoders: Overarching view',
               2,
               None,
               'autoencoders-overarching-view'),
              ('Bayesian Machine Learning',
               2,
               None,
               'bayesian-machine-learning'),
              ('Reinforcement Learning', 2, None, 'reinforcement-learning'),
              ('Transfer learning', 2, None, 'transfer-learning'),
              ('Adversarial learning', 2, None, 'adversarial-learning'),
              ('Dual learning', 2, None, 'dual-learning'),
              ('Distributed machine learning',
               2,
               None,
               'distributed-machine-learning'),
              ('Meta learning', 2, None, 'meta-learning'),
              ('The Challenges Facing Machine Learning',
               2,
               None,
               'the-challenges-facing-machine-learning'),
              ('Explainable machine learning',
               2,
               None,
               'explainable-machine-learning'),
              ('Quantum machine learning', 2, None, 'quantum-machine-learning'),
              ('Quantum machine learning algorithms based on linear algebra',
               2,
               None,
               'quantum-machine-learning-algorithms-based-on-linear-algebra'),
              ('Quantum reinforcement learning',
               2,
               None,
               'quantum-reinforcement-learning'),
              ('Quantum deep learning', 2, None, 'quantum-deep-learning'),
              ('Social machine learning', 2, None, 'social-machine-learning'),
              ('The last words?', 2, None, 'the-last-words')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="Day5-bs.html">Decision trees, Random Forests, Bagging and Boosting</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#decision-trees-overarching-aims" style="font-size: 80%;">Decision trees, overarching aims</a></li>
     <!-- navigation toc: --> <li><a href="#basics-of-a-tree" style="font-size: 80%;">Basics of a tree</a></li>
     <!-- navigation toc: --> <li><a href="#a-sketch-of-a-tree-regression-problem" style="font-size: 80%;">A Sketch of a Tree, Regression problem</a></li>
     <!-- navigation toc: --> <li><a href="#a-sketch-of-a-tree-classification-problem" style="font-size: 80%;">A Sketch of a Tree, Classification  problem</a></li>
     <!-- navigation toc: --> <li><a href="#a-typical-decision-tree-with-its-pertinent-jargon-classification-problem" style="font-size: 80%;">A typical Decision Tree with its pertinent Jargon, Classification Problem</a></li>
     <!-- navigation toc: --> <li><a href="#general-features" style="font-size: 80%;">General Features</a></li>
     <!-- navigation toc: --> <li><a href="#how-do-we-set-it-up" style="font-size: 80%;">How do we set it up?</a></li>
     <!-- navigation toc: --> <li><a href="#decision-trees-and-regression" style="font-size: 80%;">Decision trees and Regression</a></li>
     <!-- navigation toc: --> <li><a href="#building-a-tree-regression" style="font-size: 80%;">Building a tree, regression</a></li>
     <!-- navigation toc: --> <li><a href="#a-top-down-approach-recursive-binary-splitting" style="font-size: 80%;">A top-down approach, recursive binary splitting</a></li>
     <!-- navigation toc: --> <li><a href="#making-a-tree" style="font-size: 80%;">Making a tree</a></li>
     <!-- navigation toc: --> <li><a href="#pruning-the-tree" style="font-size: 80%;">Pruning the tree</a></li>
     <!-- navigation toc: --> <li><a href="#cost-complexity-pruning" style="font-size: 80%;">Cost complexity pruning</a></li>
     <!-- navigation toc: --> <li><a href="#schematic-regression-procedure" style="font-size: 80%;">Schematic Regression Procedure</a></li>
     <!-- navigation toc: --> <li><a href="#a-classification-tree" style="font-size: 80%;">A Classification Tree</a></li>
     <!-- navigation toc: --> <li><a href="#growing-a-classification-tree" style="font-size: 80%;">Growing a classification tree</a></li>
     <!-- navigation toc: --> <li><a href="#classification-tree-how-to-split-nodes" style="font-size: 80%;">Classification tree, how to split nodes</a></li>
     <!-- navigation toc: --> <li><a href="#visualizing-the-tree-classification" style="font-size: 80%;">Visualizing the Tree, Classification</a></li>
     <!-- navigation toc: --> <li><a href="#visualizing-the-tree-the-moons" style="font-size: 80%;">Visualizing the Tree, The Moons</a></li>
     <!-- navigation toc: --> <li><a href="#other-ways-of-visualizing-the-trees" style="font-size: 80%;">Other ways of visualizing the trees</a></li>
     <!-- navigation toc: --> <li><a href="#printing-out-as-text" style="font-size: 80%;">Printing out as text</a></li>
     <!-- navigation toc: --> <li><a href="#algorithms-for-setting-up-decision-trees" style="font-size: 80%;">Algorithms for Setting up Decision Trees</a></li>
     <!-- navigation toc: --> <li><a href="#the-cart-algorithm-for-classification" style="font-size: 80%;">The CART algorithm for Classification</a></li>
     <!-- navigation toc: --> <li><a href="#the-cart-algorithm-for-regression" style="font-size: 80%;">The CART algorithm for Regression</a></li>
     <!-- navigation toc: --> <li><a href="#cancer-data-again-now-with-decision-trees-and-other-methods" style="font-size: 80%;">Cancer Data again now with Decision Trees and other Methods</a></li>
     <!-- navigation toc: --> <li><a href="#another-example-the-moons-again" style="font-size: 80%;">Another example, the moons again</a></li>
     <!-- navigation toc: --> <li><a href="#playing-around-with-regions" style="font-size: 80%;">Playing around with regions</a></li>
     <!-- navigation toc: --> <li><a href="#regression-trees" style="font-size: 80%;">Regression trees</a></li>
     <!-- navigation toc: --> <li><a href="#final-regressor-code" style="font-size: 80%;">Final regressor code</a></li>
     <!-- navigation toc: --> <li><a href="#pros-and-cons-of-trees-pros" style="font-size: 80%;">Pros and cons of trees, pros</a></li>
     <!-- navigation toc: --> <li><a href="#disadvantages" style="font-size: 80%;">Disadvantages</a></li>
     <!-- navigation toc: --> <li><a href="#ensemble-methods-from-a-single-tree-to-many-trees-and-extreme-boosting-meet-the-jungle-of-methods" style="font-size: 80%;">Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
     <!-- navigation toc: --> <li><a href="#an-overview-of-ensemble-methods" style="font-size: 80%;">An Overview of Ensemble Methods</a></li>
     <!-- navigation toc: --> <li><a href="#bagging" style="font-size: 80%;">Bagging</a></li>
     <!-- navigation toc: --> <li><a href="#more-bagging" style="font-size: 80%;">More bagging</a></li>
     <!-- navigation toc: --> <li><a href="#simple-voting-example-head-or-tail" style="font-size: 80%;">Simple Voting Example, head or tail</a></li>
     <!-- navigation toc: --> <li><a href="#using-the-voting-classifier" style="font-size: 80%;">Using the Voting Classifier</a></li>
     <!-- navigation toc: --> <li><a href="#please-not-the-moons-again-voting-and-bagging" style="font-size: 80%;">Please, not the moons again! Voting and Bagging</a></li>
     <!-- navigation toc: --> <li><a href="#bagging-examples" style="font-size: 80%;">Bagging Examples</a></li>
     <!-- navigation toc: --> <li><a href="#making-your-own-bootstrap-changing-the-level-of-the-decision-tree" style="font-size: 80%;">Making your own Bootstrap: Changing the Level of the Decision Tree</a></li>
     <!-- navigation toc: --> <li><a href="#why-voting" style="font-size: 80%;">Why Voting?</a></li>
     <!-- navigation toc: --> <li><a href="#tossing-coins" style="font-size: 80%;">Tossing coins</a></li>
     <!-- navigation toc: --> <li><a href="#standard-imports-first" style="font-size: 80%;">Standard imports first</a></li>
     <!-- navigation toc: --> <li><a href="#simple-voting-example-head-or-tail" style="font-size: 80%;">Simple Voting Example, head or tail</a></li>
     <!-- navigation toc: --> <li><a href="#using-the-voting-classifier" style="font-size: 80%;">Using the Voting Classifier</a></li>
     <!-- navigation toc: --> <li><a href="#voting-and-bagging" style="font-size: 80%;">Voting and Bagging</a></li>
     <!-- navigation toc: --> <li><a href="#random-forests" style="font-size: 80%;">Random forests</a></li>
     <!-- navigation toc: --> <li><a href="#random-forest-algorithm" style="font-size: 80%;">Random Forest Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="#random-forests-compared-with-other-methods-on-the-cancer-data" style="font-size: 80%;">Random Forests Compared with other Methods on the Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="#compare-bagging-on-trees-with-random-forests" style="font-size: 80%;">Compare  Bagging on Trees with Random Forests</a></li>
     <!-- navigation toc: --> <li><a href="#boosting-a-bird-s-eye-view" style="font-size: 80%;">Boosting, a Bird's Eye View</a></li>
     <!-- navigation toc: --> <li><a href="#what-is-boosting-additive-modelling-iterative-fitting" style="font-size: 80%;">What is boosting? Additive Modelling/Iterative Fitting</a></li>
     <!-- navigation toc: --> <li><a href="#iterative-fitting-regression-and-squared-error-cost-function" style="font-size: 80%;">Iterative Fitting, Regression and Squared-error Cost Function</a></li>
     <!-- navigation toc: --> <li><a href="#squared-error-example-and-iterative-fitting" style="font-size: 80%;">Squared-Error Example and Iterative Fitting</a></li>
     <!-- navigation toc: --> <li><a href="#iterative-fitting-classification-and-adaboost" style="font-size: 80%;">Iterative Fitting, Classification and AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="#adaptive-boosting-adaboost" style="font-size: 80%;">Adaptive Boosting, AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="#building-up-adaboost" style="font-size: 80%;">Building up AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="#adaptive-boosting-adaboost-basic-algorithm" style="font-size: 80%;">Adaptive boosting: AdaBoost, Basic Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="#basic-steps-of-adaboost" style="font-size: 80%;">Basic Steps of AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="#adaboost-examples" style="font-size: 80%;">AdaBoost Examples</a></li>
     <!-- navigation toc: --> <li><a href="#gradient-boosting-basics-with-steepest-descent-functional-gradient-descent" style="font-size: 80%;">Gradient boosting: Basics with Steepest Descent/Functional Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="#the-squared-error-again-steepest-descent" style="font-size: 80%;">The Squared-Error again! Steepest Descent</a></li>
     <!-- navigation toc: --> <li><a href="#steepest-descent-example" style="font-size: 80%;">Steepest Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="#gradient-boosting-algorithm" style="font-size: 80%;">Gradient Boosting, algorithm</a></li>
     <!-- navigation toc: --> <li><a href="#gradient-boosting-examples-of-regression" style="font-size: 80%;">Gradient Boosting, Examples of Regression</a></li>
     <!-- navigation toc: --> <li><a href="#gradient-boosting-classification-example" style="font-size: 80%;">Gradient Boosting, Classification Example</a></li>
     <!-- navigation toc: --> <li><a href="#xgboost-extreme-gradient-boosting" style="font-size: 80%;">XGBoost: Extreme Gradient Boosting</a></li>
     <!-- navigation toc: --> <li><a href="#regression-case" style="font-size: 80%;">Regression Case</a></li>
     <!-- navigation toc: --> <li><a href="#xgboost-on-the-cancer-data" style="font-size: 80%;">Xgboost on the Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="#topics-we-have-covered" style="font-size: 80%;">Topics we have covered</a></li>
     <!-- navigation toc: --> <li><a href="#statistical-analysis-and-optimization-of-data" style="font-size: 80%;">Statistical analysis and optimization of data</a></li>
     <!-- navigation toc: --> <li><a href="#machine-learning" style="font-size: 80%;">Machine learning</a></li>
     <!-- navigation toc: --> <li><a href="#learning-outcomes-and-overarching-aims-of-this-course" style="font-size: 80%;">Learning outcomes and overarching aims of this course</a></li>
     <!-- navigation toc: --> <li><a href="#perspective-on-machine-learning" style="font-size: 80%;">Perspective on Machine Learning</a></li>
     <!-- navigation toc: --> <li><a href="#machine-learning-research" style="font-size: 80%;">Machine Learning Research</a></li>
     <!-- navigation toc: --> <li><a href="#starting-your-machine-learning-project" style="font-size: 80%;">Starting your Machine Learning Project</a></li>
     <!-- navigation toc: --> <li><a href="#choose-a-model-and-algorithm" style="font-size: 80%;">Choose a Model and Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="#preparing-your-data" style="font-size: 80%;">Preparing Your Data</a></li>
     <!-- navigation toc: --> <li><a href="#which-activation-and-weights-to-choose-in-neural-networks" style="font-size: 80%;">Which Activation and Weights to Choose in Neural Networks</a></li>
     <!-- navigation toc: --> <li><a href="#optimization-methods-and-hyperparameters" style="font-size: 80%;">Optimization Methods and Hyperparameters</a></li>
     <!-- navigation toc: --> <li><a href="#resampling" style="font-size: 80%;">Resampling</a></li>
     <!-- navigation toc: --> <li><a href="#what-s-the-future-like" style="font-size: 80%;">What's the future like?</a></li>
     <!-- navigation toc: --> <li><a href="#types-of-machine-learning-a-repetition" style="font-size: 80%;">Types of Machine Learning, a repetition</a></li>
     <!-- navigation toc: --> <li><a href="#autoencoders-overarching-view" style="font-size: 80%;">Autoencoders: Overarching view</a></li>
     <!-- navigation toc: --> <li><a href="#bayesian-machine-learning" style="font-size: 80%;">Bayesian Machine Learning</a></li>
     <!-- navigation toc: --> <li><a href="#reinforcement-learning" style="font-size: 80%;">Reinforcement Learning</a></li>
     <!-- navigation toc: --> <li><a href="#transfer-learning" style="font-size: 80%;">Transfer learning</a></li>
     <!-- navigation toc: --> <li><a href="#adversarial-learning" style="font-size: 80%;">Adversarial learning</a></li>
     <!-- navigation toc: --> <li><a href="#dual-learning" style="font-size: 80%;">Dual learning</a></li>
     <!-- navigation toc: --> <li><a href="#distributed-machine-learning" style="font-size: 80%;">Distributed machine learning</a></li>
     <!-- navigation toc: --> <li><a href="#meta-learning" style="font-size: 80%;">Meta learning</a></li>
     <!-- navigation toc: --> <li><a href="#the-challenges-facing-machine-learning" style="font-size: 80%;">The Challenges Facing Machine Learning</a></li>
     <!-- navigation toc: --> <li><a href="#explainable-machine-learning" style="font-size: 80%;">Explainable machine learning</a></li>
     <!-- navigation toc: --> <li><a href="#quantum-machine-learning" style="font-size: 80%;">Quantum machine learning</a></li>
     <!-- navigation toc: --> <li><a href="#quantum-machine-learning-algorithms-based-on-linear-algebra" style="font-size: 80%;">Quantum machine learning algorithms based on linear algebra</a></li>
     <!-- navigation toc: --> <li><a href="#quantum-reinforcement-learning" style="font-size: 80%;">Quantum reinforcement learning</a></li>
     <!-- navigation toc: --> <li><a href="#quantum-deep-learning" style="font-size: 80%;">Quantum deep learning</a></li>
     <!-- navigation toc: --> <li><a href="#social-machine-learning" style="font-size: 80%;">Social machine learning</a></li>
     <!-- navigation toc: --> <li><a href="#the-last-words" style="font-size: 80%;">The last words?</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Decision trees, Random Forests, Bagging and Boosting</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Feb 14, 2021</h4></center> <!-- date -->
<br>
<p>
<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->

<h2 id="decision-trees-overarching-aims" class="anchor">Decision trees, overarching aims  </h2>

<p>
We start here with the most basic algorithm, the so-called decision
tree. With this basic algorithm we can in turn build more complex
networks, spanning from homogeneous and heterogenous forests (bagging,
random forests and more) to one of the most popular supervised
algorithms nowadays, the extreme gradient boosting, or just
XGBoost. But let us start with the simplest possible ingredient.

<p>
Decision trees are supervised learning algorithms used for both,
classification and regression tasks.

<p>
The main idea of decision trees
is to find those descriptive features which contain the most
<b>information</b> regarding the target feature and then split the dataset
along the values of these features such that the target feature values
for the resulting underlying datasets are as pure as possible.

<p>
The descriptive features which reproduce best the target/output features are normally  said
to be the most informative ones. The process of finding the <b>most
informative</b> feature is done until we accomplish a stopping criteria
where we then finally end up in so called <b>leaf nodes</b>.

<p>
<!-- !split -->

<h2 id="basics-of-a-tree" class="anchor">Basics of a tree </h2>

<p>
A decision tree is typically divided into a <b>root node</b>, the <b>interior nodes</b>,
and the final <b>leaf nodes</b> or just <b>leaves</b>. These entities are then connected by so-called <b>branches</b>.

<p>
The leaf nodes
contain the predictions we will make for new query instances presented
to our trained model. This is possible since the model has 
learned the underlying structure of the training data and hence can,
given some assumptions, make predictions about the target feature value
(class) of unseen query instances.

<p>
<!-- !split -->

<h2 id="a-sketch-of-a-tree-regression-problem" class="anchor">A Sketch of a Tree, Regression problem  </h2>

<p>
<!-- FIGURE: [DataFiles/Regsimpletree.png, width=600 frac=0.8] -->

<p>
<!-- !split -->

<h2 id="a-sketch-of-a-tree-classification-problem" class="anchor">A Sketch of a Tree, Classification  problem  </h2>

<p>
<!-- FIGURE: [DataFiles/Classimpletree.png, width=600 frac=0.8] -->

<p>
<!-- !split -->

<h2 id="a-typical-decision-tree-with-its-pertinent-jargon-classification-problem" class="anchor">A typical Decision Tree with its pertinent Jargon, Classification Problem </h2>

<p>
<br /><br /><center><p><img src="DataFiles/cancer.png" align="bottom" width=600></p></center><br /><br />

<p>
This tree was produced using the Wisconsin cancer data (discussed here as well, see code examples below) using <b>Scikit-Learn</b>'s decision tree classifier. Here we have used the so-called <b>gini</b> index (see below) to split the various branches.

<p>
<!-- !split -->

<h2 id="general-features" class="anchor">General Features </h2>

<p>
The overarching approach to decision trees is a top-down approach.

<ul>
<li> A leaf provides the classification of a given instance.</li>
<li> A node specifies a test of some attribute of the instance.</li>
<li> A branch corresponds to a possible values of an attribute.</li>
<li> An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute in the given example.</li>
</ul>

This process is then repeated for the subtree rooted at the new
node.

<p>
<!-- !split -->

<h2 id="how-do-we-set-it-up" class="anchor">How do we set it up? </h2>

<p>
In simplified terms, the process of training a decision tree and
predicting the target features of query instances is as follows:

<ol>
<li> Present a dataset containing of a number of training instances characterized by a number of descriptive features and a target feature</li>
<li> Train the decision tree model by continuously splitting the target feature along the values of the descriptive features using a measure of information gain during the training process</li>
<li> Grow the tree until we accomplish a stopping criteria create leaf nodes which represent the <em>predictions</em> we want to make for new query instances</li>
<li> Show query instances to the tree and run down the tree until we arrive at leaf nodes</li>
</ol>

Then we are essentially done!

<p>
<!-- !split -->

<h2 id="decision-trees-and-regression" class="anchor">Decision trees and Regression  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

steps<span style="color: #666666">=250</span>

distance<span style="color: #666666">=0</span>
x<span style="color: #666666">=0</span>
distance_list<span style="color: #666666">=</span>[]
steps_list<span style="color: #666666">=</span>[]
<span style="color: #008000; font-weight: bold">while</span> x<span style="color: #666666">&lt;</span>steps:
    distance<span style="color: #666666">+=</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randint(<span style="color: #666666">-1</span>,<span style="color: #666666">2</span>)
    distance_list<span style="color: #666666">.</span>append(distance)
    x<span style="color: #666666">+=1</span>
    steps_list<span style="color: #666666">.</span>append(x)
plt<span style="color: #666666">.</span>plot(steps_list,distance_list, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;green&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Random Walk Data&quot;</span>)

steps_list<span style="color: #666666">=</span>np<span style="color: #666666">.</span>asarray(steps_list)
distance_list<span style="color: #666666">=</span>np<span style="color: #666666">.</span>asarray(distance_list)

X<span style="color: #666666">=</span>steps_list[:,np<span style="color: #666666">.</span>newaxis]

<span style="color: #408080; font-style: italic">#Polynomial fits</span>

<span style="color: #408080; font-style: italic">#Degree 2</span>
poly_features<span style="color: #666666">=</span>PolynomialFeatures(degree<span style="color: #666666">=2</span>, include_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
X_poly<span style="color: #666666">=</span>poly_features<span style="color: #666666">.</span>fit_transform(X)

lin_reg<span style="color: #666666">=</span>LinearRegression()
poly_fit<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>fit(X_poly,distance_list)
b<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>coef_
c<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>intercept_
<span style="color: #008000">print</span> (<span style="color: #BA2121">&quot;2nd degree coefficients:&quot;</span>)
<span style="color: #008000">print</span> (<span style="color: #BA2121">&quot;zero power: &quot;</span>,c)
<span style="color: #008000">print</span> (<span style="color: #BA2121">&quot;first power: &quot;</span>, b[<span style="color: #666666">0</span>])
<span style="color: #008000">print</span> (<span style="color: #BA2121">&quot;second power: &quot;</span>,b[<span style="color: #666666">1</span>])

z <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>, steps, <span style="color: #666666">.01</span>)
z_mod<span style="color: #666666">=</span>b[<span style="color: #666666">1</span>]<span style="color: #666666">*</span>z<span style="color: #666666">**2+</span>b[<span style="color: #666666">0</span>]<span style="color: #666666">*</span>z<span style="color: #666666">+</span>c

fit_mod<span style="color: #666666">=</span>b[<span style="color: #666666">1</span>]<span style="color: #666666">*</span>X<span style="color: #666666">**2+</span>b[<span style="color: #666666">0</span>]<span style="color: #666666">*</span>X<span style="color: #666666">+</span>c
plt<span style="color: #666666">.</span>plot(z, z_mod, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;r&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;2nd Degree Fit&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Polynomial Regression&quot;</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Steps&quot;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Distance&quot;</span>)

<span style="color: #408080; font-style: italic">#Degree 10</span>
poly_features10<span style="color: #666666">=</span>PolynomialFeatures(degree<span style="color: #666666">=10</span>, include_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
X_poly10<span style="color: #666666">=</span>poly_features10<span style="color: #666666">.</span>fit_transform(X)

poly_fit10<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>fit(X_poly10,distance_list)

y_plot<span style="color: #666666">=</span>poly_fit10<span style="color: #666666">.</span>predict(X_poly10)
plt<span style="color: #666666">.</span>plot(X, y_plot, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;black&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;10th Degree Fit&quot;</span>)

plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()


<span style="color: #408080; font-style: italic">#Decision Tree Regression</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeRegressor
regr_1<span style="color: #666666">=</span>DecisionTreeRegressor(max_depth<span style="color: #666666">=2</span>)
regr_2<span style="color: #666666">=</span>DecisionTreeRegressor(max_depth<span style="color: #666666">=5</span>)
regr_3<span style="color: #666666">=</span>DecisionTreeRegressor(max_depth<span style="color: #666666">=7</span>)
regr_1<span style="color: #666666">.</span>fit(X, distance_list)
regr_2<span style="color: #666666">.</span>fit(X, distance_list)
regr_3<span style="color: #666666">.</span>fit(X, distance_list)

X_test <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0.0</span>, steps, <span style="color: #666666">0.01</span>)[:, np<span style="color: #666666">.</span>newaxis]
y_1 <span style="color: #666666">=</span> regr_1<span style="color: #666666">.</span>predict(X_test)
y_2 <span style="color: #666666">=</span> regr_2<span style="color: #666666">.</span>predict(X_test)
y_3<span style="color: #666666">=</span>regr_3<span style="color: #666666">.</span>predict(X_test)

<span style="color: #408080; font-style: italic"># Plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>scatter(X, distance_list, s<span style="color: #666666">=2.5</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&quot;black&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;data&quot;</span>)
plt<span style="color: #666666">.</span>plot(X_test, y_1, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;red&quot;</span>,
         label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;max_depth=2&quot;</span>, linewidth<span style="color: #666666">=2</span>)
plt<span style="color: #666666">.</span>plot(X_test, y_2, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;green&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;max_depth=5&quot;</span>, linewidth<span style="color: #666666">=2</span>)
plt<span style="color: #666666">.</span>plot(X_test, y_3, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;m&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;max_depth=7&quot;</span>, linewidth<span style="color: #666666">=2</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Data&quot;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Darget&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Decision Tree Regression&quot;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="building-a-tree-regression" class="anchor">Building a tree, regression </h2>

<p>
There are mainly two steps

<ol>
<li> We split the predictor space (the set of possible values \( x_1,x_2,\dots, x_p \)) into \( J \) distinct and non-non-overlapping regions, \( R_1,R_2,\dots,R_J \).</li>  
<li> For every observation that falls into the region \( R_j \) , we make the same prediction, which is simply the mean of the response values for the training observations in \( R_j \).</li>
</ol>

How do we construct the regions \( R_1,\dots,R_J \)?  In theory, the
regions could have any shape. However, we choose to divide the
predictor space into high-dimensional rectangles, or boxes, for
simplicity and for ease of interpretation of the resulting predictive
model. The goal is to find boxes \( R_1,\dots,R_J \) that minimize the
MSE, given by

$$
\sum_{j=1}^J\sum_{i\in R_j}(y_i-\overline{y}_{R_j})^2,
$$

<p>
where \( \overline{y}_{R_j} \)  is the mean response for the training observations 
within box \( j \).

<p>
<!-- !split -->

<h2 id="a-top-down-approach-recursive-binary-splitting" class="anchor">A top-down approach, recursive binary splitting </h2>

<p>
Unfortunately, it is computationally infeasible to consider every
possible partition of the feature space into \( J \) boxes.  The common
strategy is to take a top-down approach

<p>
The approach is top-down because it begins at the top of the tree (all
observations belong to a single region) and then successively splits
the predictor space; each split is indicated via two new branches
further down on the tree. It is greedy because at each step of the
tree-building process, the best split is made at that particular step,
rather than looking ahead and picking a split that will lead to a
better tree in some future step.

<p>
<!-- !split -->

<h2 id="making-a-tree" class="anchor">Making a tree </h2>

<p>
In order to implement the recursive binary splitting we start by selecting
the predictor \( x_j \) and a cutpoint \( s \) that splits the predictor space into two regions \( R_1 \) and \( R_2 \)
$$
\left\{X\vert x_j < s\right\},
$$

and
$$
\left\{X\vert x_j \geq s\right\},
$$

so that we obtain the lowest MSE, that is
$$
\sum_{i:x_i\in R_j}(y_i-\overline{y}_{R_1})^2+\sum_{i:x_i\in R_2}(y_i-\overline{y}_{R_2})^2,
$$

<p>
which we want to minimize by considering all predictors
\( x_1,x_2,\dots,x_p \).  We consider also all possible values of \( s \) for
each predictor. These values could be determined by randomly assigned
numbers or by starting at the midpoint and then proceed till we find
an optimal value.

<p>
For any \( j \) and \( s \), we define the pair of half-planes where
\( \overline{y}_{R_1} \) is the mean response for the training
observations in \( R_1(j,s) \), and \( \overline{y}_{R_2} \) is the mean
response for the training observations in \( R_2(j,s) \).

<p>
Finding the values of \( j \) and \( s \) that minimize the above equation can be
done quite quickly, especially when the number of features \( p \) is not
too large.

<p>
Next, we repeat the process, looking
for the best predictor and best cutpoint in order to split the data
further so as to minimize the MSE within each of the resulting
regions. However, this time, instead of splitting the entire predictor
space, we split one of the two previously identified regions. We now
have three regions. Again, we look to split one of these three regions
further, so as to minimize the MSE. The process continues until a
stopping criterion is reached; for instance, we may continue until no
region contains more than five observations.

<p>
<!-- !split  -->

<h2 id="pruning-the-tree" class="anchor">Pruning the tree </h2>

<p>
The above procedure is rather straightforward, but leads often to
overfitting and unnecessarily large and complicated trees. The basic
idea is to grow a large tree \( T_0 \) and then prune it back in order to
obtain a subtree. A smaller tree with fewer splits (fewer regions) can
lead to smaller variance and better interpretation at the cost of a
little more bias.

<p>
The so-called Cost complexity pruning algorithm gives us a
way to do just this. Rather than considering every possible subtree,
we consider a sequence of trees indexed by a nonnegative tuning
parameter \( \alpha \).

<p>
Read more at the following <a href="https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py" target="_self">Scikit-Learn link on pruning</a>.

<p>
<!-- !split -->

<h2 id="cost-complexity-pruning" class="anchor">Cost complexity pruning </h2>

<p>
For each value of \( \alpha \)  there corresponds a subtree \( T \in T_0 \) such that
$$
\sum_{m=1}^{\overline{T}}\sum_{i:x_i\in R_m}(y_i-\overline{y}_{R_m})^2+\alpha\overline{T},
$$

is as small as possible. Here \( \overline{T} \) is 
the number of terminal nodes of the tree \( T \) , \( R_m \) is the
rectangle (i.e. the subset of predictor space)  corresponding to the \( m \)-th terminal node.

<p>
The tuning parameter \( \alpha \) controls a trade-off between the subtree&#8217;s
complexity and its fit to the training data. When \( \alpha = 0 \), then the
subtree \( T \) will simply equal \( T_0 \), 
because then the above equation just measures the
training error. 
However, as \( \alpha \) increases, there is a price to pay for
having a tree with many terminal nodes. The above equation will
tend to be minimized for a smaller subtree.

<p>
It turns out that as we increase \( \alpha \) from zero
branches get pruned from the tree in a nested and predictable fashion,
so obtaining the whole sequence of subtrees as a function of \( \alpha \) is
easy. We can select a value of \( \alpha \) using a validation set or using
cross-validation. We then return to the full data set and obtain the
subtree corresponding to \( \alpha \).

<p>
<!-- !split -->

<h2 id="schematic-regression-procedure" class="anchor">Schematic Regression Procedure </h2>

<p>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<ol>
<li> Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.</li>
<li> Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \( \alpha \).</li>
<li> Use for example \( K \)-fold cross-validation to choose \( \alpha \). Divide the training observations into \( K \) folds. For each \( k=1,2,\dots,K \) we:</li> 

<ul>
  <li> repeat steps 1 and 2 on all but the \( k \)-th fold of the training data.</li> 
  <li> Then we valuate the mean squared prediction error on the data in the left-out \( k \)-th fold, as a function of \( \alpha \).</li>
  <li> Finally  we average the results for each value of \( \alpha \), and pick \( \alpha \) to minimize the average error.</li>
</ul>

<li> Return the subtree from Step 2 that corresponds to the chosen value of \( \alpha \).</li> 
</ol>
</div>
</div>


<p>
<!-- !split -->

<h2 id="a-classification-tree" class="anchor">A Classification Tree </h2>

<p>
A classification tree is very similar to a regression tree, except
that it is used to predict a qualitative response rather than a
quantitative one. Recall that for a regression tree, the predicted
response for an observation is given by the mean response of the
training observations that belong to the same terminal node. In
contrast, for a classification tree, we predict that each observation
belongs to the most commonly occurring class of training observations
in the region to which it belongs. In interpreting the results of a
classification tree, we are often interested not only in the class
prediction corresponding to a particular terminal node region, but
also in the class proportions among the training observations that
fall into that region.

<p>
<!-- !split -->

<h2 id="growing-a-classification-tree" class="anchor">Growing a classification tree </h2>

<p>
The task of growing a
classification tree is quite similar to the task of growing a
regression tree. Just as in the regression setting, we use recursive
binary splitting to grow a classification tree. However, in the
classification setting, the MSE cannot be used as a criterion for making
the binary splits.  A natural alternative to MSE is the <b>classification
error rate</b>. Since we plan to assign an observation in a given region
to the most commonly occurring error rate class of training
observations in that region, the classification error rate is simply
the fraction of the training observations in that region that do not
belong to the most common class.

<p>
When building a classification tree, either the Gini index or the
entropy are typically used to evaluate the quality of a particular
split, since these two approaches are more sensitive to node purity
than is the classification error rate.

<p>
<!-- !split -->

<h2 id="classification-tree-how-to-split-nodes" class="anchor">Classification tree, how to split nodes </h2>

<p>
If our targets are the outcome of a classification process that takes
for example \( k=1,2,\dots,K \) values, the only thing we need to think of
is to set up the splitting criteria for each node.

<p>
We define a PDF \( p_{mk} \) that represents the number of observations of
a class \( k \) in a region \( R_m \) with \( N_m \) observations. We represent
this likelihood function in terms of the proportion \( I(y_i=k) \) of
observations of this class in the region \( R_m \) as

$$
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i=k).
$$

<p>
We let \( p_{mk} \) represent the majority class of observations in region
\( m \). The three most common ways of splitting a node are given by

<ul>
<li> Misclassification error</li> 
</ul>

$$
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i\ne k) = 1-p_{mk}.
$$


<ul>
<li> Gini index \( g \)</li>
</ul>

$$
g = \sum_{k=1}^K p_{mk}(1-p_{mk}).
$$


<ul>
<li> Information entropy or just entropy \( s \)</li>
</ul>

$$
s = -\sum_{k=1}^K p_{mk}\log{p_{mk}}.
$$

<p>
<!-- !split -->

<h2 id="visualizing-the-tree-classification" class="anchor">Visualizing the Tree, Classification </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> confusion_matrix
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> export_graphviz

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> Image 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">pydot</span> <span style="color: #008000; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>


cancer <span style="color: #666666">=</span> load_breast_cancer()
X <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(cancer<span style="color: #666666">.</span>data, columns<span style="color: #666666">=</span>cancer<span style="color: #666666">.</span>feature_names)
<span style="color: #008000">print</span>(X)
y <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>Categorical<span style="color: #666666">.</span>from_codes(cancer<span style="color: #666666">.</span>target, cancer<span style="color: #666666">.</span>target_names)
y <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>get_dummies(y)
<span style="color: #008000">print</span>(y)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, random_state<span style="color: #666666">=1</span>)
tree_clf <span style="color: #666666">=</span> DecisionTreeClassifier(max_depth<span style="color: #666666">=5</span>)
tree_clf<span style="color: #666666">.</span>fit(X_train, y_train)

export_graphviz(
    tree_clf,
    out_file<span style="color: #666666">=</span><span style="color: #BA2121">&quot;DataFiles/cancer.dot&quot;</span>,
    feature_names<span style="color: #666666">=</span>cancer<span style="color: #666666">.</span>feature_names,
    class_names<span style="color: #666666">=</span>cancer<span style="color: #666666">.</span>target_names,
    rounded<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
    filled<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>
)
cmd <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;dot -Tpng DataFiles/cancer.dot -o DataFiles/cancer.png&#39;</span>
os<span style="color: #666666">.</span>system(cmd)
</pre></div>
<p>
<!-- !split -->

<h2 id="visualizing-the-tree-the-moons" class="anchor">Visualizing the Tree, The Moons  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> make_moons
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> export_graphviz
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">pydot</span> <span style="color: #008000; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">42</span>)
X, y <span style="color: #666666">=</span> make_moons(n_samples<span style="color: #666666">=100</span>, noise<span style="color: #666666">=0.25</span>, random_state<span style="color: #666666">=53</span>)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X,y,random_state<span style="color: #666666">=0</span>)
tree_clf <span style="color: #666666">=</span> DecisionTreeClassifier(max_depth<span style="color: #666666">=5</span>)
tree_clf<span style="color: #666666">.</span>fit(X_train, y_train)

export_graphviz(
    tree_clf,
    out_file<span style="color: #666666">=</span><span style="color: #BA2121">&quot;DataFiles/moons.dot&quot;</span>,
    rounded<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
    filled<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>
)
cmd <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;dot -Tpng DataFiles/moons.dot -o DataFiles/moons.png&#39;</span>
os<span style="color: #666666">.</span>system(cmd)
</pre></div>
<p>
<!-- !split -->

<h2 id="other-ways-of-visualizing-the-trees" class="anchor">Other ways of visualizing the trees </h2>

<p>
<b>Scikit-Learn</b> has also another way to visualize the trees which is very useful, here with the Iris data.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_iris
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> tree
X, y <span style="color: #666666">=</span> load_iris(return_X_y<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
tree_clf <span style="color: #666666">=</span> tree<span style="color: #666666">.</span>DecisionTreeClassifier()
tree_clf <span style="color: #666666">=</span> tree_clf<span style="color: #666666">.</span>fit(X, y)
<span style="color: #408080; font-style: italic"># and then plot the tree</span>
tree<span style="color: #666666">.</span>plot_tree(tree_clf) 
</pre></div>
<p>
<!-- !split -->

<h2 id="printing-out-as-text" class="anchor">Printing out as text </h2>

<p>
Alternatively, the tree can also be exported in textual format with the function exporttext.
This method doesn&#8217;t require the installation of external libraries and is more compact:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_iris
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> export_text
iris <span style="color: #666666">=</span> load_iris()
decision_tree <span style="color: #666666">=</span> DecisionTreeClassifier(random_state<span style="color: #666666">=0</span>, max_depth<span style="color: #666666">=2</span>)
decision_tree <span style="color: #666666">=</span> decision_tree<span style="color: #666666">.</span>fit(iris<span style="color: #666666">.</span>data, iris<span style="color: #666666">.</span>target)
r <span style="color: #666666">=</span> export_text(decision_tree, feature_names<span style="color: #666666">=</span>iris[<span style="color: #BA2121">&#39;feature_names&#39;</span>])
<span style="color: #008000">print</span>(r)
</pre></div>
<p>
<!-- !split -->

<h2 id="algorithms-for-setting-up-decision-trees" class="anchor">Algorithms for Setting up Decision Trees </h2>

<p>
Two algorithms stand out in the set up of decision trees:

<ol>
<li> The CART (Classification And Regression Tree) algorithm for both classification and regression</li>
<li> The ID3 algorithm based on the computation of the information gain for classification</li>
</ol>

We discuss both algorithms with applications here. The popular library
<b>Scikit-Learn</b> uses the CART algorithm. For classification problems
you can use either the <b>gini</b> index or the <b>entropy</b> to split a tree
in two branches.

<p>
<!-- !split -->

<h2 id="the-cart-algorithm-for-classification" class="anchor">The CART algorithm for Classification </h2>

<p>
For classification, the CART algorithm splits the data set in two subsets using a single feature \( k \) and a threshold \( t_k \).
This could be for example a threshold set by a number below a certain circumference of a malign tumor.

<p>
How do we find these two quantities?
We search for the pair \( (k,t_k) \) that produces the purest subset using for example the <b>gini</b> factor \( G \).
The cost function it tries to minimize is then
$$
C(k,t_k) = \frac{m_{\mathrm{left}}}{m}G_{\mathrm{left}}+ \frac{m_{\mathrm{right}}}{m}G_{\mathrm{right}},
$$

where \( G_{\mathrm{left/right}} \) measures the impurity of the left/right subset  and \( m_{\mathrm{left/right}} \)
 is the number of instances in the left/right subset

<p>
Once it has successfully split the training set in two, it splits the subsets using the same logic, then the subsubsets
and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the
\( max\_depth \) hyperparameter), or if it cannot find a split that will reduce impurity. A few other
hyperparameters control additional stopping conditions such as the \( min\_samples\_split \),
\( min\_samples\_leaf \), \( min\_weight\_fraction\_leaf \), and \( max\_leaf\_nodes \).

<p>
<!-- !split -->

<h2 id="the-cart-algorithm-for-regression" class="anchor">The CART algorithm for Regression </h2>

<p>
The CART algorithm for regression works is similar to the one for classification except that instead of trying to split the
training set in a way that minimizes say the <b>gini</b> or <b>entropy</b> impurity, it now tries to split the training set in a way that minimizes our well-known mean-squared error (MSE). The cost function is now
$$
C(k,t_k) = \frac{m_{\mathrm{left}}}{m}\mathrm{MSE}_{\mathrm{left}}+ \frac{m_{\mathrm{right}}}{m}\mathrm{MSE}_{\mathrm{right}}.
$$

Here the MSE for a specific node is defined as
$$
\mathrm{MSE}_{\mathrm{node}}=\frac{1}{m_\mathrm{node}}\sum_{i\in \mathrm{node}}(\overline{y}_{\mathrm{node}}-y_i)^2,
$$

with
$$
\overline{y}_{\mathrm{node}}=\frac{1}{m_\mathrm{node}}\sum_{i\in \mathrm{node}}y_i,
$$

the mean value of all observations in a specific node.

<p>
Without any regularization, the regression task for decision trees, 
just like for classification tasks, is  prone to overfitting.

<p>
<!-- !split -->

<h2 id="cancer-data-again-now-with-decision-trees-and-other-methods" class="anchor">Cancer Data again now with Decision Trees and other Methods </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier

<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&#39;lbfgs&#39;</span>)
logreg<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Logistic Regression: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic"># Support vector machine</span>
svm <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&#39;auto&#39;</span>, C<span style="color: #666666">=100</span>)
svm<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with SVM: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(svm<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic"># Decision Trees</span>
deep_tree_clf <span style="color: #666666">=</span> DecisionTreeClassifier(max_depth<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>)
deep_tree_clf<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Decision Trees: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic">#now scale the data</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy Logistic Regression with scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
<span style="color: #408080; font-style: italic"># Support Vector Machine</span>
svm<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy SVM with scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
<span style="color: #408080; font-style: italic"># Decision Trees</span>
deep_tree_clf<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Decision Trees and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
</pre></div>
<p>
<!-- !split -->

<h2 id="another-example-the-moons-again" class="anchor">Another example, the moons again </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">__future__</span> <span style="color: #008000; font-weight: bold">import</span> division, print_function, unicode_literals

<span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># to make this notebook&#39;s output stable across runs</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">42</span>)

<span style="color: #408080; font-style: italic"># To plot pretty figures</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib.colors</span> <span style="color: #008000; font-weight: bold">import</span> ListedColormap
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;axes.labelsize&#39;</span>] <span style="color: #666666">=</span> <span style="color: #666666">14</span>
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;xtick.labelsize&#39;</span>] <span style="color: #666666">=</span> <span style="color: #666666">12</span>
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;ytick.labelsize&#39;</span>] <span style="color: #666666">=</span> <span style="color: #666666">12</span>


<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> datasets
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> make_moons
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> export_graphviz

Xm, ym <span style="color: #666666">=</span> make_moons(n_samples<span style="color: #666666">=100</span>, noise<span style="color: #666666">=0.25</span>, random_state<span style="color: #666666">=53</span>)

deep_tree_clf1 <span style="color: #666666">=</span> DecisionTreeClassifier(random_state<span style="color: #666666">=42</span>)
deep_tree_clf2 <span style="color: #666666">=</span> DecisionTreeClassifier(min_samples_leaf<span style="color: #666666">=4</span>, random_state<span style="color: #666666">=42</span>)
deep_tree_clf1<span style="color: #666666">.</span>fit(Xm, ym)
deep_tree_clf2<span style="color: #666666">.</span>fit(Xm, ym)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_decision_boundary</span>(clf, X, y, axes<span style="color: #666666">=</span>[<span style="color: #666666">0</span>, <span style="color: #666666">7.5</span>, <span style="color: #666666">0</span>, <span style="color: #666666">3</span>], iris<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, legend<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>, plot_training<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>):
    x1s <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(axes[<span style="color: #666666">0</span>], axes[<span style="color: #666666">1</span>], <span style="color: #666666">100</span>)
    x2s <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(axes[<span style="color: #666666">2</span>], axes[<span style="color: #666666">3</span>], <span style="color: #666666">100</span>)
    x1, x2 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(x1s, x2s)
    X_new <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[x1<span style="color: #666666">.</span>ravel(), x2<span style="color: #666666">.</span>ravel()]
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_new)<span style="color: #666666">.</span>reshape(x1<span style="color: #666666">.</span>shape)
    custom_cmap <span style="color: #666666">=</span> ListedColormap([<span style="color: #BA2121">&#39;#fafab0&#39;</span>,<span style="color: #BA2121">&#39;#9898ff&#39;</span>,<span style="color: #BA2121">&#39;#a0faa0&#39;</span>])
    plt<span style="color: #666666">.</span>contourf(x1, x2, y_pred, alpha<span style="color: #666666">=0.3</span>, cmap<span style="color: #666666">=</span>custom_cmap)
    <span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> iris:
        custom_cmap2 <span style="color: #666666">=</span> ListedColormap([<span style="color: #BA2121">&#39;#7d7d58&#39;</span>,<span style="color: #BA2121">&#39;#4c4c7f&#39;</span>,<span style="color: #BA2121">&#39;#507d50&#39;</span>])
        plt<span style="color: #666666">.</span>contour(x1, x2, y_pred, cmap<span style="color: #666666">=</span>custom_cmap2, alpha<span style="color: #666666">=0.8</span>)
    <span style="color: #008000; font-weight: bold">if</span> plot_training:
        plt<span style="color: #666666">.</span>plot(X[:, <span style="color: #666666">0</span>][y<span style="color: #666666">==0</span>], X[:, <span style="color: #666666">1</span>][y<span style="color: #666666">==0</span>], <span style="color: #BA2121">&quot;yo&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Iris-Setosa&quot;</span>)
        plt<span style="color: #666666">.</span>plot(X[:, <span style="color: #666666">0</span>][y<span style="color: #666666">==1</span>], X[:, <span style="color: #666666">1</span>][y<span style="color: #666666">==1</span>], <span style="color: #BA2121">&quot;bs&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Iris-Versicolor&quot;</span>)
        plt<span style="color: #666666">.</span>plot(X[:, <span style="color: #666666">0</span>][y<span style="color: #666666">==2</span>], X[:, <span style="color: #666666">1</span>][y<span style="color: #666666">==2</span>], <span style="color: #BA2121">&quot;g^&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Iris-Virginica&quot;</span>)
        plt<span style="color: #666666">.</span>axis(axes)
    <span style="color: #008000; font-weight: bold">if</span> iris:
        plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Petal length&quot;</span>, fontsize<span style="color: #666666">=14</span>)
        plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Petal width&quot;</span>, fontsize<span style="color: #666666">=14</span>)
    <span style="color: #008000; font-weight: bold">else</span>:
        plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&quot;$x_1$&quot;</span>, fontsize<span style="color: #666666">=18</span>)
        plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&quot;$x_2$&quot;</span>, fontsize<span style="color: #666666">=18</span>, rotation<span style="color: #666666">=0</span>)
    <span style="color: #008000; font-weight: bold">if</span> legend:
        plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;lower right&quot;</span>, fontsize<span style="color: #666666">=14</span>)
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">11</span>, <span style="color: #666666">4</span>))
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">121</span>)
plot_decision_boundary(deep_tree_clf1, Xm, ym, axes<span style="color: #666666">=</span>[<span style="color: #666666">-1.5</span>, <span style="color: #666666">2.5</span>, <span style="color: #666666">-1</span>, <span style="color: #666666">1.5</span>], iris<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;No restrictions&quot;</span>, fontsize<span style="color: #666666">=16</span>)
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">122</span>)
plot_decision_boundary(deep_tree_clf2, Xm, ym, axes<span style="color: #666666">=</span>[<span style="color: #666666">-1.5</span>, <span style="color: #666666">2.5</span>, <span style="color: #666666">-1</span>, <span style="color: #666666">1.5</span>], iris<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;min_samples_leaf = </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf2<span style="color: #666666">.</span>min_samples_leaf), fontsize<span style="color: #666666">=14</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="playing-around-with-regions" class="anchor">Playing around with regions </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">6</span>)
Xs <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>, <span style="color: #666666">2</span>) <span style="color: #666666">-</span> <span style="color: #666666">0.5</span>
ys <span style="color: #666666">=</span> (Xs[:, <span style="color: #666666">0</span>] <span style="color: #666666">&gt;</span> <span style="color: #666666">0</span>)<span style="color: #666666">.</span>astype(np<span style="color: #666666">.</span>float32) <span style="color: #666666">*</span> <span style="color: #666666">2</span>

angle <span style="color: #666666">=</span> np<span style="color: #666666">.</span>pi<span style="color: #666666">/4</span>
rotation_matrix <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[np<span style="color: #666666">.</span>cos(angle), <span style="color: #666666">-</span>np<span style="color: #666666">.</span>sin(angle)], [np<span style="color: #666666">.</span>sin(angle), np<span style="color: #666666">.</span>cos(angle)]])
Xsr <span style="color: #666666">=</span> Xs<span style="color: #666666">.</span>dot(rotation_matrix)

tree_clf_s <span style="color: #666666">=</span> DecisionTreeClassifier(random_state<span style="color: #666666">=42</span>)
tree_clf_s<span style="color: #666666">.</span>fit(Xs, ys)
tree_clf_sr <span style="color: #666666">=</span> DecisionTreeClassifier(random_state<span style="color: #666666">=42</span>)
tree_clf_sr<span style="color: #666666">.</span>fit(Xsr, ys)

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">11</span>, <span style="color: #666666">4</span>))
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">121</span>)
plot_decision_boundary(tree_clf_s, Xs, ys, axes<span style="color: #666666">=</span>[<span style="color: #666666">-0.7</span>, <span style="color: #666666">0.7</span>, <span style="color: #666666">-0.7</span>, <span style="color: #666666">0.7</span>], iris<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">122</span>)
plot_decision_boundary(tree_clf_sr, Xsr, ys, axes<span style="color: #666666">=</span>[<span style="color: #666666">-0.7</span>, <span style="color: #666666">0.7</span>, <span style="color: #666666">-0.7</span>, <span style="color: #666666">0.7</span>], iris<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="regression-trees" class="anchor">Regression trees </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Quadratic training set + noise</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">42</span>)
m <span style="color: #666666">=</span> <span style="color: #666666">200</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(m, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">4</span> <span style="color: #666666">*</span> (X <span style="color: #666666">-</span> <span style="color: #666666">0.5</span>) <span style="color: #666666">**</span> <span style="color: #666666">2</span>
y <span style="color: #666666">=</span> y <span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(m, <span style="color: #666666">1</span>) <span style="color: #666666">/</span> <span style="color: #666666">10</span>
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeRegressor

tree_reg <span style="color: #666666">=</span> DecisionTreeRegressor(max_depth<span style="color: #666666">=2</span>, random_state<span style="color: #666666">=42</span>)
tree_reg<span style="color: #666666">.</span>fit(X, y)
</pre></div>
<p>
<!-- !split -->

<h2 id="final-regressor-code" class="anchor">Final regressor code </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeRegressor

tree_reg1 <span style="color: #666666">=</span> DecisionTreeRegressor(random_state<span style="color: #666666">=42</span>, max_depth<span style="color: #666666">=2</span>)
tree_reg2 <span style="color: #666666">=</span> DecisionTreeRegressor(random_state<span style="color: #666666">=42</span>, max_depth<span style="color: #666666">=3</span>)
tree_reg1<span style="color: #666666">.</span>fit(X, y)
tree_reg2<span style="color: #666666">.</span>fit(X, y)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_regression_predictions</span>(tree_reg, X, y, axes<span style="color: #666666">=</span>[<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">-0.2</span>, <span style="color: #666666">1</span>], ylabel<span style="color: #666666">=</span><span style="color: #BA2121">&quot;$y$&quot;</span>):
    x1 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(axes[<span style="color: #666666">0</span>], axes[<span style="color: #666666">1</span>], <span style="color: #666666">500</span>)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
    y_pred <span style="color: #666666">=</span> tree_reg<span style="color: #666666">.</span>predict(x1)
    plt<span style="color: #666666">.</span>axis(axes)
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;$x_1$&quot;</span>, fontsize<span style="color: #666666">=18</span>)
    <span style="color: #008000; font-weight: bold">if</span> ylabel:
        plt<span style="color: #666666">.</span>ylabel(ylabel, fontsize<span style="color: #666666">=18</span>, rotation<span style="color: #666666">=0</span>)
    plt<span style="color: #666666">.</span>plot(X, y, <span style="color: #BA2121">&quot;b.&quot;</span>)
    plt<span style="color: #666666">.</span>plot(x1, y_pred, <span style="color: #BA2121">&quot;r.-&quot;</span>, linewidth<span style="color: #666666">=2</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">r&quot;$\hat</span><span style="color: #BB6688; font-weight: bold">{y}</span><span style="color: #BA2121">$&quot;</span>)

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">11</span>, <span style="color: #666666">4</span>))
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">121</span>)
plot_regression_predictions(tree_reg1, X, y)
<span style="color: #008000; font-weight: bold">for</span> split, style <span style="color: #AA22FF; font-weight: bold">in</span> ((<span style="color: #666666">0.1973</span>, <span style="color: #BA2121">&quot;k-&quot;</span>), (<span style="color: #666666">0.0917</span>, <span style="color: #BA2121">&quot;k--&quot;</span>), (<span style="color: #666666">0.7718</span>, <span style="color: #BA2121">&quot;k--&quot;</span>)):
    plt<span style="color: #666666">.</span>plot([split, split], [<span style="color: #666666">-0.2</span>, <span style="color: #666666">1</span>], style, linewidth<span style="color: #666666">=2</span>)
plt<span style="color: #666666">.</span>text(<span style="color: #666666">0.21</span>, <span style="color: #666666">0.65</span>, <span style="color: #BA2121">&quot;Depth=0&quot;</span>, fontsize<span style="color: #666666">=15</span>)
plt<span style="color: #666666">.</span>text(<span style="color: #666666">0.01</span>, <span style="color: #666666">0.2</span>, <span style="color: #BA2121">&quot;Depth=1&quot;</span>, fontsize<span style="color: #666666">=13</span>)
plt<span style="color: #666666">.</span>text(<span style="color: #666666">0.65</span>, <span style="color: #666666">0.8</span>, <span style="color: #BA2121">&quot;Depth=1&quot;</span>, fontsize<span style="color: #666666">=13</span>)
plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;upper center&quot;</span>, fontsize<span style="color: #666666">=18</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;max_depth=2&quot;</span>, fontsize<span style="color: #666666">=14</span>)

plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">122</span>)
plot_regression_predictions(tree_reg2, X, y, ylabel<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>)
<span style="color: #008000; font-weight: bold">for</span> split, style <span style="color: #AA22FF; font-weight: bold">in</span> ((<span style="color: #666666">0.1973</span>, <span style="color: #BA2121">&quot;k-&quot;</span>), (<span style="color: #666666">0.0917</span>, <span style="color: #BA2121">&quot;k--&quot;</span>), (<span style="color: #666666">0.7718</span>, <span style="color: #BA2121">&quot;k--&quot;</span>)):
    plt<span style="color: #666666">.</span>plot([split, split], [<span style="color: #666666">-0.2</span>, <span style="color: #666666">1</span>], style, linewidth<span style="color: #666666">=2</span>)
<span style="color: #008000; font-weight: bold">for</span> split <span style="color: #AA22FF; font-weight: bold">in</span> (<span style="color: #666666">0.0458</span>, <span style="color: #666666">0.1298</span>, <span style="color: #666666">0.2873</span>, <span style="color: #666666">0.9040</span>):
    plt<span style="color: #666666">.</span>plot([split, split], [<span style="color: #666666">-0.2</span>, <span style="color: #666666">1</span>], <span style="color: #BA2121">&quot;k:&quot;</span>, linewidth<span style="color: #666666">=1</span>)
plt<span style="color: #666666">.</span>text(<span style="color: #666666">0.3</span>, <span style="color: #666666">0.5</span>, <span style="color: #BA2121">&quot;Depth=2&quot;</span>, fontsize<span style="color: #666666">=13</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;max_depth=3&quot;</span>, fontsize<span style="color: #666666">=14</span>)

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>tree_reg1 <span style="color: #666666">=</span> DecisionTreeRegressor(random_state<span style="color: #666666">=42</span>)
tree_reg2 <span style="color: #666666">=</span> DecisionTreeRegressor(random_state<span style="color: #666666">=42</span>, min_samples_leaf<span style="color: #666666">=10</span>)
tree_reg1<span style="color: #666666">.</span>fit(X, y)
tree_reg2<span style="color: #666666">.</span>fit(X, y)

x1 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">500</span>)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y_pred1 <span style="color: #666666">=</span> tree_reg1<span style="color: #666666">.</span>predict(x1)
y_pred2 <span style="color: #666666">=</span> tree_reg2<span style="color: #666666">.</span>predict(x1)

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">11</span>, <span style="color: #666666">4</span>))

plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">121</span>)
plt<span style="color: #666666">.</span>plot(X, y, <span style="color: #BA2121">&quot;b.&quot;</span>)
plt<span style="color: #666666">.</span>plot(x1, y_pred1, <span style="color: #BA2121">&quot;r.-&quot;</span>, linewidth<span style="color: #666666">=2</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">r&quot;$\hat</span><span style="color: #BB6688; font-weight: bold">{y}</span><span style="color: #BA2121">$&quot;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">-0.2</span>, <span style="color: #666666">1.1</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;$x_1$&quot;</span>, fontsize<span style="color: #666666">=18</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;$y$&quot;</span>, fontsize<span style="color: #666666">=18</span>, rotation<span style="color: #666666">=0</span>)
plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;upper center&quot;</span>, fontsize<span style="color: #666666">=18</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;No restrictions&quot;</span>, fontsize<span style="color: #666666">=14</span>)

plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">122</span>)
plt<span style="color: #666666">.</span>plot(X, y, <span style="color: #BA2121">&quot;b.&quot;</span>)
plt<span style="color: #666666">.</span>plot(x1, y_pred2, <span style="color: #BA2121">&quot;r.-&quot;</span>, linewidth<span style="color: #666666">=2</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">r&quot;$\hat</span><span style="color: #BB6688; font-weight: bold">{y}</span><span style="color: #BA2121">$&quot;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">-0.2</span>, <span style="color: #666666">1.1</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;$x_1$&quot;</span>, fontsize<span style="color: #666666">=18</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;min_samples_leaf=</span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(tree_reg2<span style="color: #666666">.</span>min_samples_leaf), fontsize<span style="color: #666666">=14</span>)

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="pros-and-cons-of-trees-pros" class="anchor">Pros and cons of trees, pros </h2>

<ul>
<li> White box, easy to interpret model. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches discussed earlier (think of support vector machines)</li>
<li> Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!</li>
<li> No feature normalization needed</li>
<li> Tree models can handle both continuous and categorical data (Classification and Regression Trees)</li>
<li> Can model nonlinear relationships</li>
<li> Can model interactions between the different descriptive features</li>
<li> Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small)</li>
</ul>

<!-- !split -->

<h2 id="disadvantages" class="anchor">Disadvantages </h2>

<ul>
<li> Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches</li>
<li> If continuous features are used the tree may become quite large and hence less interpretable</li>
<li> Decision trees are prone to overfit the training data and hence do not well generalize the data if no stopping criteria or improvements like pruning, boosting or bagging are implemented</li>
<li> Small changes in the data may lead to a completely different tree. This issue can be addressed by using ensemble methods like bagging, boosting or random forests</li>
<li> Unbalanced datasets where some target feature values occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones.</li> 
<li> If the number of features is relatively large (high dimensional) and the number of instances is relatively low, the tree might overfit the data</li>
<li> Features with many levels may be preferred over features with less levels since for them it is <em>more easy</em> to split the dataset such that the sub datasets only contain pure target feature values. This issue can be addressed by preferring for instance the information gain ratio as splitting criteria over information gain</li>
</ul>

However, by aggregating many decision trees, using methods like
bagging, random forests, and boosting, the predictive performance of
trees can be substantially improved.

<p>
<!-- !split -->

<h2 id="ensemble-methods-from-a-single-tree-to-many-trees-and-extreme-boosting-meet-the-jungle-of-methods" class="anchor">Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods </h2>

<p>
As stated above and seen in many of the examples discussed here about
a single decision tree, we often end up overfitting our training
data. This normally means that we have a high variance. Can we reduce
the variance of a statistical learning method?

<p>
This leads us to a set of different methods that can combine different
machine learning algorithms or just use one of them to construct
forests and jungles of trees, homogeneous ones or heterogenous
ones. These methods are recognized by different names which we will
try to explain here. These are

<ol>
<li> Voting classifiers</li>
<li> Bagging and Pasting</li>
<li> Random forests</li>
<li> Boosting methods, from adaptive to Extreme Gradient Boosting (XGBoost)</li>
</ol>

We discuss these methods here.

<p>
<!-- !split -->

<h2 id="an-overview-of-ensemble-methods" class="anchor">An Overview of Ensemble Methods  </h2>

<p>
<br /><br /><center><p><img src="DataFiles/ensembleoverview.png" align="bottom" width=600></p></center><br /><br />

<p>
<!-- !split -->

<h2 id="bagging" class="anchor">Bagging </h2>

<p>
The <b>plain</b> decision trees suffer from high
variance. This means that if we split the training data into two parts
at random, and fit a decision tree to both halves, the results that we
get could be quite different. In contrast, a procedure with low
variance will yield similar results if applied repeatedly to distinct
data sets; linear regression tends to have low variance, if the ratio
of \( n \) to \( p \) is moderately large.

<p>
<b>Bootstrap aggregation</b>, or just <b>bagging</b>, is a
general-purpose procedure for reducing the variance of a statistical
learning method.

<p>
<!-- !split -->

<h2 id="more-bagging" class="anchor">More bagging </h2>

<p>
Bagging typically results in improved accuracy
over prediction using a single tree. Unfortunately, however, it can be
difficult to interpret the resulting model. Recall that one of the
advantages of decision trees is the attractive and easily interpreted
diagram that results.

<p>
However, when we bag a large number of trees, it is no longer
possible to represent the resulting statistical learning procedure
using a single tree, and it is no longer clear which variables are
most important to the procedure. Thus, bagging improves prediction
accuracy at the expense of interpretability.  Although the collection
of bagged trees is much more difficult to interpret than a single
tree, one can obtain an overall summary of the importance of each
predictor using the MSE (for bagging regression trees) or the Gini
index (for bagging classification trees). In the case of bagging
regression trees, we can record the total amount that the MSE is
decreased due to splits over a given predictor, averaged over all \( B \) possible
trees. A large value indicates an important predictor. Similarly, in
the context of bagging classification trees, we can add up the total
amount that the Gini index  is decreased by splits over a given
predictor, averaged over all \( B \) trees.

<p>
<!-- !split -->

<h2 id="simple-voting-example-head-or-tail" class="anchor">Simple Voting Example, head or tail </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>heads_proba <span style="color: #666666">=</span> <span style="color: #666666">0.51</span>
coin_tosses <span style="color: #666666">=</span> (np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">10000</span>, <span style="color: #666666">10</span>) <span style="color: #666666">&lt;</span> heads_proba)<span style="color: #666666">.</span>astype(np<span style="color: #666666">.</span>int32)
cumulative_heads_ratio <span style="color: #666666">=</span> np<span style="color: #666666">.</span>cumsum(coin_tosses, axis<span style="color: #666666">=0</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">1</span>, <span style="color: #666666">10001</span>)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">8</span>,<span style="color: #666666">3.5</span>))
plt<span style="color: #666666">.</span>plot(cumulative_heads_ratio)
plt<span style="color: #666666">.</span>plot([<span style="color: #666666">0</span>, <span style="color: #666666">10000</span>], [<span style="color: #666666">0.51</span>, <span style="color: #666666">0.51</span>], <span style="color: #BA2121">&quot;k--&quot;</span>, linewidth<span style="color: #666666">=2</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;51%&quot;</span>)
plt<span style="color: #666666">.</span>plot([<span style="color: #666666">0</span>, <span style="color: #666666">10000</span>], [<span style="color: #666666">0.5</span>, <span style="color: #666666">0.5</span>], <span style="color: #BA2121">&quot;k-&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;50%&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Number of coin tosses&quot;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Heads ratio&quot;</span>)
plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;lower right&quot;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>, <span style="color: #666666">10000</span>, <span style="color: #666666">0.42</span>, <span style="color: #666666">0.58</span>])
save_fig(<span style="color: #BA2121">&quot;votingsimple&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="using-the-voting-classifier" class="anchor">Using the Voting Classifier </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> make_moons

X, y <span style="color: #666666">=</span> make_moons(n_samples<span style="color: #666666">=500</span>, noise<span style="color: #666666">=0.30</span>, random_state<span style="color: #666666">=42</span>)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, random_state<span style="color: #666666">=42</span>)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> VotingClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC

log_clf <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;liblinear&quot;</span>, random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=10</span>, random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&quot;auto&quot;</span>, random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;hard&#39;</span>)

voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))

log_clf <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;liblinear&quot;</span>, random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=10</span>, random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&quot;auto&quot;</span>, probability<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;soft&#39;</span>)
voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>
<!-- !split -->

<h2 id="please-not-the-moons-again-voting-and-bagging" class="anchor">Please, not the moons again! Voting and Bagging </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> make_moons

X, y <span style="color: #666666">=</span> make_moons(n_samples<span style="color: #666666">=500</span>, noise<span style="color: #666666">=0.30</span>, random_state<span style="color: #666666">=42</span>)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, random_state<span style="color: #666666">=42</span>)
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> VotingClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC

log_clf <span style="color: #666666">=</span> LogisticRegression(random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;hard&#39;</span>)
voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>log_clf <span style="color: #666666">=</span> LogisticRegression(random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(probability<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;soft&#39;</span>)
voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>
<!-- !split -->

<h2 id="bagging-examples" class="anchor">Bagging Examples  </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> BaggingClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier

bag_clf <span style="color: #666666">=</span> BaggingClassifier(
    DecisionTreeClassifier(random_state<span style="color: #666666">=42</span>), n_estimators<span style="color: #666666">=500</span>,
    max_samples<span style="color: #666666">=100</span>, bootstrap<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, n_jobs<span style="color: #666666">=-1</span>, random_state<span style="color: #666666">=42</span>)
bag_clf<span style="color: #666666">.</span>fit(X_train, y_train)
y_pred <span style="color: #666666">=</span> bag_clf<span style="color: #666666">.</span>predict(X_test)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score
<span style="color: #008000">print</span>(accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>tree_clf <span style="color: #666666">=</span> DecisionTreeClassifier(random_state<span style="color: #666666">=42</span>)
tree_clf<span style="color: #666666">.</span>fit(X_train, y_train)
y_pred_tree <span style="color: #666666">=</span> tree_clf<span style="color: #666666">.</span>predict(X_test)
<span style="color: #008000">print</span>(accuracy_score(y_test, y_pred_tree))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib.colors</span> <span style="color: #008000; font-weight: bold">import</span> ListedColormap

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_decision_boundary</span>(clf, X, y, axes<span style="color: #666666">=</span>[<span style="color: #666666">-1.5</span>, <span style="color: #666666">2.5</span>, <span style="color: #666666">-1</span>, <span style="color: #666666">1.5</span>], alpha<span style="color: #666666">=0.5</span>, contour<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>):
    x1s <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(axes[<span style="color: #666666">0</span>], axes[<span style="color: #666666">1</span>], <span style="color: #666666">100</span>)
    x2s <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(axes[<span style="color: #666666">2</span>], axes[<span style="color: #666666">3</span>], <span style="color: #666666">100</span>)
    x1, x2 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(x1s, x2s)
    X_new <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[x1<span style="color: #666666">.</span>ravel(), x2<span style="color: #666666">.</span>ravel()]
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_new)<span style="color: #666666">.</span>reshape(x1<span style="color: #666666">.</span>shape)
    custom_cmap <span style="color: #666666">=</span> ListedColormap([<span style="color: #BA2121">&#39;#fafab0&#39;</span>,<span style="color: #BA2121">&#39;#9898ff&#39;</span>,<span style="color: #BA2121">&#39;#a0faa0&#39;</span>])
    plt<span style="color: #666666">.</span>contourf(x1, x2, y_pred, alpha<span style="color: #666666">=0.3</span>, cmap<span style="color: #666666">=</span>custom_cmap)
    <span style="color: #008000; font-weight: bold">if</span> contour:
        custom_cmap2 <span style="color: #666666">=</span> ListedColormap([<span style="color: #BA2121">&#39;#7d7d58&#39;</span>,<span style="color: #BA2121">&#39;#4c4c7f&#39;</span>,<span style="color: #BA2121">&#39;#507d50&#39;</span>])
        plt<span style="color: #666666">.</span>contour(x1, x2, y_pred, cmap<span style="color: #666666">=</span>custom_cmap2, alpha<span style="color: #666666">=0.8</span>)
    plt<span style="color: #666666">.</span>plot(X[:, <span style="color: #666666">0</span>][y<span style="color: #666666">==0</span>], X[:, <span style="color: #666666">1</span>][y<span style="color: #666666">==0</span>], <span style="color: #BA2121">&quot;yo&quot;</span>, alpha<span style="color: #666666">=</span>alpha)
    plt<span style="color: #666666">.</span>plot(X[:, <span style="color: #666666">0</span>][y<span style="color: #666666">==1</span>], X[:, <span style="color: #666666">1</span>][y<span style="color: #666666">==1</span>], <span style="color: #BA2121">&quot;bs&quot;</span>, alpha<span style="color: #666666">=</span>alpha)
    plt<span style="color: #666666">.</span>axis(axes)
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&quot;$x_1$&quot;</span>, fontsize<span style="color: #666666">=18</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&quot;$x_2$&quot;</span>, fontsize<span style="color: #666666">=18</span>, rotation<span style="color: #666666">=0</span>)
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">11</span>,<span style="color: #666666">4</span>))
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">121</span>)
plot_decision_boundary(tree_clf, X, y)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Decision Tree&quot;</span>, fontsize<span style="color: #666666">=14</span>)
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">122</span>)
plot_decision_boundary(bag_clf, X, y)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Decision Trees with Bagging&quot;</span>, fontsize<span style="color: #666666">=14</span>)
save_fig(<span style="color: #BA2121">&quot;baggingtree&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="making-your-own-bootstrap-changing-the-level-of-the-decision-tree" class="anchor">Making your own Bootstrap: Changing the Level of the Decision Tree </h2>

<p>
Let us bring up our good old boostrap example from the linear regression lectures. We change the linerar regression algorithm with
a decision tree wth different depths and perform a bootstrap aggregate (in this case we perform as many bootstraps as data points \( n \)).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> make_pipeline
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.utils</span> <span style="color: #008000; font-weight: bold">import</span> resample
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeRegressor

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
n_boostraps <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdepth <span style="color: #666666">=</span> <span style="color: #666666">8</span>

<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
error <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdepth)
bias <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdepth)
variance <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdepth)
polydegree <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdepth)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(x, y, test_size<span style="color: #666666">=0.2</span>)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

<span style="color: #408080; font-style: italic"># we produce a simple tree first as benchmark</span>
simpletree <span style="color: #666666">=</span> DecisionTreeRegressor(max_depth<span style="color: #666666">=3</span>) 
simpletree<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
simpleprediction <span style="color: #666666">=</span> simpletree<span style="color: #666666">.</span>predict(X_test_scaled)
<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,maxdepth):
    model <span style="color: #666666">=</span> DecisionTreeRegressor(max_depth<span style="color: #666666">=</span>degree) 
    y_pred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>empty((y_test<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], n_boostraps))
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_boostraps):
        x_, y_ <span style="color: #666666">=</span> resample(X_train_scaled, y_train)
        model<span style="color: #666666">.</span>fit(x_, y_)
        y_pred[:, i] <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X_test_scaled)<span style="color: #408080; font-style: italic">#.ravel()</span>

    polydegree[degree] <span style="color: #666666">=</span> degree
    error[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> y_pred)<span style="color: #666666">**2</span>, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>) )
    bias[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( (y_test <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_pred, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>))<span style="color: #666666">**2</span> )
    variance[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>var(y_pred, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>) )
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Polynomial degree:&#39;</span>, degree)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;</span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> &gt;= </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> + </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> = </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(error[degree], bias[degree], variance[degree], bias[degree]<span style="color: #666666">+</span>variance[degree]))
 
mse_simpletree<span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> simpleprediction)<span style="color: #666666">**2</span>)
<span style="color: #008000">print</span>(mse_simpletree)
plt<span style="color: #666666">.</span>xlim(<span style="color: #666666">1</span>,maxdepth)
plt<span style="color: #666666">.</span>plot(polydegree, error, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, bias, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;bias&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, variance, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Variance&#39;</span>)
plt<span style="color: #666666">.</span>legend()
save_fig(<span style="color: #BA2121">&quot;baggingboot&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="why-voting" class="anchor">Why Voting? </h2>

<p>
The idea behind boosting, and voting as well can be phrased as follows:
<b>Can a group of people somehow arrive at highly
reasoned decisions, despite the weak judgement of the individual
members?</b>

<p>
The aim is to create a good classifier by combining several weak classifiers.
<b>A weak classifier is a classifier which is able to produce results that are only slightly better than guessing at random.</b>

<p>
The basic approach is to apply repeatedly (in boosting this is done in an iterative way) a weak classifier to modifications of the data.
In voting we simply apply the law of large numbers while in boosting we give more weight to misclassified data in
each iteration.

<p>
Decision trees play an important role as our weak classifier. They serve as the basic method.

<p>
<!-- !split -->

<h2 id="tossing-coins" class="anchor">Tossing coins </h2>

<p>
The simplest case is a so-called voting ensemble. To illustrate this,
think of yourself tossing coins with a biased outcome of 51 per cent
for heads and 49% for tails. With only few tosses,
you may not clearly see this distribution for heads and tails. However, after some
thousands of tosses, there will be a clear majority of heads.  With 2000 tosses
you should see approximately 1020 heads and 980 tails.

<p>
We can then state that the outcome is a clear majority of heads. If
you do this ten thousand times, it is easy to see that there is a 97%
likelihood of a majority of heads.

<p>
Another example would be to collect all polls before an
election. Different polls may show different likelihoods for a
candidate winning with say a majority  of the popular vote. The majority vote
would then consist in many polls indicating that this candidate will
actually win.

<p>
The example here shows how we can implement the coin tossing case,
clealry demostrating that after some tosses we see the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_self">law of large</a>
numbers kicking in.

<p>
<!-- !split -->

<h2 id="standard-imports-first" class="anchor">Standard imports first </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> Image 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">pydot</span> <span style="color: #008000; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> export_graphviz
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler, OneHotEncoder
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.compose</span> <span style="color: #008000; font-weight: bold">import</span> ColumnTransformer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> Image 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">pydot</span> <span style="color: #008000; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)
</pre></div>
<p>
<!-- !split -->

<h2 id="simple-voting-example-head-or-tail" class="anchor">Simple Voting Example, head or tail </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib.colors</span> <span style="color: #008000; font-weight: bold">import</span> ListedColormap
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;axes.labelsize&#39;</span>] <span style="color: #666666">=</span> <span style="color: #666666">14</span>
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;xtick.labelsize&#39;</span>] <span style="color: #666666">=</span> <span style="color: #666666">12</span>
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;ytick.labelsize&#39;</span>] <span style="color: #666666">=</span> <span style="color: #666666">12</span>

heads_proba <span style="color: #666666">=</span> <span style="color: #666666">0.51</span>
coin_tosses <span style="color: #666666">=</span> (np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">10000</span>, <span style="color: #666666">10</span>) <span style="color: #666666">&lt;</span> heads_proba)<span style="color: #666666">.</span>astype(np<span style="color: #666666">.</span>int32)
cumulative_heads_ratio <span style="color: #666666">=</span> np<span style="color: #666666">.</span>cumsum(coin_tosses, axis<span style="color: #666666">=0</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">1</span>, <span style="color: #666666">10001</span>)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">8</span>,<span style="color: #666666">3.5</span>))
plt<span style="color: #666666">.</span>plot(cumulative_heads_ratio)
plt<span style="color: #666666">.</span>plot([<span style="color: #666666">0</span>, <span style="color: #666666">10000</span>], [<span style="color: #666666">0.51</span>, <span style="color: #666666">0.51</span>], <span style="color: #BA2121">&quot;k--&quot;</span>, linewidth<span style="color: #666666">=2</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;51%&quot;</span>)
plt<span style="color: #666666">.</span>plot([<span style="color: #666666">0</span>, <span style="color: #666666">10000</span>], [<span style="color: #666666">0.5</span>, <span style="color: #666666">0.5</span>], <span style="color: #BA2121">&quot;k-&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;50%&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Number of coin tosses&quot;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Heads ratio&quot;</span>)
plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;lower right&quot;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>, <span style="color: #666666">10000</span>, <span style="color: #666666">0.42</span>, <span style="color: #666666">0.58</span>])
save_fig(<span style="color: #BA2121">&quot;votingsimple&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="using-the-voting-classifier" class="anchor">Using the Voting Classifier </h2>

<p>
We can use the voting classifier on other data sets, here the exciting binary case of two distinct objects using the make moons functionality of <b>Scikit-Learn</b>.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> make_moons

X, y <span style="color: #666666">=</span> make_moons(n_samples<span style="color: #666666">=500</span>, noise<span style="color: #666666">=0.30</span>, random_state<span style="color: #666666">=42</span>)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, random_state<span style="color: #666666">=42</span>)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> VotingClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC

log_clf <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;liblinear&quot;</span>, random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=10</span>, random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&quot;auto&quot;</span>, random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;hard&#39;</span>)

voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))

log_clf <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;liblinear&quot;</span>, random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=10</span>, random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&quot;auto&quot;</span>, probability<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, random_state<span style="color: #666666">=42</span>)
voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;soft&#39;</span>)
voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>
<!-- !split -->

<h2 id="voting-and-bagging" class="anchor">Voting and Bagging </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> make_moons

X, y <span style="color: #666666">=</span> make_moons(n_samples<span style="color: #666666">=500</span>, noise<span style="color: #666666">=0.30</span>, random_state<span style="color: #666666">=42</span>)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, random_state<span style="color: #666666">=42</span>)
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> VotingClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC

log_clf <span style="color: #666666">=</span> LogisticRegression(random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;hard&#39;</span>)
voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>log_clf <span style="color: #666666">=</span> LogisticRegression(random_state<span style="color: #666666">=42</span>)
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(random_state<span style="color: #666666">=42</span>)
svm_clf <span style="color: #666666">=</span> SVC(probability<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, random_state<span style="color: #666666">=42</span>)

voting_clf <span style="color: #666666">=</span> VotingClassifier(
    estimators<span style="color: #666666">=</span>[(<span style="color: #BA2121">&#39;lr&#39;</span>, log_clf), (<span style="color: #BA2121">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #BA2121">&#39;svc&#39;</span>, svm_clf)],
    voting<span style="color: #666666">=</span><span style="color: #BA2121">&#39;soft&#39;</span>)
voting_clf<span style="color: #666666">.</span>fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score

<span style="color: #008000; font-weight: bold">for</span> clf <span style="color: #AA22FF; font-weight: bold">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf<span style="color: #666666">.</span>fit(X_train, y_train)
    y_pred <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(clf<span style="color: #666666">.</span><span style="color: #19177C">__class__</span><span style="color: #666666">.</span><span style="color: #19177C">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>
<!-- !split -->

<h2 id="random-forests" class="anchor">Random forests </h2>

<p>
Random forests provide an improvement over bagged trees by way of a
small tweak that decorrelates the trees.

<p>
As in bagging, we build a
number of decision trees on bootstrapped training samples. But when
building these decision trees, each time a split in a tree is
considered, a random sample of \( m \) predictors is chosen as split
candidates from the full set of \( p \) predictors. The split is allowed to
use only one of those \( m \) predictors.

<p>
A fresh sample of \( m \) predictors is
taken at each split, and typically we choose 

$$
m\approx \sqrt{p}.
$$

<p>
In building a random forest, at
each split in the tree, the algorithm is not even allowed to consider
a majority of the available predictors.

<p>
The reason for this is rather clever. Suppose that there is one very
strong predictor in the data set, along with a number of other
moderately strong predictors. Then in the collection of bagged
variable importance random forest trees, most or all of the trees will
use this strong predictor in the top split. Consequently, all of the
bagged trees will look quite similar to each other. Hence the
predictions from the bagged trees will be highly correlated.
Unfortunately, averaging many highly correlated quantities does not
lead to as large of a reduction in variance as averaging many
uncorrelated quantities. In particular, this means that bagging will
not lead to a substantial reduction in variance over a single tree in
this setting.

<p>
<!-- !split -->

<h2 id="random-forest-algorithm" class="anchor">Random Forest Algorithm </h2>
The algorithm described here can be applied to both classification and regression problems.

<p>
We will grow of forest of say \( B \) trees.

<ol>
<li> For \( b=1:B \)</li>

<ul>
  <li> Draw a bootstrap sample from the training data organized in our \( \boldsymbol{X} \) matrix.</li>
  <li> We grow then a random forest tree \( T_b \) based on the bootstrapped data by repeating the steps outlined till we reach the maximum node size is reached</li>

<ol>
   <li> we select \( m \le p \) variables at random from the \( p \) predictors/features</li>
   <li> pick the best split point among the \( m \) features using for example the CART algorithm and create a new node</li>
   <li> split the node into daughter nodes</li>
</ol>

</ul>

<li> Output then the ensemble of trees \( \{T_b\}_1^{B} \) and make predictions for either a regression type of problem or a classification type of problem.</li> 
</ol>

<!-- !split -->

<h2 id="random-forests-compared-with-other-methods-on-the-cancer-data" class="anchor">Random Forests Compared with other Methods on the Cancer Data </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> BaggingClassifier

<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&#39;lbfgs&#39;</span>)
logreg<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Logistic Regression: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic"># Support vector machine</span>
svm <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&#39;auto&#39;</span>, C<span style="color: #666666">=100</span>)
svm<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with SVM: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(svm<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic"># Decision Trees</span>
deep_tree_clf <span style="color: #666666">=</span> DecisionTreeClassifier(max_depth<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>)
deep_tree_clf<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Decision Trees: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic">#now scale the data</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy Logistic Regression with scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
<span style="color: #408080; font-style: italic"># Support Vector Machine</span>
svm<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy SVM with scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
<span style="color: #408080; font-style: italic"># Decision Trees</span>
deep_tree_clf<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Decision Trees and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf<span style="color: #666666">.</span>score(X_test_scaled,y_test)))


<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> LabelEncoder
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_validate
<span style="color: #408080; font-style: italic"># Data set not specificied</span>
<span style="color: #408080; font-style: italic">#Instantiate the model with 500 trees and entropy as splitting criteria</span>
Random_Forest_model <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=500</span>,criterion<span style="color: #666666">=</span><span style="color: #BA2121">&quot;entropy&quot;</span>)
Random_Forest_model<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #408080; font-style: italic">#Cross validation</span>
accuracy <span style="color: #666666">=</span> cross_validate(Random_Forest_model,X_test_scaled,y_test,cv<span style="color: #666666">=10</span>)[<span style="color: #BA2121">&#39;test_score&#39;</span>]
<span style="color: #008000">print</span>(accuracy)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Random Forests and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(Random_Forest_model<span style="color: #666666">.</span>score(X_test_scaled,y_test)))


<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
y_pred <span style="color: #666666">=</span> Random_Forest_model<span style="color: #666666">.</span>predict(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_confusion_matrix(y_test, y_pred, normalize<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
plt<span style="color: #666666">.</span>show()
y_probas <span style="color: #666666">=</span> Random_Forest_model<span style="color: #666666">.</span>predict_proba(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_roc(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_cumulative_gain(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
Recall that the cumulative gains curve shows the percentage of the
overall number of cases in a given category <em>gained</em> by targeting a
percentage of the total number of cases.

<p>
Similarly, the receiver operating characteristic curve, or ROC curve,
displays the diagnostic ability of a binary classifier system as its
discrimination threshold is varied. It plots the true positive rate against the false positive rate.

<p>
<!-- !split -->

<h2 id="compare-bagging-on-trees-with-random-forests" class="anchor">Compare  Bagging on Trees with Random Forests  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>bag_clf <span style="color: #666666">=</span> BaggingClassifier(
    DecisionTreeClassifier(splitter<span style="color: #666666">=</span><span style="color: #BA2121">&quot;random&quot;</span>, max_leaf_nodes<span style="color: #666666">=16</span>, random_state<span style="color: #666666">=42</span>),
    n_estimators<span style="color: #666666">=500</span>, max_samples<span style="color: #666666">=1.0</span>, bootstrap<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, n_jobs<span style="color: #666666">=-1</span>, random_state<span style="color: #666666">=42</span>)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>bag_clf<span style="color: #666666">.</span>fit(X_train, y_train)
y_pred <span style="color: #666666">=</span> bag_clf<span style="color: #666666">.</span>predict(X_test)
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
rnd_clf <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=500</span>, max_leaf_nodes<span style="color: #666666">=16</span>, n_jobs<span style="color: #666666">=-1</span>, random_state<span style="color: #666666">=42</span>)
rnd_clf<span style="color: #666666">.</span>fit(X_train, y_train)
y_pred_rf <span style="color: #666666">=</span> rnd_clf<span style="color: #666666">.</span>predict(X_test)
np<span style="color: #666666">.</span>sum(y_pred <span style="color: #666666">==</span> y_pred_rf) <span style="color: #666666">/</span> <span style="color: #008000">len</span>(y_pred) 
</pre></div>
<p>
<!-- !split -->

<h2 id="boosting-a-bird-s-eye-view" class="anchor">Boosting, a Bird's Eye View </h2>

<p>
The basic idea is to combine weak classifiers in order to create a good
classifier. With a weak classifier we often intend a classifier which
produces results which are only slightly better than we would get by
random guesses.

<p>
This is done by applying in an iterative way a weak (or a standard
classifier like decision trees) to modify the data. In each iteration
we emphasize those observations which are misclassified by weighting
them with a factor.

<p>
<!-- !split -->

<h2 id="what-is-boosting-additive-modelling-iterative-fitting" class="anchor">What is boosting? Additive Modelling/Iterative Fitting </h2>

<p>
Boosting is a way of fitting an additive expansion in a set of
elementary basis functions like for example some simple polynomials.
Assume for example that we have a function
$$
f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m),
$$

<p>
where \( \beta_m \) are the expansion parameters to be determined in a
minimization process and \( b(x;\gamma_m) \) are some simple functions of
the multivariable parameter \( x \) which is characterized by the
parameters \( \gamma_m \).

<p>
As an example, consider the Sigmoid function we used in logistic
regression. In that case, we can translate the function
\( b(x;\gamma_m) \) into the Sigmoid function

$$
\sigma(t) = \frac{1}{1+\exp{(-t)}},
$$

<p>
where \( t=\gamma_0+\gamma_1 x \) and the parameters \( \gamma_0 \) and
\( \gamma_1 \) were determined by the Logistic Regression fitting
algorithm.

<p>
As another example, consider the cost function we defined for linear regression
$$
C(\boldsymbol{y},\boldsymbol{f}) = \frac{1}{n} \sum_{i=0}^{n-1}(y_i-f(x_i))^2.
$$

<p>
In this case the function \( f(x) \) was replaced by the design matrix
\( \boldsymbol{X} \) and the unknown linear regression parameters \( \boldsymbol{\beta} \),
that is \( \boldsymbol{f}=\boldsymbol{X}\boldsymbol{\beta} \). In linear regression we can 
simply invert a matrix and obtain the parameters \( \beta \) by

$$
\boldsymbol{\beta}=\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$

<p>
In iterative fitting or additive modeling, we minimize the cost function with respect to the parameters \( \beta_m \) and \( \gamma_m \).

<p>
<!-- !split -->

<h2 id="iterative-fitting-regression-and-squared-error-cost-function" class="anchor">Iterative Fitting, Regression and Squared-error Cost Function </h2>

<p>
The way we proceed is as follows (here we specialize to the squared-error cost function)

<ol>
<li> Establish a cost function, here \( {\cal C}(\boldsymbol{y},\boldsymbol{f}) = \frac{1}{n} \sum_{i=0}^{n-1}(y_i-f_M(x_i))^2 \) with \( f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m) \).</li>
<li> Initialize with a guess \( f_0(x) \). It could be one or even zero or some random numbers.</li>
<li> For \( m=1:M \)

<ol type="a"></li>
 <li> minimize \( \sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\beta b(x;\gamma))^2 \) wrt \( \gamma \) and \( \beta \)</li>
 <li> This gives the optimal values \( \beta_m \) and \( \gamma_m \)</li>
 <li> Determine then the new values \( f_m(x)=f_{m-1}(x) +\beta_m b(x;\gamma_m) \)</li>
</ol>

</ol>

We could use any of the algorithms we have discussed till now. If we
use trees, \( \gamma \) parameterizes the split variables and split points
at the internal nodes, and the predictions at the terminal nodes.

<p>
<!-- !split -->

<h2 id="squared-error-example-and-iterative-fitting" class="anchor">Squared-Error Example and Iterative Fitting </h2>

<p>
To better understand what happens, let us develop the steps for the iterative fitting using the above squared error function.

<p>
For simplicity we assume also that our functions \( b(x;\gamma)=1+\gamma x \).

<p>
This means that for every iteration \( m \), we need to optimize

$$
(\beta_m,\gamma_m) = \mathrm{argmin}_{\beta,\lambda}\hspace{0.1cm} \sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\beta b(x;\gamma))^2=\sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\beta(1+\gamma x_i))^2.
$$

<p>
We start our iteration by simply setting \( f_0(x)=0 \). 
Taking the derivatives  with respect to \( \beta \) and \( \gamma \) we obtain
$$
\frac{\partial {\cal C}}{\partial \beta} = -2\sum_{i}(1+\gamma x_i)(y_i-\beta(1+\gamma x_i))=0,
$$

and
$$
\frac{\partial {\cal C}}{\partial \gamma} =-2\sum_{i}\beta x_i(y_i-\beta(1+\gamma x_i))=0.
$$

We can then rewrite these equations as (defining \( \boldsymbol{w}=\boldsymbol{e}+\gamma \boldsymbol{x}) \) with \( \boldsymbol{e} \) being the unit vector)
$$
\gamma \boldsymbol{w}^T(\boldsymbol{y}-\beta\gamma \boldsymbol{w})=0,
$$

which gives us \( \beta = \boldsymbol{w}^T\boldsymbol{y}/(\boldsymbol{w}^T\boldsymbol{w}) \). Similarly we have 
$$
\beta\gamma \boldsymbol{x}^T(\boldsymbol{y}-\beta(1+\gamma \boldsymbol{x}))=0,
$$

<p>
which leads to \( \gamma =(\boldsymbol{x}^T\boldsymbol{y}-\beta\boldsymbol{x}^T\boldsymbol{e})/(\beta\boldsymbol{x}^T\boldsymbol{x}) \).  Inserting
for \( \beta \) gives us an equation for \( \gamma \). This is a non-linear equation in the unknown \( \gamma \) and has to be solved numerically.

<p>
The solution to these two equations gives us in turn \( \beta_1 \) and \( \gamma_1 \) leading to the new expression for \( f_1(x) \) as
\( f_1(x) = \beta_1(1+\gamma_1x) \). Doing this \( M \) times results in our final estimate for the function \( f \).

<p>
<!-- !split -->

<h2 id="iterative-fitting-classification-and-adaboost" class="anchor">Iterative Fitting, Classification and AdaBoost </h2>

<p>
Let us consider a binary classification problem with two outcomes \( y_i \in \{-1,1\} \) and \( i=0,1,2,\dots,n-1 \) as our set of
observations. We define a classification function \( G(x) \) which produces a prediction taking one or the other of the two values 
\( \{-1,1\} \).

<p>
The error rate of the training sample is then

$$
\mathrm{\overline{err}}=\frac{1}{n} \sum_{i=0}^{n-1} I(y_i\ne G(x_i)). 
$$

<p>
The iterative procedure starts with defining a weak classifier whose
error rate is barely better than random guessing.  The iterative
procedure in boosting is to sequentially apply a  weak
classification algorithm to repeatedly modified versions of the data
producing a sequence of weak classifiers \( G_m(x) \).

<p>
Here we will express our  function \( f(x) \) in terms of \( G(x) \). That is
$$
f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m),
$$

will be a function of 
$$
G_M(x) = \mathrm{sign} \sum_{i=1}^M \alpha_m G_m(x).
$$

<p>
<!-- !split -->

<h2 id="adaptive-boosting-adaboost" class="anchor">Adaptive Boosting, AdaBoost </h2>

<p>
In our iterative procedure we define thus
$$
f_m(x) = f_{m-1}(x)+\beta_mG_m(x).
$$

<p>
The simplest possible cost function which leads (also simple from a computational point of view) to the AdaBoost algorithm is the
exponential cost/loss function defined as
$$
C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1}\exp{(-y_i(f_{m-1}(x_i)+\beta G(x_i))}.
$$

<p>
We optimize \( \beta \) and \( G \) for each value of \( m=1:M \) as we did in the regression case.
This is normally done in two steps. Let us however first rewrite the cost function as 

$$
C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1}w_i^{m}\exp{(-y_i\beta G(x_i))},
$$

where we have defined \( w_i^m= \exp{(-y_if_{m-1}(x_i))} \).

<p>
<!-- !split -->

<h2 id="building-up-adaboost" class="anchor">Building up AdaBoost </h2>

<p>
First, for any \( \beta > 0 \), we optimize \( G \) by setting
$$
G_m(x) = \mathrm{sign} \sum_{i=0}^{n-1} w_i^m I(y_i \ne G_(x_i)),
$$

which is the classifier that minimizes the weighted error rate in predicting \( y \).

<p>
We can do this by rewriting
$$
\exp{-(\beta)}\sum_{y_i=G(x_i)}w_i^m+\exp{(\beta)}\sum_{y_i\ne G(x_i)}w_i^m,
$$

which can be rewritten as
$$
(\exp{(\beta)}-\exp{-(\beta)})\sum_{i=0}^{n-1}w_i^mI(y_i\ne G(x_i))+\exp{(-\beta)}\sum_{i=0}^{n-1}w_i^m=0,
$$

which leads to
$$
\beta_m = \frac{1}{2}\log{\frac{1-\mathrm{\overline{err}}}{\mathrm{\overline{err}}}},
$$

where we have redefined the error as 
$$
\mathrm{\overline{err}}_m=\frac{1}{n}\frac{\sum_{i=0}^{n-1}w_i^mI(y_i\ne G(x_i)}{\sum_{i=0}^{n-1}w_i^m},
$$

which leads to an update of
$$
f_m(x) = f_{m-1}(x) +\beta_m G_m(x).
$$

This leads to the new weights
$$
w_i^{m+1} = w_i^m \exp{(-y_i\beta_m G_m(x_i))}
$$

<p>
<!-- !split -->

<h2 id="adaptive-boosting-adaboost-basic-algorithm" class="anchor">Adaptive boosting: AdaBoost, Basic Algorithm </h2>

<p>
The algorithm here is rather straightforward. Assume that our weak
classifier is a decision tree and we consider a binary set of outputs
with \( y_i \in \{-1,1\} \) and \( i=0,1,2,\dots,n-1 \) as our set of
observations. Our design matrix is given in terms of the
feature/predictor vectors
\( \boldsymbol{X}=[\boldsymbol{x}_0\boldsymbol{x}_1\dots\boldsymbol{x}_{p-1}] \). Finally, we define also a
classifier determined by our data via a function \( G(x) \). This function tells us how well we are able to classify our outputs/targets \( \boldsymbol{y} \).

<p>
We have already defined the misclassification error \( \mathrm{err} \) as
$$
\mathrm{err}=\frac{1}{n}\sum_{i=0}^{n-1}I(y_i\ne G(x_i)),
$$

where the function \( I() \) is one if we misclassify and zero if we classify correctly.

<p>
<!-- !split -->

<h2 id="basic-steps-of-adaboost" class="anchor">Basic Steps of AdaBoost </h2>

<p>
With the above definitions we are now ready to set up the algorithm for AdaBoost.
The basic idea is to set up weights which will be used to scale the correctly classified and the misclassified cases.

<ol>
<li> We start by initializing all weights to \( w_i = 1/n \), with \( i=0,1,2,\dots n-1 \). It is easy to see that we must have \( \sum_{i=0}^{n-1}w_i = 1 \).</li>
<li> We rewrite the misclassification error as</li> 
</ol>

$$
\mathrm{\overline{err}}_m=\frac{\sum_{i=0}^{n-1}w_i^m I(y_i\ne G(x_i))}{\sum_{i=0}^{n-1}w_i},
$$


<ol>
<li> Then we start looping over all attempts at classifying, namely we start an iterative process for \( m=1:M \), where \( M \) is the final number of classifications. Our given classifier could for example be a plain decision tree.

<ol type="a"></li>
 <li> Fit then a given classifier to the training set using the weights \( w_i \).</li>
 <li> Compute then \( \mathrm{err} \) and figure out which events are classified properly and which are classified wrongly.</li>
 <li> Define a quantity \( \alpha_{m} = \log{(1-\mathrm{\overline{err}}_m)/\mathrm{\overline{err}}_m} \)</li>
 <li> Set the new weights to \( w_i = w_i\times \exp{(\alpha_m I(y_i\ne G(x_i)} \).</li>
</ol>

<li> Compute the new classifier \( G(x)= \sum_{i=0}^{n-1}\alpha_m I(y_i\ne G(x_i) \).</li>
</ol>

For the iterations with \( m \le 2 \) the weights are modified
individually at each steps. The observations which were misclassified
at iteration \( m-1 \) have a weight which is larger than those which were
classified properly. As this proceeds, the observations which were
difficult to classifiy correctly are given a larger influence. Each
new classification step \( m \) is then forced to concentrate on those
observations that are missed in the previous iterations.

<p>
<!-- !split -->

<h2 id="adaboost-examples" class="anchor">AdaBoost Examples </h2>

<p>
Using <b>Scikit-Learn</b> it is easy to apply the adaptive boosting algorithm, as done here.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> AdaBoostClassifier

ada_clf <span style="color: #666666">=</span> AdaBoostClassifier(
    DecisionTreeClassifier(max_depth<span style="color: #666666">=1</span>), n_estimators<span style="color: #666666">=200</span>,
    algorithm<span style="color: #666666">=</span><span style="color: #BA2121">&quot;SAMME.R&quot;</span>, learning_rate<span style="color: #666666">=0.5</span>, random_state<span style="color: #666666">=42</span>)
ada_clf<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> AdaBoostClassifier

ada_clf <span style="color: #666666">=</span> AdaBoostClassifier(
    DecisionTreeClassifier(max_depth<span style="color: #666666">=1</span>), n_estimators<span style="color: #666666">=200</span>,
    algorithm<span style="color: #666666">=</span><span style="color: #BA2121">&quot;SAMME.R&quot;</span>, learning_rate<span style="color: #666666">=0.5</span>, random_state<span style="color: #666666">=42</span>)
ada_clf<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
y_pred <span style="color: #666666">=</span> ada_clf<span style="color: #666666">.</span>predict(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_confusion_matrix(y_test, y_pred, normalize<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
plt<span style="color: #666666">.</span>show()
y_probas <span style="color: #666666">=</span> ada_clf<span style="color: #666666">.</span>predict_proba(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_roc(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_cumulative_gain(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="gradient-boosting-basics-with-steepest-descent-functional-gradient-descent" class="anchor">Gradient boosting: Basics with Steepest Descent/Functional Gradient Descent </h2>

<p>
Gradient boosting is again a similar technique to Adaptive boosting,
it combines so-called weak classifiers or regressors into a strong
method via a series of iterations.

<p>
In order to understand the method, let us illustrate its basics by
bringing back the essential steps in linear regression, where our cost
function was the least squares function.

<p>
<!-- !split -->

<h2 id="the-squared-error-again-steepest-descent" class="anchor">The Squared-Error again! Steepest Descent </h2>

<p>
We start again with our cost function \( {\cal C}(\boldsymbol{y}m\boldsymbol{f})=\sum_{i=0}^{n-1}{\cal L}(y_i, f(x_i)) \) where we want to minimize
This means that for every iteration, we need to optimize

$$
(\hat{\boldsymbol{f}}) = \mathrm{argmin}_{\boldsymbol{f}}\hspace{0.1cm} \sum_{i=0}^{n-1}(y_i-f(x_i))^2.
$$

<p>
We define a real function \( h_m(x) \) that defines our final function \( f_M(x) \) as
$$
f_M(x) = \sum_{m=0}^M h_m(x).
$$

<p>
In the steepest decent approach we approximate \( h_m(x) = -\rho_m g_m(x) \), where \( \rho_m \) is a scalar and \( g_m(x) \) the gradient defined as
$$
g_m(x_i) = \left[ \frac{\partial {\cal L}(y_i, f(x_i))}{\partial f(x_i)}\right]_{f(x_i)=f_{m-1}(x_i)}.
$$

<p>
With the new gradient we can update \( f_m(x) = f_{m-1}(x) -\rho_m g_m(x) \). Using the above squared-error function we see that
the gradient is \( g_m(x_i) = -2(y_i-f(x_i)) \).

<p>
Choosing \( f_0(x)=0 \) we obtain \( g_m(x) = -2y_i \) and inserting this into the minimization problem for the cost function we have
$$
(\rho_1) = \mathrm{argmin}_{\rho}\hspace{0.1cm} \sum_{i=0}^{n-1}(y_i+2\rho y_i)^2.
$$

<p>
<!-- !split -->

<h2 id="steepest-descent-example" class="anchor">Steepest Descent Example </h2>

<p>
Optimizing with respect to \( \rho \) we obtain (taking the derivative) that \( \rho_1 = -1/2 \). We have then that
$$
f_1(x) = f_{0}(x) -\rho_1 g_1(x)=-y_i.
$$

We can then proceed and compute
$$
g_2(x_i) = \left[ \frac{\partial {\cal L}(y_i, f(x_i))}{\partial f(x_i)}\right]_{f(x_i)=f_{1}(x_i)=y_i}=-4y_i,
$$

and find a new value for \( \rho_2=-1/2 \) and continue till we have reached \( m=M \). We can modify the steepest descent method, or steepest boosting, by introducing what is called <b>gradient boosting</b>.

<p>
<!-- !split -->

<h2 id="gradient-boosting-algorithm" class="anchor">Gradient Boosting, algorithm </h2>

<p>
Steepest descent is however not much used, since it only optimizes \( f \) at a fixed set of \( n \) points,
so we do not learn a function that can generalize. However, we can modify the algorithm by
fitting a weak learner to approximate the negative gradient signal.

<p>
Suppose we have a cost function \( C(f)=\sum_{i=0}^{n-1}L(y_i, f(x_i)) \) where \( y_i \) is our target and \( f(x_i) \) the function which is meant to model \( y_i \). The above cost function could be our standard  squared-error  function
$$
C(\boldsymbol{y},\boldsymbol{f})=\sum_{i=0}^{n-1}(y_i-f(x_i))^2.
$$

<p>
The way we proceed in an iterative fashion is to

<ol>
<li> Initialize our estimate \( f_0(x) \).</li>
<li> For \( m=1:M \), we

<ol type="a"></li>
 <li> compute the negative gradient vector \( \boldsymbol{u}_m = -\partial C(\boldsymbol{y},\boldsymbol{f})/\partial \boldsymbol{f}(x) \) at \( f(x) = f_{m-1}(x) \);</li>
 <li> fit the so-called base-learner to the negative gradient \( h_m(u_m,x) \);</li>
 <li> update the estimate \( f_m(x) = f_{m-1}(x)+h_m(u_m,x) \);</li>
</ol>

<li> The final estimate is then \( f_M(x) = \sum_{m=1}^M h_m(u_m,x) \).</li>
</ol>

<!-- !split -->

<h2 id="gradient-boosting-examples-of-regression" class="anchor">Gradient Boosting, Examples of Regression </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> GradientBoostingRegressor
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">6</span>

<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)

error <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
bias <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
variance <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
polydegree <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(x, y, test_size<span style="color: #666666">=0.2</span>)
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,maxdegree):
    model <span style="color: #666666">=</span> GradientBoostingRegressor(max_depth<span style="color: #666666">=</span>degree, n_estimators<span style="color: #666666">=100</span>, learning_rate<span style="color: #666666">=1.0</span>)  
    model<span style="color: #666666">.</span>fit(X_train_scaled,y_train)
    y_pred <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X_test_scaled)
    polydegree[degree] <span style="color: #666666">=</span> degree
    error[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> y_pred)<span style="color: #666666">**2</span>) )
    bias[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( (y_test <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_pred))<span style="color: #666666">**2</span> )
    variance[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>var(y_pred) )
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Max depth:&#39;</span>, degree)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;</span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> &gt;= </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> + </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> = </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(error[degree], bias[degree], variance[degree], bias[degree]<span style="color: #666666">+</span>variance[degree]))

plt<span style="color: #666666">.</span>xlim(<span style="color: #666666">1</span>,maxdegree<span style="color: #666666">-1</span>)
plt<span style="color: #666666">.</span>plot(polydegree, error, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Error&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, bias, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;bias&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, variance, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Variance&#39;</span>)
plt<span style="color: #666666">.</span>legend()
save_fig(<span style="color: #BA2121">&quot;gdregression&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="gradient-boosting-classification-example" class="anchor">Gradient Boosting, Classification Example </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> GradientBoostingClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_validate

<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic">#now scale the data</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

gd_clf <span style="color: #666666">=</span> GradientBoostingClassifier(max_depth<span style="color: #666666">=3</span>, n_estimators<span style="color: #666666">=100</span>, learning_rate<span style="color: #666666">=1.0</span>)  
gd_clf<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #408080; font-style: italic">#Cross validation</span>
accuracy <span style="color: #666666">=</span> cross_validate(gd_clf,X_test_scaled,y_test,cv<span style="color: #666666">=10</span>)[<span style="color: #BA2121">&#39;test_score&#39;</span>]
<span style="color: #008000">print</span>(accuracy)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Random Forests and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(gd_clf<span style="color: #666666">.</span>score(X_test_scaled,y_test)))

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
y_pred <span style="color: #666666">=</span> gd_clf<span style="color: #666666">.</span>predict(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_confusion_matrix(y_test, y_pred, normalize<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
save_fig(<span style="color: #BA2121">&quot;gdclassiffierconfusion&quot;</span>)
plt<span style="color: #666666">.</span>show()
y_probas <span style="color: #666666">=</span> gd_clf<span style="color: #666666">.</span>predict_proba(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_roc(y_test, y_probas)
save_fig(<span style="color: #BA2121">&quot;gdclassiffierroc&quot;</span>)
plt<span style="color: #666666">.</span>show()
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_cumulative_gain(y_test, y_probas)
save_fig(<span style="color: #BA2121">&quot;gdclassiffiercgain&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="xgboost-extreme-gradient-boosting" class="anchor">XGBoost: Extreme Gradient Boosting </h2>

<p>
<a href="https://github.com/dmlc/xgboost" target="_self">XGBoost</a> or Extreme Gradient
Boosting, is an optimized distributed gradient boosting library
designed to be highly efficient, flexible and portable. It implements
machine learning algorithms under the Gradient Boosting
framework. XGBoost provides a parallel tree boosting that solve many
data science problems in a fast and accurate way. See the <a href="https://arxiv.org/abs/1603.02754" target="_self">article by Chen and Guestrin</a>.

<p>
The authors design and build a highly scalable end-to-end tree
boosting system. It has  a theoretically justified weighted quantile
sketch for efficient proposal calculation. It introduces a novel sparsity-aware algorithm for parallel tree learning and an effective cache-aware block structure for out-of-core tree learning.

<p>
It is now the algorithm which wins essentially all ML competitions!!!

<p>
<!-- !split -->

<h2 id="regression-case" class="anchor">Regression Case </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">xgboost</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">xgb</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">6</span>

<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)

error <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
bias <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
variance <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
polydegree <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(maxdegree)
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(x, y, test_size<span style="color: #666666">=0.2</span>)
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(maxdegree):
    model <span style="color: #666666">=</span>  xgb<span style="color: #666666">.</span>XGBRegressor(objective <span style="color: #666666">=</span><span style="color: #BA2121">&#39;reg:squarederror&#39;</span>, colsaobjective <span style="color: #666666">=</span><span style="color: #BA2121">&#39;reg:squarederror&#39;</span>, colsample_bytree <span style="color: #666666">=</span> <span style="color: #666666">0.3</span>, learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.1</span>,max_depth <span style="color: #666666">=</span> degree, alpha <span style="color: #666666">=</span> <span style="color: #666666">10</span>, n_estimators <span style="color: #666666">=</span> <span style="color: #666666">200</span>)

    model<span style="color: #666666">.</span>fit(X_train_scaled,y_train)
    y_pred <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X_test_scaled)
    polydegree[degree] <span style="color: #666666">=</span> degree
    error[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>mean((y_test <span style="color: #666666">-</span> y_pred)<span style="color: #666666">**2</span>) )
    bias[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( (y_test <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_pred))<span style="color: #666666">**2</span> )
    variance[degree] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean( np<span style="color: #666666">.</span>var(y_pred) )
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Max depth:&#39;</span>, degree)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;</span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> &gt;= </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> + </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121"> = </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">.</span>format(error[degree], bias[degree], variance[degree], bias[degree]<span style="color: #666666">+</span>variance[degree]))

plt<span style="color: #666666">.</span>xlim(<span style="color: #666666">1</span>,maxdegree<span style="color: #666666">-1</span>)
plt<span style="color: #666666">.</span>plot(polydegree, error, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Error&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, bias, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;bias&#39;</span>)
plt<span style="color: #666666">.</span>plot(polydegree, variance, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Variance&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="xgboost-on-the-cancer-data" class="anchor">Xgboost on the Cancer Data </h2>

<p>
As you will see from the confusion matrix below, XGBoots does an excellent job on the Wisconsin cancer data and outperforms essentially all agorithms we have discussed till now. 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> LabelEncoder
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_validate
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">xgboost</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">xgb</span>
<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic">#now scale the data</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)

xg_clf <span style="color: #666666">=</span> xgb<span style="color: #666666">.</span>XGBClassifier()
xg_clf<span style="color: #666666">.</span>fit(X_train_scaled,y_train)

y_test <span style="color: #666666">=</span> xg_clf<span style="color: #666666">.</span>predict(X_test_scaled)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Random Forests and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(xg_clf<span style="color: #666666">.</span>score(X_test_scaled,y_test)))

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
y_pred <span style="color: #666666">=</span> xg_clf<span style="color: #666666">.</span>predict(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_confusion_matrix(y_test, y_pred, normalize<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
save_fig(<span style="color: #BA2121">&quot;xdclassiffierconfusion&quot;</span>)
plt<span style="color: #666666">.</span>show()
y_probas <span style="color: #666666">=</span> xg_clf<span style="color: #666666">.</span>predict_proba(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_roc(y_test, y_probas)
save_fig(<span style="color: #BA2121">&quot;xdclassiffierroc&quot;</span>)
plt<span style="color: #666666">.</span>show()
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_cumulative_gain(y_test, y_probas)
save_fig(<span style="color: #BA2121">&quot;gdclassiffiercgain&quot;</span>)
plt<span style="color: #666666">.</span>show()


xgb<span style="color: #666666">.</span>plot_tree(xg_clf,num_trees<span style="color: #666666">=0</span>)
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;figure.figsize&#39;</span>] <span style="color: #666666">=</span> [<span style="color: #666666">50</span>, <span style="color: #666666">10</span>]
save_fig(<span style="color: #BA2121">&quot;xgtree&quot;</span>)
plt<span style="color: #666666">.</span>show()

xgb<span style="color: #666666">.</span>plot_importance(xg_clf)
plt<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;figure.figsize&#39;</span>] <span style="color: #666666">=</span> [<span style="color: #666666">5</span>, <span style="color: #666666">5</span>]
save_fig(<span style="color: #BA2121">&quot;xgparams&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split -->

<h2 id="topics-we-have-covered" class="anchor">Topics we have covered  </h2>

<p>
The course has two central parts

<ol>
<li> Statistical analysis and optimization of data</li>
<li> Machine learning</li>
</ol>

<!-- !split -->

<h2 id="statistical-analysis-and-optimization-of-data" class="anchor">Statistical analysis and optimization of data </h2>

<p>
The following topics have been discussed:

<ol>
<li> Basic concepts, expectation values, variance, covariance, correlation functions and errors;</li>
<li> Gradient methods for data optimization</li>
<li> Estimation of errors using cross-validation, bootstrapping and jackknife methods;</li>
</ol>

<!-- !split -->

<h2 id="machine-learning" class="anchor">Machine learning </h2>

<p>
The following topics will be covered

<ol>
<li> Linear methods for regression and classification:

<ol type="a"></li>
 <li> Ordinary Least Squares</li>
 <li> Ridge regression</li>
 <li> Lasso regression</li>
 <li> Logistic regression</li>
</ol>

<li> Neural networks and deep learning:

<ol type="a"></li>
 <li> Feed Forward Neural Networks</li>
 <li> Convolutional Neural Networks</li>
 <li> Recurrent Neural Networks</li>
</ol>

<li> Decisions trees and ensemble methods:

<ol type="a"></li>
 <li> Decision trees</li>
 <li> Bagging and voting</li>
 <li> Random forests</li>
 <li> Boosting and gradient boosting</li>
</ol>

</ol>

<!-- !split -->

<h2 id="learning-outcomes-and-overarching-aims-of-this-course" class="anchor">Learning outcomes and overarching aims of this course </h2>

<p>
The course introduces a variety of central algorithms and methods
essential for studies of data analysis and machine learning. The
course is project based and through the various projects, normally
three, you will be exposed to fundamental research problems
in these fields, with the aim to reproduce state of the art scientific
results. The students will learn to develop and structure large codes
for studying these systems, get acquainted with computing facilities
and learn to handle large scientific projects. A good scientific and
ethical conduct is emphasized throughout the course. 

<ul>
<li> Understand linear methods for regression and classification;</li>
<li> Learn about neural network;</li>
<li> Learn about bagging, boosting and trees</li>
<li> Learn about basic data analysis;</li>
<li> Be capable of extending the acquired knowledge to other systems and cases;</li>
<li> Have an understanding of central algorithms used in data analysis and machine learning;</li>
<li> Work on numerical projects to illustrate the theory. The projects play a central role and you are expected to know modern programming languages like Python or C++.</li>
</ul>

<!-- !split -->

<h2 id="perspective-on-machine-learning" class="anchor">Perspective on Machine Learning </h2>

<ol>
<li> Rapidly emerging application area</li>
<li> Experiment AND theory are evolving in many many fields. Still many low-hanging fruits.</li>
<li> Requires education/retraining for more widespread adoption</li>
<li> A lot of &#8220;word-of-mouth&#8221; development methods</li>
</ol>

Huge amounts of data sets require automation, classical analysis tools often inadequate. 
High energy physics hit this wall in the 90&#8217;s.
In 2009 single top quark production was determined via <a href="https://arxiv.org/pdf/0903.0850.pdf" target="_self">Boosted decision trees, Bayesian
Neural Networks, etc.</a>

<p>
<!-- !split -->

<h2 id="machine-learning-research" class="anchor">Machine Learning Research </h2>

<p>
Where to find recent results:

<ol>
<li> Conference proceedings, arXiv and blog posts!</li>
<li> <b>NIPS</b>: <a href="https://papers.nips.cc" target="_self">Neural Information Processing Systems</a></li>
<li> <b>ICLR</b>: <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference#accepted-oral-papers" target="_self">International Conference on Learning Representations</a></li>
<li> <b>ICML</b>: International Conference on Machine Learning</li>
<li> <a href="http://www.jmlr.org/papers/v19/" target="_self">Journal of Machine Learning Research</a></li> 
<li> <a href="https://arxiv.org/list/cs.LG/recent" target="_self">Follow ML on ArXiv</a></li>
</ol>

<!-- !split -->

<h2 id="starting-your-machine-learning-project" class="anchor">Starting your Machine Learning Project  </h2>

<ol>
<li> Identify problem type: classification, regression</li>
<li> Consider your data carefully</li>
<li> Choose a simple model that fits 1. and 2.</li>
<li> Consider your data carefully again! Think of data representation more carefully.</li>
<li> Based on your results, feedback loop to earliest possible point</li>
</ol>

<!-- !split -->

<h2 id="choose-a-model-and-algorithm" class="anchor">Choose a Model and Algorithm  </h2>

<ol>
<li> Supervised?</li>
<li> Start with the simplest model that fits your problem</li>
<li> Start with minimal processing of data</li>
</ol>

<!-- !split -->

<h2 id="preparing-your-data" class="anchor">Preparing Your Data </h2>

<ol>
<li> Shuffle your data</li>
<li> Mean center your data</li>

<ul>
  <li> Why?</li>
</ul>

<li> Normalize the variance</li>

<ul>
  <li> Why?</li>
</ul>

<li> <b>Whitening</b></li>

<ul>
  <li> Decorrelates data</li>
  <li> Can be hit or miss</li>
</ul>

<li> When to do train/test split?</li>
</ol>

<!-- !split -->

<h2 id="which-activation-and-weights-to-choose-in-neural-networks" class="anchor">Which Activation and Weights to Choose in Neural Networks </h2>

<ol>
<li> RELU? ELU?</li>
<li> Sigmoid or Tanh?</li>
<li> Set all weights to 0?</li>

<ul>
  <li> Terrible idea</li>
</ul>

<li> Set all weights to random values?</li>

<ul>
  <li> Small random values</li>
</ul>

</ol>

<!-- !split -->

<h2 id="optimization-methods-and-hyperparameters" class="anchor">Optimization Methods and Hyperparameters </h2>

<ol>
<li> Stochastic gradient descent

<ol type="a"></li>
<li> Stochastic gradient descent + momentum</li>
</ol>

<li> State-of-the-art approaches:</li>

<ul>
  <li> RMSProp</li>
  <li> Adam</li>
  <li> and more</li>
</ul>

</ol>

Which regularization and hyperparameters? \( L_1 \) or \( L_2 \), soft
classifiers, depths of trees and many other. Need to explore a large
set of hyperparameters and regularization methods.

<p>
<!-- !split -->

<h2 id="resampling" class="anchor">Resampling </h2>

<p>
When do we resample?

<ol>
<li> <a href="https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A" target="_self">Bootstrap</a></li>
<li> <a href="https://www.youtube.com/watch?v=fSytzGwwBVw&ab_channel=StatQuestwithJoshStarmer" target="_self">Cross-validation</a></li>
<li> Jackknife and many other</li>
</ol>

<!-- !split -->

<h2 id="what-s-the-future-like" class="anchor">What's the future like?  </h2>

<p>
Based on multi-layer nonlinear neural networks, deep learning can
learn directly from raw data, automatically extract and abstract
features from layer to layer, and then achieve the goal of regression,
classification, or ranking. Deep learning has made breakthroughs in
computer vision, speech processing and natural language, and reached
or even surpassed human level. The success of deep learning is mainly
due to the three factors: big data, big model, and big computing.

<p>
In the past few decades, many different architectures of deep neural
networks have been proposed, such as

<ol>
<li> Convolutional neural networks, which are mostly used in image and video data processing, and have also been applied to sequential data such as text processing;</li>
<li> Recurrent neural networks, which can process sequential data of variable length and have been widely used in natural language understanding and speech processing;</li>
<li> Encoder-decoder framework, which is mostly used for image or sequence generation, such as machine translation, text summarization, and image captioning.</li>
</ol>

<!-- !split -->

<h2 id="types-of-machine-learning-a-repetition" class="anchor">Types of Machine Learning, a repetition </h2>

<p>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The approaches to machine learning are many, but are often split into two main categories. 
In <em>supervised learning</em> we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, <em>unsupervised learning</em>
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.
Some authours also operate with a third category, namely <em>reinforcement learning</em>. This is a paradigm 
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.

<p>
Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:

<ul>
  <li> Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.</li>
  <li> Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.</li>
  <li> Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.</li>
  <li> Other unsupervised learning algortihms like <b>Boltzmann machines</b></li>
</ul>
</div>
</div>


<p>
<!-- !split -->

<h2 id="autoencoders-overarching-view" class="anchor">Autoencoders: Overarching view </h2>

<p>
Autoencoders are artificial neural networks capable of learning
efficient representations of the input data (these representations are called codings)  without
any supervision (i.e., the training set is unlabeled). These codings
typically have a much lower dimensionality than the input data, making
autoencoders useful for dimensionality reduction.

<p>
More importantly, autoencoders act as powerful feature detectors, and
they can be used for unsupervised pretraining of deep neural networks.

<p>
Lastly, they are capable of randomly generating new data that looks
very similar to the training data; this is called a generative
model. For example, you could train an autoencoder on pictures of
faces, and it would then be able to generate new faces.  Surprisingly,
autoencoders work by simply learning to copy their inputs to their
outputs. This may sound like a trivial task, but we will see that
constraining the network in various ways can make it rather
difficult. For example, you can limit the size of the internal
representation, or you can add noise to the inputs and train the
network to recover the original inputs. These constraints prevent the
autoencoder from trivially copying the inputs directly to the outputs,
which forces it to learn efficient ways of representing the data. In
short, the codings are byproducts of the autoencoder&#8217;s attempt to
learn the identity function under some constraints.

<p>
<a href="https://www.coursera.org/lecture/building-deep-learning-models-with-tensorflow/autoencoders-1U4L3" target="_self">Video on autoencoders</a>

<p>
See also A. Geron's textbook, chapter 15.

<p>
<!-- !split -->

<h2 id="bayesian-machine-learning" class="anchor">Bayesian Machine Learning </h2>

<p>
This is an important topic if we aim at extracting a probability
distribution. This gives us also a confidence interval and error
estimates.

<p>
Bayesian machine learning allows us to encode our prior beliefs about
what those models should look like, independent of what the data tells
us. This is especially useful when we don&#8217;t have a ton of data to
confidently learn our model.

<p>
<a href="https://www.youtube.com/watch?v=E1qhGw8QxqY&ab_channel=AndrewGordonWilson" target="_self">Video on Bayesian deep learning</a>

<p>
See also the <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/Articles/lec03.pdf" target="_self">slides here</a>.

<p>
<!-- !split -->

<h2 id="reinforcement-learning" class="anchor">Reinforcement Learning </h2>

<p>
Reinforcement Learning (RL) is one of the most exciting fields of
Machine Learning today, and also one of the oldest. It has been around
since the 1950s, producing many interesting applications over the
years.

<p>
It studies
how agents take actions based on trial and error, so as to maximize
some notion of cumulative reward in a dynamic system or
environment. Due to its generality, the problem has also been studied
in many other disciplines, such as game theory, control theory,
operations research, information theory, multi-agent systems, swarm
intelligence, statistics, and genetic algorithms.

<p>
In March 2016, AlphaGo, a computer program that plays the board game
Go, beat Lee Sedol in a five-game match. This was the first time a
computer Go program had beaten a 9-dan (highest rank) professional
without handicaps. AlphaGo is based on deep convolutional neural
networks and reinforcement learning. AlphaGo&#8217;s victory was a major
milestone in artificial intelligence and it has also made
reinforcement learning a hot research area in the field of machine
learning.

<p>
<a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&ab_channel=stanfordonline" target="_self">Lecture on Reinforcement Learning</a>.

<p>
See also A. Geron's textbook, chapter 16.

<p>
<!-- !split -->

<h2 id="transfer-learning" class="anchor">Transfer learning </h2>

<p>
The goal of transfer learning is to transfer the model or knowledge
obtained from a source task to the target task, in order to resolve
the issues of insufficient training data in the target task. The
rationality of doing so lies in that usually the source and target
tasks have inter-correlations, and therefore either the features,
samples, or models in the source task might provide useful information
for us to better solve the target task. Transfer learning is a hot
research topic in recent years, with many problems still waiting to be studied.

<p>
<a href="https://www.ias.edu/video/machinelearning/2020/0331-SamoryKpotufe" target="_self">Lecture on transfer learning</a>.

<p>
<!-- !split -->

<h2 id="adversarial-learning" class="anchor">Adversarial learning </h2>

<p>
The conventional deep generative model has a potential problem: the
model tends to generate extreme instances to maximize the
probabilistic likelihood, which will hurt its performance. Adversarial
learning utilizes the adversarial behaviors (e.g., generating
adversarial instances or training an adversarial model) to enhance the
robustness of the model and improve the quality of the generated
data. In recent years, one of the most promising unsupervised learning
technologies, generative adversarial networks (GAN), has already been
successfully applied to image, speech, and text.

<p>
<a href="https://www.youtube.com/watch?v=CIfsB_EYsVI&ab_channel=StanfordUniversitySchoolofEngineering" target="_self">Lecture on adversial learning</a>.

<p>
<!-- !split -->

<h2 id="dual-learning" class="anchor">Dual learning </h2>

<p>
Dual learning is a new learning paradigm, the basic idea of which is
to use the primal-dual structure between machine learning tasks to
obtain effective feedback/regularization, and guide and strengthen the
learning process, thus reducing the requirement of large-scale labeled
data for deep learning. The idea of dual learning has been applied to
many problems in machine learning, including machine translation,
image style conversion, question answering and generation, image
classification and generation, text classification and generation,
image-to-text, and text-to-image.

<p>
<!-- !split -->

<h2 id="distributed-machine-learning" class="anchor">Distributed machine learning </h2>

<p>
Distributed computation will speed up machine learning algorithms,
significantly improve their efficiency, and thus enlarge their
application. When distributed meets machine learning, more than just
implementing the machine learning algorithms in parallel is required.

<p>
<!-- !split -->

<h2 id="meta-learning" class="anchor">Meta learning </h2>

<p>
Meta learning is an emerging research direction in machine
learning. Roughly speaking, meta learning concerns learning how to
learn, and focuses on the understanding and adaptation of the learning
itself, instead of just completing a specific learning task. That is,
a meta learner needs to be able to evaluate its own learning methods
and adjust its own learning methods according to specific learning
tasks.

<p>
<!-- !split -->

<h2 id="the-challenges-facing-machine-learning" class="anchor">The Challenges Facing Machine Learning </h2>

<p>
While there has been much progress in machine learning, there are also challenges.

<p>
For example, the mainstream machine learning technologies are
black-box approaches, making us concerned about their potential
risks. To tackle this challenge, we may want to make machine learning
more explainable and controllable. As another example, the
computational complexity of machine learning algorithms is usually
very high and we may want to invent lightweight algorithms or
implementations. Furthermore, in many domains such as physics,
chemistry, biology, and social sciences, people usually seek elegantly
simple equations (e.g., the Schr&#246;dinger equation) to uncover the
underlying laws behind various phenomena. In the field of machine
learning, can we reveal simple laws instead of designing more complex
models for data fitting? Although there are many challenges, we are
still very optimistic about the future of machine learning. As we look
forward to the future, here are what we think the research hotspots in
the next ten years will be.

<p>
See the article on <a href="https://www.frontiersin.org/articles/10.3389/frai.2020.00025/full" target="_self">Discovery of Physics From Data: Universal Laws and Discrepancies</a>

<p>
<!-- !split -->

<h2 id="explainable-machine-learning" class="anchor">Explainable machine learning </h2>

<p>
Machine learning, especially deep learning, evolves rapidly. The
ability gap between machine and human on many complex cognitive tasks
becomes narrower and narrower. However, we are still in the very early
stage in terms of explaining why those effective models work and how
they work.

<p>
<b>What is missing: the gap between correlation and causation</b>. Standard Machine Learning is based on what e have called a frequentist approach.

<p>
Most
machine learning techniques, especially the statistical ones, depend
highly on correlations in data sets to make predictions and analyses. In
contrast, rational humans tend to reply on clear and trustworthy
causality relations obtained via logical reasoning on real and clear
facts. It is one of the core goals of explainable machine learning to
transition from solving problems by data correlation to solving
problems by logical reasoning.

<p>
<b>Bayesian Machine Learning is one of the exciting research directions in this field</b>.

<p>
<!-- !split -->

<h2 id="quantum-machine-learning" class="anchor">Quantum machine learning </h2>

<p>
Quantum machine learning is an emerging interdisciplinary research
area at the intersection of quantum computing and machine learning.

<p>
Quantum computers use effects such as quantum coherence and quantum
entanglement to process information, which is fundamentally different
from classical computers. Quantum algorithms have surpassed the best
classical algorithms in several problems (e.g., searching for an
unsorted database, inverting a sparse matrix), which we call quantum
acceleration.

<p>
When quantum computing meets machine learning, it can be a mutually
beneficial and reinforcing process, as it allows us to take advantage
of quantum computing to improve the performance of classical machine
learning algorithms. In addition, we can also use the machine learning
algorithms (on classic computers) to analyze and improve quantum
computing systems.

<p>
<a href="https://www.youtube.com/watch?v=Xh9pUu3-WxM&ab_channel=InstituteforPure%26AppliedMathematics%28IPAM%29" target="_self">Lecture on Quantum ML</a>.

<p>
<a href="https://physics.aps.org/articles/v13/179?utm_campaign=weekly&utm_medium=email&utm_source=emailalert" target="_self">Read interview with Maria Schuld on her work on Quantum Machine Learning</a>. See also <a href="https://www.springer.com/gp/book/9783319964232" target="_self">her recent textbook</a>.

<p>
<!-- !split -->

<h2 id="quantum-machine-learning-algorithms-based-on-linear-algebra" class="anchor">Quantum machine learning algorithms based on linear algebra </h2>

<p>
Many quantum machine learning algorithms are based on variants of
quantum algorithms for solving linear equations, which can efficiently
solve N-variable linear equations with complexity of O(log2 N) under
certain conditions. The quantum matrix inversion algorithm can
accelerate many machine learning methods, such as least square linear
regression, least square version of support vector machine, Gaussian
process, and more. The training of these algorithms can be simplified
to solve linear equations. The key bottleneck of this type of quantum
machine learning algorithms is data input&#8212;that is, how to initialize
the quantum system with the entire data set. Although efficient
data-input algorithms exist for certain situations, how to efficiently
input data into a quantum system is as yet unknown for most cases.

<p>
<!-- !split -->

<h2 id="quantum-reinforcement-learning" class="anchor">Quantum reinforcement learning </h2>

<p>
In quantum reinforcement learning, a quantum agent interacts with the
classical environment to obtain rewards from the environment, so as to
adjust and improve its behavioral strategies. In some cases, it
achieves quantum acceleration by the quantum processing capabilities
of the agent or the possibility of exploring the environment through
quantum superposition. Such algorithms have been proposed in
superconducting circuits and systems of trapped ions.

<p>
<!-- !split -->

<h2 id="quantum-deep-learning" class="anchor">Quantum deep learning </h2>

<p>
Dedicated quantum information processors, such as quantum annealers
and programmable photonic circuits, are well suited for building deep
quantum networks. The simplest deep quantum network is the Boltzmann
machine. The classical Boltzmann machine consists of bits with tunable
interactions and is trained by adjusting the interaction of these bits
so that the distribution of its expression conforms to the statistics
of the data. To quantize the Boltzmann machine, the neural network can
simply be represented as a set of interacting quantum spins that
correspond to an adjustable Ising model. Then, by initializing the
input neurons in the Boltzmann machine to a fixed state and allowing
the system to heat up, we can read out the output qubits to get the
result.

<p>
<!-- !split -->

<h2 id="social-machine-learning" class="anchor">Social machine learning </h2>

<p>
Machine learning aims to imitate how humans
learn. While we have developed successful machine learning algorithms,
until now we have ignored one important fact: humans are social. Each
of us is one part of the total society and it is difficult for us to
live, learn, and improve ourselves, alone and isolated. Therefore, we
should design machines with social properties. Can we let machines
evolve by imitating human society so as to achieve more effective,
intelligent, interpretable &#8220;social machine learning&#8221;?

<p>
And much more.

<p>
<!-- !split -->

<h2 id="the-last-words" class="anchor">The last words? </h2>

<p>
Early computer scientist Alan Kay said, <b>The best way to predict the
future is to create it</b>. Therefore, all machine learning
practitioners, whether scholars or engineers, professors or students,
need to work together to advance these important research
topics. Together, we will not just predict the future, but create it.

<p>

<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2021, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

